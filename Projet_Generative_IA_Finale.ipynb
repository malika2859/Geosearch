{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJET IA MOTEUR DE RECHERCHE  GEOSEARCH (GEOSPATIALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "Le projet GeoSearch est une initiative visant à développer un moteur de recherche intelligent spécialisé dans les sciences géospatiales. Dans le cadre de ce projet, j'utiliserai une Intelligence Artificielle Générative pour générer des réponses aux requêtes des utilisateurs, en utilisant une base de connaissances scientifiques enrichie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GeoSearch Diagram](./IM3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (3.10.10)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (0.3.15)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (0.1.140)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-community in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (0.3.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.6 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (0.3.7)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (0.3.15)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (0.1.140)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.17.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (2.23.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openai in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (1.54.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pypdf in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (5.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: chromadb in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (0.5.18)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (2.9.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (0.115.4)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (0.49b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (1.28.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (0.20.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (4.67.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (1.67.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (0.13.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (3.10.11)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (0.27.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from build>=1.0.3->chromadb) (24.1)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.41.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.36.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.28.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.49b0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.49b0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-instrumentation==0.49b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.49b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.26.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.10.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tiktoken in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: yt_dlp in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (2024.11.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pydub in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (0.25.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installation des packages necessaire à notre projet \n",
    "\n",
    "%pip install langchain\n",
    "%pip install langchain-community\n",
    "%pip install openai\n",
    "%pip install pypdf  \n",
    "%pip install chromadb  \n",
    "%pip install tiktoken  \n",
    "%pip install beautifulsoup4 \n",
    "%pip install yt_dlp  \n",
    "%pip install pydub "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecte de donnée dans le cadre de l'application de notre projet ( Data collection sources )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construire un prototype fonctionnel de GeoSearch en utilisant des outils open source et gratuits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecte et Pretraitement des  Données Documentaires sur le sol , la géologie , le cadastre , etude circulation  et les Infrastructures dans les Villes Sélectionnées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction des PDF et Normalisation de ces documents en JSON, incluant les champs importants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche de PDF pour : Abidjan études de sol géologie PDF\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_3.json\n",
      "Recherche de PDF pour : Abidjan rapport infrastructure PDF\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_3.json\n",
      "Recherche de PDF pour : Abidjan document cadastral PDF\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_1.pdf\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_2.json\n",
      "Recherche de PDF pour : Abidjan étude circulation PDF\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_2.json\n",
      "Pas un PDF valide ou erreur HTTP : https://www.afdb.org/fileadmin/uploads/afdb/Documents/Environmental-and-Social-Assessments/Cote_Ivoire-PTUA-Summary_ESIA_August_2016-FR.pdf\n",
      "Recherche de PDF pour : Abidjan plans réseaux souterrains PDF\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_3.json\n",
      "Recherche de PDF pour : Bamako études de sol géologie PDF\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_3.json\n",
      "Recherche de PDF pour : Bamako rapport infrastructure PDF\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_3.json\n",
      "Recherche de PDF pour : Bamako document cadastral PDF\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_3.json\n",
      "Recherche de PDF pour : Bamako étude circulation PDF\n",
      "Pas un PDF valide ou erreur HTTP : https://www.afdb.org/fileadmin/uploads/afdb/Documents/Publications/etude_securite_routiere_Bamako_2018.pdf\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_3.json\n",
      "Recherche de PDF pour : Bamako plans réseaux souterrains PDF\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_1.json\n",
      "Erreur lors du téléchargement de https://mirror.unhabitat.org/downloads/docs/BamakoCDS_ReportFrench.pdf : HTTPSConnectionPool(host='mirror.unhabitat.org', port=443): Max retries exceeded with url: /downloads/docs/BamakoCDS_ReportFrench.pdf (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "Pas un PDF valide ou erreur HTTP : https://www.afdb.org/sites/default/files/documents/environmental-and-social-assessments/mali_-_projet_dalimentation_en_eau_potable_de_la_ville_de_bamako_a_partir_de_kabala_-_resume_pges.pdf\n",
      "Recherche de PDF pour : Libreville études de sol géologie PDF\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_3.json\n",
      "Recherche de PDF pour : Libreville rapport infrastructure PDF\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_3.json\n",
      "Recherche de PDF pour : Libreville document cadastral PDF\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_3.json\n",
      "Recherche de PDF pour : Libreville étude circulation PDF\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_3.json\n",
      "Recherche de PDF pour : Libreville plans réseaux souterrains PDF\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_3.json\n",
      "Recherche de PDF pour : Cotonou études de sol géologie PDF\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_3.json\n",
      "Recherche de PDF pour : Cotonou rapport infrastructure PDF\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_2.json\n",
      "Pas un PDF valide ou erreur HTTP : https://www.afdb.org/sites/default/files/documents/projects-and-operations/benin_-_modernisation_et_extension_du_port_autonome_de_cotonou_-_note_de_synthese_de_projet.pdf\n",
      "Recherche de PDF pour : Cotonou document cadastral PDF\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_3.json\n",
      "Recherche de PDF pour : Cotonou étude circulation PDF\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_3.json\n",
      "Recherche de PDF pour : Cotonou plans réseaux souterrains PDF\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_3.json\n",
      "Recherche de PDF pour : Paris études de sol géologie PDF\n",
      "PDF téléchargé : pdf_data\\Paris_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Paris_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Paris_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_3.json\n",
      "Recherche de PDF pour : Paris rapport infrastructure PDF\n",
      "Pas un PDF valide ou erreur HTTP : https://www.un.org/pga/71/wp-content/uploads/sites/40/2017/02/New-Climate-Economy-Report-2016-Executive-Summary.pdf\n",
      "Pas un PDF valide ou erreur HTTP : https://unfccc.int/sites/default/files/resource/cma2021_08rev01_adv.pdf\n",
      "Pas un PDF valide ou erreur HTTP : https://unfccc.int/files/meetings/paris_nov_2015/application/pdf/paris_agreement_english_.pdf\n",
      "Recherche de PDF pour : Paris document cadastral PDF\n",
      "PDF téléchargé : pdf_data\\Paris_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Paris_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Paris_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_3.json\n",
      "Recherche de PDF pour : Paris étude circulation PDF\n",
      "PDF téléchargé : pdf_data\\Paris_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Paris_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Paris_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_3.json\n",
      "Recherche de PDF pour : Paris plans réseaux souterrains PDF\n",
      "PDF téléchargé : pdf_data\\Paris_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Paris_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_2.json\n",
      "Recherche de PDF pour : New York soil studies geology PDF\n",
      "PDF téléchargé : pdf_data\\New_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_1.json\n",
      "PDF téléchargé : pdf_data\\New_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_2.json\n",
      "PDF téléchargé : pdf_data\\New_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_3.json\n",
      "Recherche de PDF pour : New York infrastructure report PDF\n",
      "Pas un PDF valide ou erreur HTTP : https://infrastructurereportcard.org/wp-content/uploads/2024/04/NY-2024-State-Fact-Sheet-ASCE.pdf\n",
      "Pas un PDF valide ou erreur HTTP : https://infrastructurereportcard.org/wp-content/uploads/2017/01/IRC_Brochure-NY2022.pdf\n",
      "PDF téléchargé : pdf_data\\New_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_3.json\n",
      "Recherche de PDF pour : New York cadastral document PDF\n",
      "PDF téléchargé : pdf_data\\New_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_1.json\n",
      "PDF téléchargé : pdf_data\\New_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_2.json\n",
      "Recherche de PDF pour : New York traffic study PDF\n",
      "PDF téléchargé : pdf_data\\New_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_1.json\n",
      "PDF téléchargé : pdf_data\\New_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_2.json\n",
      "PDF téléchargé : pdf_data\\New_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_3.json\n",
      "Recherche de PDF pour : New York underground network plans PDF\n",
      "PDF téléchargé : pdf_data\\New_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_1.json\n",
      "PDF téléchargé : pdf_data\\New_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_2.json\n",
      "Recherche de PDF pour : Tokyo soil studies geology PDF\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_3.json\n",
      "Recherche de PDF pour : Tokyo infrastructure report PDF\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_3.json\n",
      "Recherche de PDF pour : Tokyo cadastral document PDF\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_3.json\n",
      "Recherche de PDF pour : Tokyo traffic study PDF\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_1.json\n",
      "Recherche de PDF pour : Tokyo underground network plans PDF\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_2.pdf\n",
      "Pas un PDF valide ou erreur HTTP : https://www.jreast.co.jp/e/press/2014/pdf/20140703.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF pour l'extraction de texte\n",
    "import json\n",
    "\n",
    "# Répertoires pour sauvegarder les PDF et les fichiers normalisés\n",
    "PDF_DIR = \"pdf_data\"\n",
    "NORMALIZED_DIR = \"normalized_data\"\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "os.makedirs(NORMALIZED_DIR, exist_ok=True)\n",
    "\n",
    "# Liste des recherches pour chaque ville\n",
    "cities_keywords = [\n",
    "    # Abidjan\n",
    "    \"Abidjan études de sol géologie PDF\",\n",
    "    \"Abidjan rapport infrastructure PDF\",\n",
    "    \"Abidjan document cadastral PDF\",\n",
    "    \"Abidjan étude circulation PDF\",\n",
    "    \"Abidjan plans réseaux souterrains PDF\",\n",
    "    # Bamako\n",
    "    \"Bamako études de sol géologie PDF\",\n",
    "    \"Bamako rapport infrastructure PDF\",\n",
    "    \"Bamako document cadastral PDF\",\n",
    "    \"Bamako étude circulation PDF\",\n",
    "    \"Bamako plans réseaux souterrains PDF\",\n",
    "    # Libreville\n",
    "    \"Libreville études de sol géologie PDF\",\n",
    "    \"Libreville rapport infrastructure PDF\",\n",
    "    \"Libreville document cadastral PDF\",\n",
    "    \"Libreville étude circulation PDF\",\n",
    "    \"Libreville plans réseaux souterrains PDF\",\n",
    "    # Cotonou\n",
    "    \"Cotonou études de sol géologie PDF\",\n",
    "    \"Cotonou rapport infrastructure PDF\",\n",
    "    \"Cotonou document cadastral PDF\",\n",
    "    \"Cotonou étude circulation PDF\",\n",
    "    \"Cotonou plans réseaux souterrains PDF\",\n",
    "    # Paris\n",
    "    \"Paris études de sol géologie PDF\",\n",
    "    \"Paris rapport infrastructure PDF\",\n",
    "    \"Paris document cadastral PDF\",\n",
    "    \"Paris étude circulation PDF\",\n",
    "    \"Paris plans réseaux souterrains PDF\",\n",
    "    # New York\n",
    "    \"New York soil studies geology PDF\",\n",
    "    \"New York infrastructure report PDF\",\n",
    "    \"New York cadastral document PDF\",\n",
    "    \"New York traffic study PDF\",\n",
    "    \"New York underground network plans PDF\",\n",
    "    # Tokyo\n",
    "    \"Tokyo soil studies geology PDF\",\n",
    "    \"Tokyo infrastructure report PDF\",\n",
    "    \"Tokyo cadastral document PDF\",\n",
    "    \"Tokyo traffic study PDF\",\n",
    "    \"Tokyo underground network plans PDF\",\n",
    "]\n",
    "\n",
    "\n",
    "# Fonction pour télécharger un fichier PDF\n",
    "def download_pdf(url, city_name, count):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200 and \"application/pdf\" in response.headers.get(\"Content-Type\", \"\"):\n",
    "            filename = os.path.join(PDF_DIR, f\"{city_name}_doc_{count}.pdf\")\n",
    "            with open(filename, \"wb\") as f:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "            print(f\"PDF téléchargé : {filename}\")\n",
    "            return filename\n",
    "        else:\n",
    "            print(f\"Pas un PDF valide ou erreur HTTP : {url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du téléchargement de {url} : {e}\")\n",
    "        return None\n",
    "\n",
    "# Fonction pour extraire le contenu textuel des PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        content = \"\"\n",
    "        for page in doc:\n",
    "            content += page.get_text()\n",
    "        doc.close()\n",
    "        return content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'extraction du texte de {pdf_path} : {e}\")\n",
    "        return None\n",
    "\n",
    "# Fonction pour normaliser les données et les sauvegarder en JSON\n",
    "def normalize_and_save_pdf(pdf_path, city_name):\n",
    "    try:\n",
    "        content = extract_text_from_pdf(pdf_path)\n",
    "        if content:\n",
    "            # Créer un dictionnaire structuré\n",
    "            normalized_data = {\n",
    "                \"city\": city_name,\n",
    "                \"file_name\": os.path.basename(pdf_path),\n",
    "                \"content\": content,\n",
    "            }\n",
    "            # Sauvegarder en JSON\n",
    "            json_path = os.path.join(NORMALIZED_DIR, os.path.basename(pdf_path).replace(\".pdf\", \".json\"))\n",
    "            with open(json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "                json.dump(normalized_data, json_file, ensure_ascii=False, indent=4)\n",
    "            print(f\"Données normalisées sauvegardées : {json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la normalisation de {pdf_path} : {e}\")\n",
    "\n",
    "# Fonction pour rechercher et télécharger des PDF\n",
    "def search_and_download_pdfs(cities_keywords, max_results=3):\n",
    "    base_url = \"https://html.duckduckgo.com/html/\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    for query in cities_keywords:\n",
    "        city_name = query.split()[0]  # Utiliser le premier mot (nom de la ville) pour nommer les fichiers\n",
    "        print(f\"Recherche de PDF pour : {query}\")\n",
    "        try:\n",
    "            data = {\"q\": query, \"t\": \"h_\", \"ia\": \"web\"}\n",
    "            response = requests.post(base_url, data=data, headers=headers)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            links_found = 0\n",
    "            downloaded_links = set()\n",
    "\n",
    "            # Parcourir les liens pour trouver des PDF\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                href = link[\"href\"]\n",
    "                if \".pdf\" in href.lower() and links_found < max_results:\n",
    "                    pdf_url = href if href.startswith(\"http\") else f\"https://{href}\"\n",
    "\n",
    "                    if pdf_url not in downloaded_links:\n",
    "                        pdf_path = download_pdf(pdf_url, city_name, links_found + 1)\n",
    "                        if pdf_path:\n",
    "                            normalize_and_save_pdf(pdf_path, city_name)\n",
    "                        downloaded_links.add(pdf_url)\n",
    "                        links_found += 1\n",
    "\n",
    "            if links_found == 0:\n",
    "                print(f\"Aucun PDF trouvé pour {city_name}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la recherche pour {query} : {e}\")\n",
    "\n",
    "# Lancer la recherche, le téléchargement et la normalisation\n",
    "search_and_download_pdfs(cities_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " validation et le nettoyage des données normalisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation et nettoyage des fichiers normalisés...\n",
      "Document non pertinent : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\New_doc_1.json\n",
      "Fichier supprimé : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\New_doc_1.json\n",
      "Enregistrement des fichiers PDF invalides...\n",
      "Liste des fichiers invalides enregistrée dans C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\invalid_pdfs.log\n",
      "Processus terminé.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Chemin vers le répertoire des données normalisées\n",
    "NORMALIZED_DIR = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\"\n",
    "LOG_FILE = os.path.join(NORMALIZED_DIR, \"invalid_pdfs.log\")\n",
    "\n",
    "# Fonction pour valider les données normalisées\n",
    "def validate_normalized_data(json_path, keywords):\n",
    "    \"\"\"\n",
    "    Valide si le contenu d'un document normalisé est pertinent.\n",
    "    \n",
    "    :param json_path: Chemin vers le fichier JSON normalisé.\n",
    "    :param keywords: Liste de mots-clés pour vérifier la pertinence.\n",
    "    :return: True si le document est valide, sinon False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        content = data.get(\"content\", \"\").strip()\n",
    "        \n",
    "        # Vérifier si le contenu est vide\n",
    "        if not content:\n",
    "            print(f\"Document vide détecté : {json_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Vérifier la pertinence avec les mots-clés\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in content.lower():\n",
    "                return True\n",
    "        \n",
    "        print(f\"Document non pertinent : {json_path}\")\n",
    "        return False\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la validation du fichier {json_path} : {e}\")\n",
    "        return False\n",
    "\n",
    "# Fonction pour nettoyer les données normalisées\n",
    "def clean_normalized_data(normalized_dir, keywords):\n",
    "    \"\"\"\n",
    "    Parcourt les fichiers normalisés et supprime les documents invalides.\n",
    "    \n",
    "    :param normalized_dir: Répertoire contenant les fichiers JSON normalisés.\n",
    "    :param keywords: Liste de mots-clés pour la validation.\n",
    "    \"\"\"\n",
    "    for json_file in os.listdir(normalized_dir):\n",
    "        json_path = os.path.join(normalized_dir, json_file)\n",
    "        if not validate_normalized_data(json_path, keywords):\n",
    "            os.remove(json_path)\n",
    "            print(f\"Fichier supprimé : {json_path}\")\n",
    "\n",
    "# Fonction pour enregistrer les fichiers PDF invalides\n",
    "def log_invalid_pdfs(normalized_dir, log_file):\n",
    "    \"\"\"\n",
    "    Enregistre les chemins des fichiers PDF invalides dans un fichier de log.\n",
    "    \n",
    "    :param normalized_dir: Répertoire contenant les fichiers JSON normalisés.\n",
    "    :param log_file: Chemin vers le fichier de log.\n",
    "    \"\"\"\n",
    "    invalid_files = []\n",
    "    \n",
    "    for json_file in os.listdir(normalized_dir):\n",
    "        json_path = os.path.join(normalized_dir, json_file)\n",
    "        try:\n",
    "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                content = data.get(\"content\", \"\").strip()\n",
    "                if not content:\n",
    "                    invalid_files.append(data.get(\"file_name\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la lecture du fichier {json_path} : {e}\")\n",
    "    \n",
    "    # Sauvegarder les fichiers invalides dans un log\n",
    "    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for invalid_file in invalid_files:\n",
    "            f.write(invalid_file + \"\\n\")\n",
    "    \n",
    "    print(f\"Liste des fichiers invalides enregistrée dans {log_file}\")\n",
    "\n",
    "# Liste des mots-clés pour valider la pertinence\n",
    "keywords = [\"géologie\", \"infrastructure\", \"cadastral\", \"circulation\", \"réseaux\"]\n",
    "\n",
    "# Lancer le nettoyage et la validation\n",
    "print(\"Validation et nettoyage des fichiers normalisés...\")\n",
    "clean_normalized_data(NORMALIZED_DIR, keywords)\n",
    "\n",
    "# Enregistrer les fichiers invalides\n",
    "print(\"Enregistrement des fichiers PDF invalides...\")\n",
    "log_invalid_pdfs(NORMALIZED_DIR, LOG_FILE)\n",
    "\n",
    "print(\"Processus terminé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Fractionnement des JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractionnement des fichiers JSON...\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Abidjan_doc_1.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Abidjan_doc_2.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Abidjan_doc_3.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Bamako_doc_1.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Bamako_doc_2.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Bamako_doc_3.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Cotonou_doc_1.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Cotonou_doc_2.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Cotonou_doc_3.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Libreville_doc_1.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Libreville_doc_2.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Libreville_doc_3.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\New_doc_2.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\New_doc_3.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Paris_doc_1.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Paris_doc_2.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Paris_doc_3.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Tokyo_doc_1.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Tokyo_doc_2.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Tokyo_doc_3.json\n",
      "Processus terminé.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Répertoires\n",
    "NORMALIZED_DIR = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\"\n",
    "FRACTIONED_DIR = os.path.join(NORMALIZED_DIR, \"fractioned_data\")\n",
    "os.makedirs(FRACTIONED_DIR, exist_ok=True)\n",
    "\n",
    "# Fonction pour fractionner le contenu textuel\n",
    "def split_content_into_segments(content, max_length=500):\n",
    "    \"\"\"\n",
    "    Fractionne le contenu textuel en segments de taille limitée.\n",
    "    \n",
    "    :param content: Texte complet à fractionner.\n",
    "    :param max_length: Nombre maximal de caractères par segment.\n",
    "    :return: Liste de segments.\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    words = content.split()\n",
    "    segment = []\n",
    "\n",
    "    for word in words:\n",
    "        if len(\" \".join(segment + [word])) <= max_length:\n",
    "            segment.append(word)\n",
    "        else:\n",
    "            segments.append(\" \".join(segment))\n",
    "            segment = [word]\n",
    "\n",
    "    if segment:\n",
    "        segments.append(\" \".join(segment))\n",
    "    \n",
    "    return segments\n",
    "\n",
    "# Fonction pour fractionner un fichier JSON\n",
    "def fractionate_json(json_path, output_dir, max_length=500):\n",
    "    \"\"\"\n",
    "    Fractionne le contenu d'un JSON en segments et les sauvegarde individuellement.\n",
    "    \n",
    "    :param json_path: Chemin vers le fichier JSON normalisé.\n",
    "    :param output_dir: Répertoire pour sauvegarder les segments fractionnés.\n",
    "    :param max_length: Longueur maximale d'un segment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        content = data.get(\"content\", \"\").strip()\n",
    "        if not content:\n",
    "            print(f\"Contenu vide dans : {json_path}\")\n",
    "            return\n",
    "\n",
    "        # Fractionner le contenu\n",
    "        segments = split_content_into_segments(content, max_length)\n",
    "\n",
    "        for i, segment in enumerate(segments):\n",
    "            segment_data = {\n",
    "                \"city\": data.get(\"city\"),\n",
    "                \"file_name\": data.get(\"file_name\"),\n",
    "                \"segment_id\": i + 1,\n",
    "                \"segment_content\": segment,\n",
    "                \"document_type\": data.get(\"document_type\", \"unknown\"),\n",
    "            }\n",
    "\n",
    "            # Sauvegarder chaque segment dans un fichier JSON séparé\n",
    "            segment_file = os.path.join(\n",
    "                output_dir, f\"{os.path.basename(json_path).replace('.json', '')}_segment_{i + 1}.json\"\n",
    "            )\n",
    "            with open(segment_file, \"w\", encoding=\"utf-8\") as segment_f:\n",
    "                json.dump(segment_data, segment_f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Fractionnement terminé pour : {json_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du fractionnement de {json_path} : {e}\")\n",
    "\n",
    "# Fonction pour parcourir tous les fichiers JSON et les fractionner\n",
    "def process_all_json_files(normalized_dir, fractioned_dir, max_length=500):\n",
    "    \"\"\"\n",
    "    Parcourt tous les fichiers JSON normalisés et les fractionne.\n",
    "    \n",
    "    :param normalized_dir: Répertoire contenant les fichiers JSON normalisés.\n",
    "    :param fractioned_dir: Répertoire pour sauvegarder les segments fractionnés.\n",
    "    :param max_length: Longueur maximale d'un segment.\n",
    "    \"\"\"\n",
    "    for json_file in os.listdir(normalized_dir):\n",
    "        json_path = os.path.join(normalized_dir, json_file)\n",
    "        if json_file.endswith(\".json\"):\n",
    "            fractionate_json(json_path, fractioned_dir, max_length)\n",
    "\n",
    "# Exécution du fractionnement\n",
    "print(\"Fractionnement des fichiers JSON...\")\n",
    "process_all_json_files(NORMALIZED_DIR, FRACTIONED_DIR)\n",
    "print(\"Processus terminé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecte et Traitement des images satellitaires acquises sur SentilHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: shapely in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (2.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from shapely) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wheel in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (0.45.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pipwin in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: docopt in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (0.6.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (2.32.3)\n",
      "Requirement already satisfied: pyprind in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (2.11.3)\n",
      "Requirement already satisfied: six in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (4.12.3)\n",
      "Requirement already satisfied: js2py in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (0.74)\n",
      "Requirement already satisfied: packaging in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (24.1)\n",
      "Requirement already satisfied: pySmartDL>=1.3.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (1.3.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from beautifulsoup4>=4.9.0->pipwin) (2.6)\n",
      "Requirement already satisfied: tzlocal>=1.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from js2py->pipwin) (5.2)\n",
      "Requirement already satisfied: pyjsparser>=2.5.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from js2py->pipwin) (2.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests->pipwin) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests->pipwin) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests->pipwin) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests->pipwin) (2024.8.30)\n",
      "Requirement already satisfied: tzdata in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from tzlocal>=1.2->js2py->pipwin) (2024.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Scripts\\pipwin.exe\\__main__.py\", line 4, in <module>\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\pipwin\\command.py\", line 28, in <module>\n",
      "    from . import pipwin, __version__\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\pipwin\\pipwin.py\", line 13, in <module>\n",
      "    import js2py\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\__init__.py\", line 72, in <module>\n",
      "    from .base import PyJsException\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\base.py\", line 2965, in <module>\n",
      "    @Js\n",
      "     ^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\base.py\", line 165, in Js\n",
      "    return PyJsFunction(val, FunctionPrototype)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\base.py\", line 1377, in __init__\n",
      "    cand = fix_js_args(func)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\utils\\injector.py\", line 27, in fix_js_args\n",
      "    code = append_arguments(six.get_function_code(func), ('this', 'arguments'))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\utils\\injector.py\", line 121, in append_arguments\n",
      "    arg = name_translations[inst.arg]\n",
      "          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shapely==2.0.1Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached shapely-2.0.1.tar.gz (275 kB)\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [3 lines of output]\n",
      "      <string>:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "      Could not find geos-config executable. Either append the path to geos-config to PATH or manually provide the include_dirs, library_dirs, libraries and other link args for compiling against a GEOS version >=3.5.\n",
      "      ERROR: Cython is required to build shapely from source.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Scripts\\pipwin.exe\\__main__.py\", line 4, in <module>\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\pipwin\\command.py\", line 28, in <module>\n",
      "    from . import pipwin, __version__\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\pipwin\\pipwin.py\", line 13, in <module>\n",
      "    import js2py\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\__init__.py\", line 72, in <module>\n",
      "    from .base import PyJsException\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\base.py\", line 2965, in <module>\n",
      "    @Js\n",
      "     ^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\base.py\", line 165, in Js\n",
      "    return PyJsFunction(val, FunctionPrototype)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\base.py\", line 1377, in __init__\n",
      "    cand = fix_js_args(func)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\utils\\injector.py\", line 27, in fix_js_args\n",
      "    code = append_arguments(six.get_function_code(func), ('this', 'arguments'))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\utils\\injector.py\", line 121, in append_arguments\n",
      "    arg = name_translations[inst.arg]\n",
      "          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 3\n"
     ]
    }
   ],
   "source": [
    "# Importation des library pour la collecte et traitement des données \n",
    "%pip install requests shapely\n",
    "%pip install wheel\n",
    "%pip install pipwin\n",
    "%pipwin install GDAL==3.6.0\n",
    "%pip install shapely==2.0.1 --no-build-isolation\n",
    "%pipwin install rasterio==1.3.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Collecte de donnée géospatiale ( image sentinel ) sur la plateforme SentinelHub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-01.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-01.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-02.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-02.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-03.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-03.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-04.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-04.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-05.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-05.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-06.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-06.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-07.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-07.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-08.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-08.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-09.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-09.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-10.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-10.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-01.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-01.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-02.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-02.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-03.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-03.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-04.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-04.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-05.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-05.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-06.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-06.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-07.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-07.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-08.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-08.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-09.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-09.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-10.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-10.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-01.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-01.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-02.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-02.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-03.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-03.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-04.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-04.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-05.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-05.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-06.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-06.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-07.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-07.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-08.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-08.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-09.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-09.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-10.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-10.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-01.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-01.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-02.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-02.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-03.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-03.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-04.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-04.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-05.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-05.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-06.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-06.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-07.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-07.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-08.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-08.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-09.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-09.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-10.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-10.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-01.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-01.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-02.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-02.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-03.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-03.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-04.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-04.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-05.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-05.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-06.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-06.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-07.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-07.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-08.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-08.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-09.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-09.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-10.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-10.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-01.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-01.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-02.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-02.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-03.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-03.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-04.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-04.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-05.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-05.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-06.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-06.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-07.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-07.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-08.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-08.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-09.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-09.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-10.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-10.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-01.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-01.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-02.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-02.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-03.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-03.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-04.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-04.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-05.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-05.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-06.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-06.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-07.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-07.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-08.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-08.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-09.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-09.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-10.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-10.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.geometry import mapping, box\n",
    "import os\n",
    "\n",
    "# Informations pour l'accès à l'API Sentinel Hub\n",
    "CLIENT_ID = 'c576337e-156f-4bfb-bec1-da8c343c7a00'\n",
    "CLIENT_SECRET = 'hJ1yQ9PI07MJw4R9fVpooFvpwkL59U6x'\n",
    "\n",
    "def get_sentinel_token():\n",
    "    \"\"\"\n",
    "    Obtient un jeton d'authentification pour l'API Sentinel Hub.\n",
    "    \"\"\"\n",
    "    url = \"https://services.sentinel-hub.com/oauth/token\"\n",
    "    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "    data = {\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': CLIENT_ID,\n",
    "        'client_secret': CLIENT_SECRET\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=data)\n",
    "    response.raise_for_status()  # Gérer les erreurs d'authentification\n",
    "    return response.json().get(\"access_token\")\n",
    "\n",
    "def fetch_satellite_image(city_name, latitude, longitude, date='2023-10-01', resolution=10, bbox_size=0.05):\n",
    "    \"\"\"\n",
    "    Télécharge une image satellite pour une position géographique donnée\n",
    "    et crée un fichier JSON contenant les métadonnées de l'image.\n",
    "    \"\"\"\n",
    "    # Création de la bbox autour du point central (ville)\n",
    "    bbox = box(\n",
    "        longitude - bbox_size, latitude - bbox_size,\n",
    "        longitude + bbox_size, latitude + bbox_size\n",
    "    )\n",
    "\n",
    "    # Ajouter l'evalscript pour l'imagerie RGB\n",
    "    evalscript = \"\"\"\n",
    "    // Evalscript pour une image RGB\n",
    "    // Sélectionne les canaux R, G, B pour l'affichage des couleurs naturelles\n",
    "    function setup() {\n",
    "        return {\n",
    "            input: [\"B04\", \"B03\", \"B02\"],\n",
    "            output: { bands: 3 }\n",
    "        };\n",
    "    }\n",
    "    function evaluatePixel(sample) {\n",
    "        return [sample.B04, sample.B03, sample.B02];\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    # Préparer la demande à l'API Sentinel Hub\n",
    "    url = \"https://services.sentinel-hub.com/api/v1/process\"\n",
    "    token = get_sentinel_token()\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"input\": {\n",
    "            \"bounds\": {\"geometry\": mapping(bbox)},\n",
    "            \"data\": [\n",
    "                {\n",
    "                    \"type\": \"S2L2A\",\n",
    "                    \"dataFilter\": {\n",
    "                        \"timeRange\": {\n",
    "                            \"from\": f\"{date}T00:00:00Z\",\n",
    "                            \"to\": f\"{date}T23:59:59Z\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"output\": {\n",
    "            \"width\": int(1000 / resolution),\n",
    "            \"height\": int(1000 / resolution),\n",
    "            \"responses\": [\n",
    "                {\"identifier\": \"default\", \"format\": {\"type\": \"image/jpeg\"}}\n",
    "            ]\n",
    "        },\n",
    "        \"evalscript\": evalscript  # Ajouter l'evalscript ici\n",
    "    }\n",
    "\n",
    "    # Envoyer la demande et gérer les erreurs éventuelles\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        # Créer le répertoire pour la ville\n",
    "        directory = f\"images/{city_name}\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Sauvegarder l'image\n",
    "        image_filename = f\"{directory}/{city_name}_{date}.jpg\"\n",
    "        with open(image_filename, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Image satellite téléchargée pour {city_name} : {image_filename}\")\n",
    "\n",
    "        # Créer et sauvegarder les métadonnées dans un fichier JSON\n",
    "        metadata = {\n",
    "            \"city\": city_name,\n",
    "            \"date\": date,\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"resolution\": resolution,\n",
    "            \"bbox\": {\n",
    "                \"min_lat\": latitude - bbox_size,\n",
    "                \"min_lon\": longitude - bbox_size,\n",
    "                \"max_lat\": latitude + bbox_size,\n",
    "                \"max_lon\": longitude + bbox_size\n",
    "            },\n",
    "            \"image_path\": image_filename\n",
    "        }\n",
    "        metadata_filename = f\"{directory}/{city_name}_{date}.json\"\n",
    "        with open(metadata_filename, \"w\", encoding=\"utf-8\") as metadata_file:\n",
    "            json.dump(metadata, metadata_file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Métadonnées sauvegardées pour {city_name} : {metadata_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Erreur pour {city_name} à la date {date}: {response.text}\")\n",
    "\n",
    "def collect_images_for_cities(cities, dates):\n",
    "    \"\"\"\n",
    "    Télécharge des images satellite pour une liste de villes et de dates données.\n",
    "    \"\"\"\n",
    "    for city in cities:\n",
    "        for date in dates:\n",
    "            fetch_satellite_image(city[\"name\"], city[\"latitude\"], city[\"longitude\"], date=date)\n",
    "\n",
    "# Liste des villes demandées\n",
    "cities = [\n",
    "    {\"name\": \"Abidjan\", \"latitude\": 5.347, \"longitude\": -4.0244},\n",
    "    {\"name\": \"Cotonou\", \"latitude\": 6.3703, \"longitude\": 2.3912},\n",
    "    {\"name\": \"Bamako\", \"latitude\": 12.6392, \"longitude\": -8.0029},\n",
    "    {\"name\": \"Libreville\", \"latitude\": 0.4162, \"longitude\": 9.4673},\n",
    "    {\"name\": \"Paris\", \"latitude\": 48.8566, \"longitude\": 2.3522},\n",
    "    {\"name\": \"New York\", \"latitude\": 40.7128, \"longitude\": -74.0060},\n",
    "    {\"name\": \"Tokyo\", \"latitude\": 35.6895, \"longitude\": 139.6917}\n",
    "]\n",
    "\n",
    "# Liste de dates pour la collecte d'images\n",
    "start_date = datetime(2023, 10, 1)\n",
    "end_date = datetime(2023, 10, 10)\n",
    "dates = [(start_date + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range((end_date - start_date).days + 1)]\n",
    "\n",
    "# Collecte des images\n",
    "collect_images_for_cities(cities, dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecte de la donnée  sur OPenstreetMap (OSM) pour plusieurs villes à partir d'un API open acces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [5.3, -4.05, 5.4, -4.0]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Abidjan_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Abidjan_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [5.3, -4.05, 5.4, -4.0]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Abidjan_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Abidjan_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [5.3, -4.05, 5.4, -4.0]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Abidjan_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Abidjan_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [6.34, 2.36, 6.38, 2.4]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Cotonou_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Cotonou_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [6.34, 2.36, 6.38, 2.4]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Cotonou_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Cotonou_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [6.34, 2.36, 6.38, 2.4]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Cotonou_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Cotonou_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [12.6, -8.05, 12.65, -7.95]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Bamako_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Bamako_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [12.6, -8.05, 12.65, -7.95]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Bamako_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Bamako_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [12.6, -8.05, 12.65, -7.95]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Bamako_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Bamako_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [0.37, 9.4, 0.46, 9.5]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Libreville_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Libreville_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [0.37, 9.4, 0.46, 9.5]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Libreville_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Libreville_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [0.37, 9.4, 0.46, 9.5]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Libreville_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Libreville_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [48.8156, 2.2242, 48.9021, 2.4699]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Paris_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Paris_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [48.8156, 2.2242, 48.9021, 2.4699]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Paris_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Paris_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [48.8156, 2.2242, 48.9021, 2.4699]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Paris_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Paris_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [40.7, -74.02, 40.78, -73.93]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\New York_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\New York_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [40.7, -74.02, 40.78, -73.93]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\New York_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\New York_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [40.7, -74.02, 40.78, -73.93]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\New York_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\New York_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [35.68, 139.75, 35.7, 139.77]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Tokyo_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Tokyo_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [35.68, 139.75, 35.7, 139.77]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Tokyo_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Tokyo_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [35.68, 139.75, 35.7, 139.77]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Tokyo_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Tokyo_amenity_metadata.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from geojson import FeatureCollection, Feature, Point, LineString\n",
    "from shapely.geometry import Point as ShapelyPoint\n",
    "\n",
    "def overpass_to_geojson(overpass_data):\n",
    "    \"\"\"\n",
    "    Convertit les données Overpass API en GeoJSON valide avec une colonne 'geom'.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Créer un dictionnaire pour accéder rapidement aux nodes par ID\n",
    "    nodes = {element[\"id\"]: element for element in overpass_data.get(\"elements\", []) if element[\"type\"] == \"node\"}\n",
    "\n",
    "    for element in overpass_data.get(\"elements\", []):\n",
    "        if element[\"type\"] == \"node\":\n",
    "            # Ajouter des points avec une colonne 'geom' contenant la géométrie\n",
    "            geom = ShapelyPoint(element[\"lon\"], element[\"lat\"])\n",
    "            feature = Feature(\n",
    "                geometry=Point((element[\"lon\"], element[\"lat\"])),\n",
    "                properties={**element.get(\"tags\", {}), \"geom\": geom.wkt}  # Ajout du WKT dans les propriétés\n",
    "            )\n",
    "            features.append(feature)\n",
    "        elif element[\"type\"] == \"way\":\n",
    "            # Construire des lignes en utilisant les nœuds associés\n",
    "            coordinates = [(nodes[node_id][\"lon\"], nodes[node_id][\"lat\"]) for node_id in element.get(\"nodes\", []) if node_id in nodes]\n",
    "            feature = Feature(\n",
    "                geometry=LineString(coordinates),\n",
    "                properties=element.get(\"tags\", {})\n",
    "            )\n",
    "            features.append(feature)\n",
    "\n",
    "    return FeatureCollection(features)\n",
    "\n",
    "\n",
    "def download_osm_data(bbox, output_path, query_type, metadata_output, city_name):\n",
    "    \"\"\"\n",
    "    Télécharge les données OpenStreetMap pour une zone géographique donnée (bbox) via Overpass API\n",
    "    et génère un fichier GeoJSON valide avec métadonnées et une colonne 'geom'.\n",
    "    \"\"\"\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "\n",
    "    overpass_query = f\"\"\"\n",
    "    [out:json];\n",
    "    (\n",
    "      node[\"{query_type}\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      way[\"{query_type}\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      relation[\"{query_type}\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        print(f\"Envoi de la requête à Overpass API pour le type '{query_type}' et la zone {bbox}\")\n",
    "        response = requests.get(overpass_url, params={\"data\": overpass_query})\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            # Conversion des données Overpass en GeoJSON\n",
    "            geojson_data = overpass_to_geojson(data)\n",
    "\n",
    "            # Sauvegarde des données en GeoJSON\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(geojson_data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Données téléchargées et sauvegardées dans : {output_path}\")\n",
    "\n",
    "            # Création des métadonnées\n",
    "            metadata = {\n",
    "                \"city\": city_name,\n",
    "                \"type\": query_type,\n",
    "                \"bbox\": {\n",
    "                    \"min_lat\": bbox[0],\n",
    "                    \"min_lon\": bbox[1],\n",
    "                    \"max_lat\": bbox[2],\n",
    "                    \"max_lon\": bbox[3]\n",
    "                },\n",
    "                \"file_path\": output_path\n",
    "            }\n",
    "\n",
    "            # Sauvegarde des métadonnées\n",
    "            with open(metadata_output, \"w\", encoding=\"utf-8\") as meta_file:\n",
    "                json.dump(metadata, meta_file, ensure_ascii=False, indent=2)\n",
    "            print(f\"Métadonnées sauvegardées dans : {metadata_output}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Erreur : statut {response.status_code}. Requête échouée pour la zone {bbox}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue lors de la requête Overpass : {e}\")\n",
    "\n",
    "\n",
    "# Liste des villes avec leurs coordonnées bbox\n",
    "cities = [\n",
    "    {\"name\": \"Abidjan\", \"bbox\": [5.3000, -4.0500, 5.4000, -4.0000]},\n",
    "    {\"name\": \"Cotonou\", \"bbox\": [6.3400, 2.3600, 6.3800, 2.4000]},\n",
    "    {\"name\": \"Bamako\", \"bbox\": [12.6000, -8.0500, 12.6500, -7.9500]},\n",
    "    {\"name\": \"Libreville\", \"bbox\": [0.3700, 9.4000, 0.4600, 9.5000]},\n",
    "    {\"name\": \"Paris\", \"bbox\": [48.8156, 2.2242, 48.9021, 2.4699]},\n",
    "    {\"name\": \"New York\", \"bbox\": [40.7000, -74.0200, 40.7800, -73.9300]},\n",
    "    {\"name\": \"Tokyo\", \"bbox\": [35.6800, 139.7500, 35.7000, 139.7700]},\n",
    "]\n",
    "\n",
    "# Types de données OSM à télécharger\n",
    "query_types = [\"highway\", \"building\", \"amenity\"]\n",
    "\n",
    "# Dossier de sortie\n",
    "output_folder = \"osm_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Télécharger les données pour chaque ville et chaque type\n",
    "for city in cities:\n",
    "    for query_type in query_types:\n",
    "        output_file = os.path.join(output_folder, f\"{city['name']}_{query_type}.geojson\")\n",
    "        metadata_file = os.path.join(output_folder, f\"{city['name']}_{query_type}_metadata.json\")\n",
    "        download_osm_data(city[\"bbox\"], output_file, query_type=query_type, metadata_output=metadata_file, city_name=city[\"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [5.3, -4.05, 5.4, -4.0]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Abidjan_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Abidjan_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [5.3, -4.05, 5.4, -4.0]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Abidjan_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Abidjan_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [5.3, -4.05, 5.4, -4.0]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Abidjan_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Abidjan_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [6.34, 2.36, 6.38, 2.4]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Cotonou_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Cotonou_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [6.34, 2.36, 6.38, 2.4]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Cotonou_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Cotonou_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [6.34, 2.36, 6.38, 2.4]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Cotonou_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Cotonou_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [12.6, -8.05, 12.65, -7.95]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Bamako_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Bamako_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [12.6, -8.05, 12.65, -7.95]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Bamako_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Bamako_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [12.6, -8.05, 12.65, -7.95]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Bamako_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Bamako_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [0.37, 9.4, 0.46, 9.5]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Libreville_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Libreville_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [0.37, 9.4, 0.46, 9.5]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Libreville_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Libreville_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [0.37, 9.4, 0.46, 9.5]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Libreville_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Libreville_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [48.8156, 2.2242, 48.9021, 2.4699]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Paris_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Paris_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [48.8156, 2.2242, 48.9021, 2.4699]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Paris_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Paris_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [48.8156, 2.2242, 48.9021, 2.4699]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Paris_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Paris_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [40.7, -74.02, 40.78, -73.93]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\New York_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\New York_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [40.7, -74.02, 40.78, -73.93]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\New York_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\New York_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [40.7, -74.02, 40.78, -73.93]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\New York_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\New York_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [35.68, 139.75, 35.7, 139.77]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Tokyo_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Tokyo_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [35.68, 139.75, 35.7, 139.77]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Tokyo_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Tokyo_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [35.68, 139.75, 35.7, 139.77]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Tokyo_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Tokyo_amenity_metadata.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from geojson import FeatureCollection, Feature, Point, LineString, Polygon\n",
    "\n",
    "def overpass_to_geojson(overpass_data):\n",
    "    \"\"\"\n",
    "    Convertit les données Overpass API en GeoJSON valide.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Créer un dictionnaire pour accéder rapidement aux nodes par ID\n",
    "    nodes = {element[\"id\"]: element for element in overpass_data.get(\"elements\", []) if element[\"type\"] == \"node\"}\n",
    "\n",
    "    for element in overpass_data.get(\"elements\", []):\n",
    "        if element[\"type\"] == \"node\":\n",
    "            # Ajouter des points\n",
    "            features.append(\n",
    "                Feature(\n",
    "                    geometry=Point((element[\"lon\"], element[\"lat\"])),\n",
    "                    properties=element.get(\"tags\", {})\n",
    "                )\n",
    "            )\n",
    "        elif element[\"type\"] == \"way\":\n",
    "            # Construire des lignes en utilisant les nœuds associés\n",
    "            coordinates = [(nodes[node_id][\"lon\"], nodes[node_id][\"lat\"]) for node_id in element.get(\"nodes\", []) if node_id in nodes]\n",
    "            features.append(\n",
    "                Feature(\n",
    "                    geometry=LineString(coordinates),\n",
    "                    properties=element.get(\"tags\", {})\n",
    "                )\n",
    "            )\n",
    "        elif element[\"type\"] == \"relation\":\n",
    "            # Les relations nécessitent une gestion spécifique pour construire des polygones ou autres\n",
    "            # Ici, on peut les ignorer ou gérer les cas les plus fréquents\n",
    "            pass\n",
    "\n",
    "    return FeatureCollection(features)\n",
    "\n",
    "\n",
    "def download_osm_data(bbox, output_path, query_type, metadata_output, city_name):\n",
    "    \"\"\"\n",
    "    Télécharge les données OpenStreetMap pour une zone géographique donnée (bbox) via Overpass API\n",
    "    et génère un fichier GeoJSON valide avec métadonnées.\n",
    "    \"\"\"\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "\n",
    "    overpass_query = f\"\"\"\n",
    "    [out:json];\n",
    "    (\n",
    "      node[\"{query_type}\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      way[\"{query_type}\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      relation[\"{query_type}\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        print(f\"Envoi de la requête à Overpass API pour le type '{query_type}' et la zone {bbox}\")\n",
    "        response = requests.get(overpass_url, params={\"data\": overpass_query})\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            # Conversion des données Overpass en GeoJSON\n",
    "            geojson_data = overpass_to_geojson(data)\n",
    "\n",
    "            # Sauvegarde des données en GeoJSON\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(geojson_data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Données téléchargées et sauvegardées dans : {output_path}\")\n",
    "\n",
    "            # Création des métadonnées\n",
    "            metadata = {\n",
    "                \"city\": city_name,\n",
    "                \"type\": query_type,\n",
    "                \"bbox\": {\n",
    "                    \"min_lat\": bbox[0],\n",
    "                    \"min_lon\": bbox[1],\n",
    "                    \"max_lat\": bbox[2],\n",
    "                    \"max_lon\": bbox[3]\n",
    "                },\n",
    "                \"file_path\": output_path\n",
    "            }\n",
    "\n",
    "            # Sauvegarde des métadonnées\n",
    "            with open(metadata_output, \"w\", encoding=\"utf-8\") as meta_file:\n",
    "                json.dump(metadata, meta_file, ensure_ascii=False, indent=2)\n",
    "            print(f\"Métadonnées sauvegardées dans : {metadata_output}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Erreur : statut {response.status_code}. Requête échouée pour la zone {bbox}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue lors de la requête Overpass : {e}\")\n",
    "\n",
    "\n",
    "# Liste des villes avec leurs coordonnées bbox\n",
    "cities = [\n",
    "    {\"name\": \"Abidjan\", \"bbox\": [5.3000, -4.0500, 5.4000, -4.0000]},\n",
    "    {\"name\": \"Cotonou\", \"bbox\": [6.3400, 2.3600, 6.3800, 2.4000]},\n",
    "    {\"name\": \"Bamako\", \"bbox\": [12.6000, -8.0500, 12.6500, -7.9500]},\n",
    "    {\"name\": \"Libreville\", \"bbox\": [0.3700, 9.4000, 0.4600, 9.5000]},\n",
    "    {\"name\": \"Paris\", \"bbox\": [48.8156, 2.2242, 48.9021, 2.4699]},\n",
    "    {\"name\": \"New York\", \"bbox\": [40.7000, -74.0200, 40.7800, -73.9300]},\n",
    "    {\"name\": \"Tokyo\", \"bbox\": [35.6800, 139.7500, 35.7000, 139.7700]},\n",
    "]\n",
    "\n",
    "# Types de données OSM à télécharger\n",
    "query_types = [\"highway\", \"building\", \"amenity\"]\n",
    "\n",
    "# Dossier de sortie\n",
    "output_folder = \"osm_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Télécharger les données pour chaque ville et chaque type\n",
    "for city in cities:\n",
    "    for query_type in query_types:\n",
    "        output_file = os.path.join(output_folder, f\"{city['name']}_{query_type}.geojson\")\n",
    "        metadata_file = os.path.join(output_folder, f\"{city['name']}_{query_type}_metadata.json\")\n",
    "        download_osm_data(city[\"bbox\"], output_file, query_type=query_type, metadata_output=metadata_file, city_name=city[\"name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection de données educatives et documentaires vidéos avec un  API Open Access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche de vidéos pour : Abidjan géomatique extraction de données\n",
      "Résultats sauvegardés dans : youtube_videos\\Abidjan_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\Abidjan_videos.json\n",
      "Recherche de vidéos pour : Abidjan études de sol géologie\n",
      "Résultats sauvegardés dans : youtube_videos\\Abidjan_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\Abidjan_videos.json\n",
      "Recherche de vidéos pour : Cotonou rapport cartographique\n",
      "Résultats sauvegardés dans : youtube_videos\\Cotonou_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\Cotonou_videos.json\n",
      "Recherche de vidéos pour : Bamako études géologiques\n",
      "Résultats sauvegardés dans : youtube_videos\\Bamako_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\Bamako_videos.json\n",
      "Recherche de vidéos pour : Libreville géomatique rapports cartographiques\n",
      "Résultats sauvegardés dans : youtube_videos\\Libreville_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\Libreville_videos.json\n",
      "Recherche de vidéos pour : Paris géomatique études de sol\n",
      "Résultats sauvegardés dans : youtube_videos\\Paris_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\Paris_videos.json\n",
      "Recherche de vidéos pour : New York géospatial data analysis\n",
      "Résultats sauvegardés dans : youtube_videos\\New_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\New_videos.json\n",
      "Recherche de vidéos pour : Tokyo geomatics data extraction\n",
      "Résultats sauvegardés dans : youtube_videos\\Tokyo_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\Tokyo_videos.json\n"
     ]
    }
   ],
   "source": [
    "from youtubesearchpython import VideosSearch\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Répertoire pour sauvegarder les résultats\n",
    "RESULTS_DIR = \"youtube_videos\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Liste des recherches pour chaque ville\n",
    "search_queries = [\n",
    "    \"Abidjan géomatique extraction de données\",\n",
    "    \"Abidjan études de sol géologie\",\n",
    "    \"Cotonou rapport cartographique\",\n",
    "    \"Bamako études géologiques\",\n",
    "    \"Libreville géomatique rapports cartographiques\",\n",
    "    \"Paris géomatique études de sol\",\n",
    "    \"New York géospatial data analysis\",\n",
    "    \"Tokyo geomatics data extraction\"\n",
    "]\n",
    "\n",
    "# Fonction pour rechercher des vidéos\n",
    "def search_videos(query, max_results=5):\n",
    "    \"\"\"\n",
    "    Recherche des vidéos sur YouTube via l'API Python.\n",
    "    \"\"\"\n",
    "    videos_search = VideosSearch(query, limit=max_results)\n",
    "    results = videos_search.result()[\"result\"]\n",
    "    videos = []\n",
    "    for video in results:\n",
    "        videos.append({\n",
    "            \"title\": video[\"title\"],\n",
    "            \"url\": video[\"link\"],\n",
    "            \"description\": video[\"descriptionSnippet\"][0][\"text\"] if video.get(\"descriptionSnippet\") else \"No description\",\n",
    "            \"city\": query.split()[0],  # Ajouter la ville depuis la requête\n",
    "            \"extraction_date\": datetime.now().strftime('%Y-%m-%d')  # Date d'extraction\n",
    "        })\n",
    "    return videos\n",
    "\n",
    "# Enregistrer les résultats dans un fichier CSV\n",
    "def save_videos_to_csv(videos, city_name):\n",
    "    \"\"\"\n",
    "    Sauvegarde des résultats dans un fichier CSV.\n",
    "    \"\"\"\n",
    "    csv_file = os.path.join(RESULTS_DIR, f\"{city_name}_videos.csv\")\n",
    "    with open(csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"title\", \"url\", \"description\", \"city\", \"extraction_date\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(videos)\n",
    "    print(f\"Résultats sauvegardés dans : {csv_file}\")\n",
    "\n",
    "# Enregistrer les résultats dans un fichier JSON\n",
    "def save_videos_to_json(videos, city_name):\n",
    "    \"\"\"\n",
    "    Sauvegarde des résultats dans un fichier JSON.\n",
    "    \"\"\"\n",
    "    json_file = os.path.join(RESULTS_DIR, f\"{city_name}_videos.json\")\n",
    "    with open(json_file, mode=\"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(videos, file, ensure_ascii=False, indent=2)\n",
    "    print(f\"Résultats sauvegardés dans : {json_file}\")\n",
    "\n",
    "# Recherche et sauvegarde des vidéos\n",
    "def search_and_save_videos(search_queries, max_results=5):\n",
    "    \"\"\"\n",
    "    Recherche et sauvegarde des vidéos pour chaque requête.\n",
    "    \"\"\"\n",
    "    for query in search_queries:\n",
    "        city_name = query.split()[0]  # Utiliser le premier mot comme nom de la ville\n",
    "        print(f\"Recherche de vidéos pour : {query}\")\n",
    "        videos = search_videos(query, max_results=max_results)\n",
    "        if videos:\n",
    "            save_videos_to_csv(videos, city_name)\n",
    "            save_videos_to_json(videos, city_name)\n",
    "        else:\n",
    "            print(f\"Aucune vidéo trouvée pour : {query}\")\n",
    "\n",
    "# Lancer la recherche\n",
    "search_and_save_videos(search_queries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger et Fusionner les CSV et JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Les études géotechniques nécessaires avant tou...   \n",
      "1  Ecole Internationale des Ponts et Chaussées d'...   \n",
      "2  LA TERRE C'EST POUR LES PAUVRES, ARCHITECTURE ...   \n",
      "3  Institut Universitaire d'Abidjan,L'université ...   \n",
      "4  ABIDJAN : Les travaux de construction de 3 nou...   \n",
      "\n",
      "                                         description  \\\n",
      "0  ... de sol on veut ériger cette fondation d'où...   \n",
      "1   L'École Internationale des Ponts et Chaussées d'   \n",
      "2  j'espère que vous avez aimé la vidéo !!! Nous ...   \n",
      "3                                     No description   \n",
      "4  Si vous êtes intéressé par un partenariat avec...   \n",
      "\n",
      "                                           url     city   type metadata  \n",
      "0  https://www.youtube.com/watch?v=KJxl3UOv-h4  Abidjan  video       {}  \n",
      "1  https://www.youtube.com/watch?v=bQnHde84SqY  Abidjan  video       {}  \n",
      "2  https://www.youtube.com/watch?v=6VHgMPIb2-c  Abidjan  video       {}  \n",
      "3  https://www.youtube.com/watch?v=pnZ3g9hjaiE  Abidjan  video       {}  \n",
      "4  https://www.youtube.com/watch?v=NsvwwYX9F-s  Abidjan  video       {}  \n",
      "Fichier fusionné sauvegardé dans : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\youtube_videos\\merged_videos_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Répertoire contenant les fichiers CSV et JSON\n",
    "DATA_DIR = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\youtube_videos\"\n",
    "\n",
    "# Liste des fichiers CSV et JSON\n",
    "csv_files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".csv\")]\n",
    "json_files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".json\")]\n",
    "\n",
    "# Fusionner les CSV avec les métadonnées JSON\n",
    "all_data = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    # Charger le CSV\n",
    "    csv_path = os.path.join(DATA_DIR, csv_file)\n",
    "    videos_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Associer les JSON correspondants\n",
    "    for _, row in videos_df.iterrows():\n",
    "        city = row.get(\"city\", \"\")\n",
    "        json_path = os.path.join(DATA_DIR, f\"{city}.json\")\n",
    "        metadata = {}\n",
    "\n",
    "        if os.path.exists(json_path):\n",
    "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "        \n",
    "        # Fusionner les données\n",
    "        all_data.append({\n",
    "            \"title\": row.get(\"title\", \"\"),\n",
    "            \"description\": row.get(\"description\", \"\"),\n",
    "            \"url\": row.get(\"url\", \"\"),\n",
    "            \"city\": row.get(\"city\", \"\"),\n",
    "            \"type\": \"video\",\n",
    "            \"metadata\": metadata\n",
    "        })\n",
    "\n",
    "# Convertir en DataFrame pour un traitement facile\n",
    "merged_df = pd.DataFrame(all_data)\n",
    "\n",
    "# Aperçu des données fusionnées\n",
    "print(merged_df.head())\n",
    "\n",
    "# Sauvegarder le DataFrame fusionné dans un fichier CSV local\n",
    "output_file = os.path.join(DATA_DIR, \"merged_videos_data.csv\")\n",
    "merged_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"Fichier fusionné sauvegardé dans : {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stockage ( storage) des données acquises Données  (textes normalisés , IMG , Données géospatiales OSM ,Vidéos éducatives et documentaires )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base centrale pour les métadonnées :\n",
    "  * une base commune  Elasticsearch pour stocker les métadonnées de tous les types de données.\n",
    "\tCette base  va liée les différentes sources de données (académiques, géospatiales, vidéos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (8.16.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from elasticsearch) (8.15.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2.2.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 02:41:21,597 - INFO - HEAD https://localhost:9200/ [status:200 duration:0.384s]\n",
      "2024-12-03 02:41:21,599 - INFO - Connexion à Elasticsearch réussie.\n",
      "2024-12-03 02:41:21,636 - INFO - HEAD https://localhost:9200/geosearch_metadata [status:200 duration:0.036s]\n",
      "2024-12-03 02:41:21,636 - INFO - L'index 'geosearch_metadata' existe déjà.\n",
      "2024-12-03 02:41:21,641 - INFO - Indexation des fichiers dans le répertoire : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\meta_doc_PDF\n",
      "2024-12-03 02:41:21,643 - WARNING - Le répertoire C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\meta_doc_PDF n'existe pas. Ignoré.\n",
      "2024-12-03 02:41:21,643 - INFO - Indexation des fichiers dans le répertoire : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_OSM\n",
      "2024-12-03 02:41:21,644 - WARNING - Le répertoire C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_OSM n'existe pas. Ignoré.\n",
      "2024-12-03 02:41:21,645 - INFO - Indexation des fichiers dans le répertoire : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_IMG\n",
      "2024-12-03 02:41:21,646 - WARNING - Le répertoire C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_IMG n'existe pas. Ignoré.\n",
      "2024-12-03 02:41:21,647 - INFO - Indexation des fichiers dans le répertoire : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_videos\n",
      "2024-12-03 02:41:21,648 - WARNING - Le répertoire C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_videos n'existe pas. Ignoré.\n",
      "2024-12-03 02:41:21,649 - INFO - Indexation des métadonnées terminée.\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from requests.exceptions import SSLError\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "# Configurer les logs\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Paramètres Elasticsearch\n",
    "ES_HOST = \"https://localhost:9200\"\n",
    "ES_USERNAME = \"elastic\"\n",
    "ES_PASSWORD = \"4fHbHMK5UaiGY3tzqeTJ\"  # Remplacez par vos identifiants\n",
    "INDEX_NAME = \"geosearch_metadata\"\n",
    "\n",
    "# Répertoires des métadonnées\n",
    "DIRECTORIES = {\n",
    "    \"meta_doc_PDF\": r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\meta_doc_PDF\",\n",
    "    \"Json_OSM\": r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_OSM\",\n",
    "    \"Json_IMG\": r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_IMG\",\n",
    "    \"Json_videos\": r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_videos\",\n",
    "}\n",
    "\n",
    "# Répertoire de sauvegarde local\n",
    "BACKUP_DIR = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Geosearch_Backup\"\n",
    "\n",
    "# Vérifiez et créez le répertoire de sauvegarde s'il n'existe pas\n",
    "if not os.path.exists(BACKUP_DIR):\n",
    "    os.makedirs(BACKUP_DIR)\n",
    "\n",
    "# Connexion sécurisée à Elasticsearch\n",
    "try:\n",
    "    es = Elasticsearch(\n",
    "        [ES_HOST],\n",
    "        basic_auth=(ES_USERNAME, ES_PASSWORD),\n",
    "        verify_certs=False,  # À éviter en production, ajoutez un certificat si possible\n",
    "    )\n",
    "    if es.ping():\n",
    "        logging.info(\"Connexion à Elasticsearch réussie.\")\n",
    "    else:\n",
    "        logging.error(\"Échec de la connexion à Elasticsearch. Vérifiez les paramètres.\")\n",
    "        exit()\n",
    "except SSLError as ssl_error:\n",
    "    logging.error(f\"Erreur SSL : {ssl_error}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erreur de connexion à Elasticsearch : {e}\")\n",
    "    exit()\n",
    "\n",
    "# Création de l'index avec mapping\n",
    "try:\n",
    "    if not es.indices.exists(index=INDEX_NAME):\n",
    "        es.indices.create(index=INDEX_NAME, body={\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"category\": {\"type\": \"keyword\"},\n",
    "                    \"file_path\": {\"type\": \"text\"},\n",
    "                    \"metadata\": {\"type\": \"object\"},\n",
    "                    \"location\": {\"type\": \"geo_point\"}  # Optionnel pour les données géospatiales\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "        logging.info(f\"Index '{INDEX_NAME}' créé avec succès.\")\n",
    "    else:\n",
    "        logging.info(f\"L'index '{INDEX_NAME}' existe déjà.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erreur lors de la création de l'index '{INDEX_NAME}': {e}\")\n",
    "    exit()\n",
    "\n",
    "# Fonction pour charger, indexer les fichiers JSON, et sauvegarder localement\n",
    "def load_metadata(directory, category):\n",
    "    if not os.path.exists(directory):\n",
    "        logging.warning(f\"Le répertoire {directory} n'existe pas. Ignoré.\")\n",
    "        return\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.json'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    # Préparation des données pour Elasticsearch\n",
    "                    doc = {\n",
    "                        \"category\": category,\n",
    "                        \"file_path\": file_path,\n",
    "                        \"metadata\": data\n",
    "                    }\n",
    "                    # Ajout de champs géospatiaux si disponibles\n",
    "                    if \"latitude\" in data and \"longitude\" in data:\n",
    "                        doc[\"location\"] = {\"lat\": data[\"latitude\"], \"lon\": data[\"longitude\"]}\n",
    "\n",
    "                    # Indexation dans Elasticsearch\n",
    "                    es.index(index=INDEX_NAME, body=doc)\n",
    "                    logging.info(f\"Fichier indexé : {file}\")\n",
    "\n",
    "                    # Sauvegarder une copie locale dans le répertoire BACKUP_DIR\n",
    "                    category_backup_dir = os.path.join(BACKUP_DIR, category)\n",
    "                    if not os.path.exists(category_backup_dir):\n",
    "                        os.makedirs(category_backup_dir)\n",
    "                    shutil.copy(file_path, category_backup_dir)\n",
    "                    logging.info(f\"Fichier sauvegardé localement : {file_path}\")\n",
    "            except json.JSONDecodeError:\n",
    "                logging.error(f\"Erreur JSON dans le fichier : {file}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Erreur lors du traitement du fichier {file}: {e}\")\n",
    "\n",
    "# Indexer les fichiers dans chaque répertoire\n",
    "for category, directory in DIRECTORIES.items():\n",
    "    logging.info(f\"Indexation des fichiers dans le répertoire : {directory}\")\n",
    "    load_metadata(directory, category)\n",
    "\n",
    "logging.info(\"Indexation des métadonnées terminée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "2024-11-29 00:39:54,280 - INFO - HEAD https://localhost:9200/geosearch_metadata [status:200 duration:0.004s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'index 'geosearch_metadata' existe.\n"
     ]
    }
   ],
   "source": [
    "# Vérifiez si l'index existe\n",
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    print(f\"L'index '{INDEX_NAME}' existe.\")\n",
    "else:\n",
    "    print(f\"L'index '{INDEX_NAME}' n'existe pas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "2024-11-29 00:40:09,538 - INFO - POST https://localhost:9200/geosearch_metadata/_search [status:200 duration:9.904s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents indexés :\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Abidjan_doc_1_metadata.json', 'metadata': {'title': '', 'author': 'HP', 'creation_date': \"D:20220905171359+00'00'\", 'modification_date': \"D:20220905171359+00'00'\", 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Abidjan_doc_2_metadata.json', 'metadata': {'title': 'World Bank Document', 'author': 'World Bank Group', 'creation_date': \"D:20190318032621-04'00'\", 'modification_date': 'D:20190318072648', 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Abidjan_doc_3_metadata.json', 'metadata': {'title': '', 'author': '清水研', 'creation_date': \"D:20190312105944+09'00'\", 'modification_date': \"D:20190516171006+09'00'\", 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Bamako_doc_1_metadata.json', 'metadata': {'title': 'PRIMATURE', 'author': 'dougoucolo konare', 'creation_date': 'D:20080818170321Z', 'modification_date': \"D:20220228192811+01'00'\", 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Bamako_doc_2_metadata.json', 'metadata': {'title': 'Microsoft Word - PNAEP TEXTE V.FINALE corrigée.DOC', 'author': 'Administrateur', 'creation_date': \"D:20041126105856+01'00'\", 'modification_date': \"D:20041126105856+01'00'\", 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Bamako_doc_3_metadata.json', 'metadata': {'title': 'PEMU', 'author': 'Veronique Verdeil', 'creation_date': \"D:20181130081348-05'00'\", 'modification_date': \"D:20181130081348-05'00'\", 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Cotonou_doc_1_metadata.json', 'metadata': {'title': '', 'author': 'AKPO Eric Ayedesso (PAC)', 'creation_date': \"D:20230830155225+01'00'\", 'modification_date': \"D:20230830155342+01'00'\", 'subject': '', 'keywords': ', docId:B80E961F3383D1631ACD76CDEDCE6B6A'}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Cotonou_doc_2_metadata.json', 'metadata': {'title': \"Projet de renforcement et de réhabilitation du réseau de distribution d'électricité au niveau régional et à Cotonou\", 'author': 'Millennium Challenge Corporation', 'creation_date': \"D:20211026180014-04'00'\", 'modification_date': \"D:20220113143552-05'00'\", 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Cotonou_doc_3_metadata.json', 'metadata': {'title': '', 'author': 'AKPO Eric Ayedesso (PAC)', 'creation_date': \"D:20230825164223+01'00'\", 'modification_date': \"D:20230830155319+01'00'\", 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Libreville_doc_1_metadata.json', 'metadata': {'title': '', 'author': '', 'creation_date': \"D:20230424152155+01'00'\", 'modification_date': \"D:20240207155538-01'00'\", 'subject': '', 'keywords': ''}}\n"
     ]
    }
   ],
   "source": [
    "response = es.search(index=INDEX_NAME, body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Documents indexés :\")\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit[\"_source\"])  # Affiche les données indexées\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Construction des vecteurs à partir des données textuelles (PDF fractionnés )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration de Hugging Face avec le Token mise à notre disposition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion à Hugging Face réussie.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Connectez-vous à Hugging Face avec votre token\n",
    "login(token=\"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\")\n",
    "print(\"Connexion à Hugging Face réussie.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les segments PDF  doivent être transformés en vecteurs en utilisant un modèle pré-entraîné ( sentence-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 02:43:29,935 - INFO - Use pytorch device_name: cpu\n",
      "2024-12-03 02:43:29,935 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc40352985274a258d9c58278e51d7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c62d9e3201436085b33ffa6727fb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5914eaa8c3dc48349972adf40b538c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40e1bd4a9d441c8aeed3990a3e71993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0899f17f394746a3b120236e89b993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1dd23c8ec6547f5b08694a9a679ff25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbffdd526de459e9a1649fded6b5708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7cf30a7d3d483fbb17c883e5d9fc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d6a6ec4682461e99517167e98c2233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8653f6b817f149d4ac5fdc3fd85a9825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b2bc2c042942afb0a81df8f990cbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f001694da984f0182b5d258a0ffcce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6484a0ea634b99b70ca5cc507c758a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7d340071274880a8785adf1b6c361b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f84bf996734a6db0a8f4daf132b9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a3716d388c4cb295c026f109818e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add1f91b297245118b64c5a22e880552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e088a5d18a64954b45b88ce9b1299ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5659da027d48928f50fb38894d2d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de vecteurs encodés : 19\n",
      "Vecteurs sauvegardés dans : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Encoded_Files\\vectors.npy\n",
      "Métadonnées sauvegardées dans : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Encoded_Files\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Charger le modèle Sentence-BERT\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Répertoire contenant les fichiers PDF fractionnés\n",
    "FRACTIONED_DIR = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Avant\\Frac_pdf\"\n",
    "\n",
    "# Répertoire pour sauvegarder les vecteurs et métadonnées\n",
    "OUTPUT_DIR = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Encoded_Files\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Listes pour stocker les vecteurs et métadonnées\n",
    "vectors = []\n",
    "metadata = []\n",
    "\n",
    "# Parcourir les fichiers PDF fractionnés et encoder les segments\n",
    "for pdf_file in os.listdir(FRACTIONED_DIR):\n",
    "    if pdf_file.endswith('.txt'):  # Les segments fractionnés devraient être en .txt (text)\n",
    "        file_path = os.path.join(FRACTIONED_DIR, pdf_file)\n",
    "        \n",
    "        # Lire le contenu textuel du segment\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            segment_content = f.read().strip()\n",
    "            if segment_content:\n",
    "                # Encoder le texte en vecteur\n",
    "                vector = model.encode(segment_content)\n",
    "                vectors.append(vector)\n",
    "\n",
    "                # Ajouter les métadonnées\n",
    "                pdf_metadata = {\n",
    "                    \"file_name\": pdf_file,\n",
    "                    \"segment_content\": segment_content,\n",
    "                    \"vector\": vector.tolist()  # Inclure le vecteur encodé\n",
    "                }\n",
    "                metadata.append(pdf_metadata)\n",
    "\n",
    "                # Sauvegarder chaque fichier encodé dans un fichier JSON séparé\n",
    "                output_file = os.path.join(OUTPUT_DIR, f\"{os.path.splitext(pdf_file)[0]}_encoded.json\")\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "                    json.dump(pdf_metadata, out_f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Convertir les vecteurs en matrice NumPy\n",
    "vectors_np = np.array(vectors, dtype='float32')\n",
    "\n",
    "# Afficher le nombre de vecteurs encodés\n",
    "print(f\"Nombre de vecteurs encodés : {len(vectors_np)}\")\n",
    "\n",
    "# Sauvegarder les vecteurs dans un fichier NumPy\n",
    "vectors_file = os.path.join(OUTPUT_DIR, \"vectors.npy\")\n",
    "np.save(vectors_file, vectors_np)\n",
    "\n",
    "# Sauvegarder les métadonnées dans un fichier JSON\n",
    "metadata_file = os.path.join(OUTPUT_DIR, \"metadata.json\")\n",
    "with open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Vecteurs sauvegardés dans : {vectors_file}\")\n",
    "print(f\"Métadonnées sauvegardées dans : {metadata_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stockage des vecteurs dans FAISS (Utiliser FAISS pour stocker et rechercher des vecteurs similaires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hachage recalculé : 61d2256ac9c8fd8d28d384af47e9c087193beca94f3e3a420b92dde593ca27d2\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def calculate_hash(file_path):\n",
    "    sha256 = hashlib.sha256()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        while chunk := f.read(8192):\n",
    "            sha256.update(chunk)\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "FAISS_INDEX_PATH = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Encoded_Files\\faiss_index.bin\"\n",
    "current_hash = calculate_hash(FAISS_INDEX_PATH)\n",
    "print(f\"Hachage recalculé : {current_hash}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouvel index FAISS sauvegardé dans : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Encoded_Files\\faiss_index.bin\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Chemin vers les vecteurs et le fichier de sortie\n",
    "VECTORS_FILE = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Encoded_Files\\vectors.npy\"\n",
    "FAISS_INDEX_PATH = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Encoded_Files\\faiss_index.bin\"\n",
    "\n",
    "# Charger les vecteurs\n",
    "vectors_np = np.load(VECTORS_FILE)\n",
    "\n",
    "# Initialiser l'index FAISS\n",
    "dimension = vectors_np.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(vectors_np)\n",
    "\n",
    "# Sauvegarder l'index\n",
    "faiss.write_index(index, FAISS_INDEX_PATH)\n",
    "print(f\"Nouvel index FAISS sauvegardé dans : {FAISS_INDEX_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vecteurs chargés : (19, 384)\n",
      "19 vecteurs ajoutés à l'index FAISS.\n",
      "Index FAISS sauvegardé dans : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Encoded_Files\\faiss_index.bin\n",
      "Métadonnées sauvegardées dans : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Encoded_Files\\metadata_faiss.json\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Chemin du fichier de vecteurs et répertoire de sortie\n",
    "VECTORS_FILE = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Encoded_Files\\vectors.npy\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Encoded_Files\"\n",
    "\n",
    "# Vérifiez que le fichier de vecteurs existe\n",
    "if not os.path.exists(VECTORS_FILE):\n",
    "    raise FileNotFoundError(f\"Le fichier vecteurs {VECTORS_FILE} est introuvable.\")\n",
    "vectors_np = np.load(VECTORS_FILE)\n",
    "print(f\"Vecteurs chargés : {vectors_np.shape}\")\n",
    "\n",
    "# Vérifiez et créez le répertoire de sortie si nécessaire\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"Répertoire créé : {OUTPUT_DIR}\")\n",
    "\n",
    "# Initialiser un index FAISS basé sur la distance euclidienne (L2)\n",
    "dimension = vectors_np.shape[1]  # Taille des vecteurs (par exemple, 384 pour MiniLM)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Ajouter les vecteurs à l'index FAISS\n",
    "index.add(vectors_np)\n",
    "print(f\"{index.ntotal} vecteurs ajoutés à l'index FAISS.\")\n",
    "\n",
    "# Sauvegarder l'index FAISS dans le répertoire de sortie\n",
    "faiss_index_file = os.path.join(OUTPUT_DIR, \"faiss_index.bin\")\n",
    "try:\n",
    "    faiss.write_index(index, faiss_index_file)\n",
    "    print(f\"Index FAISS sauvegardé dans : {faiss_index_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de la sauvegarde de l'index FAISS : {e}\")\n",
    "\n",
    "# Sauvegarder les métadonnées (optionnel)\n",
    "metadata_file = os.path.join(OUTPUT_DIR, \"metadata.json\")\n",
    "if os.path.exists(metadata_file):\n",
    "    with open(metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "    metadata_faiss_file = os.path.join(OUTPUT_DIR, \"metadata_faiss.json\")\n",
    "    with open(metadata_faiss_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Métadonnées sauvegardées dans : {metadata_faiss_file}\")\n",
    "else:\n",
    "    print(\"Aucune métadonnée trouvée pour être sauvegardée avec FAISS.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Chargement et Connexion aux Bases de Données ( FAISS , Elasticsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier vecteurs existe : True\n",
      "Fichier index FAISS existe : True\n",
      "Fichier métadonnées existe : True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "FAISS_DIR = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Encoded_Files\"\n",
    "vectors_file = os.path.join(FAISS_DIR, \"vectors.npy\")\n",
    "faiss_index_file = os.path.join(FAISS_DIR, \"faiss_index.bin\")\n",
    "metadata_file = os.path.join(FAISS_DIR, \"metadata_faiss.json\")\n",
    "\n",
    "print(f\"Fichier vecteurs existe : {os.path.exists(vectors_file)}\")\n",
    "print(f\"Fichier index FAISS existe : {os.path.exists(faiss_index_file)}\")\n",
    "print(f\"Fichier métadonnées existe : {os.path.exists(metadata_file)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 11:12:48,204 - INFO - HEAD https://localhost:9200/ [status:200 duration:0.151s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion à Elasticsearch réussie.\n",
      "Index FAISS chargé avec succès.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Répertoires\n",
    "ELASTICSEARCH_DIR = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Elastic_search_BD\"\n",
    "FAISS_DIR = r\"C:\\FAISS\"\n",
    "\n",
    "# Configurer Elasticsearch\n",
    "ES_HOST = \"https://localhost:9200\"\n",
    "ES_USERNAME = \"elastic\"\n",
    "ES_PASSWORD = \"4fHbHMK5UaiGY3tzqeTJ\"  # Remplacez par vos identifiants\n",
    "INDEX_NAME = \"geosearch_metadata\"\n",
    "\n",
    "# Connexion sécurisée à Elasticsearch\n",
    "es = Elasticsearch(\n",
    "    [ES_HOST],\n",
    "    basic_auth=(ES_USERNAME, ES_PASSWORD),\n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "# Vérifier la connexion\n",
    "if es.ping():\n",
    "    print(\"Connexion à Elasticsearch réussie.\")\n",
    "else:\n",
    "    raise ConnectionError(\"Impossible de se connecter à Elasticsearch.\")\n",
    "\n",
    "# Charger l'index FAISS\n",
    "vectors_file = os.path.join(FAISS_DIR, \"vectors.npy\")\n",
    "faiss_index_file = os.path.join(FAISS_DIR, \"faiss_index.bin\")\n",
    "metadata_file = os.path.join(FAISS_DIR, \"metadata_faiss.json\")\n",
    "\n",
    "if not (os.path.exists(vectors_file) and os.path.exists(faiss_index_file) and os.path.exists(metadata_file)):\n",
    "    raise FileNotFoundError(\"Un ou plusieurs fichiers nécessaires à FAISS sont introuvables.\")\n",
    "\n",
    "vectors = np.load(vectors_file)\n",
    "index = faiss.read_index(faiss_index_file)\n",
    "\n",
    "with open(metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(\"Index FAISS chargé avec succès.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Recherche et Récupération des Informations :\n",
    "\n",
    " * Effectuez une recherche dans Elasticsearch pour récupérer les métadonnées liées à des mots-clés.\n",
    " * Effectuez une recherche dans l’espace vectoriel pour trouver les PDF encodés pertinents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_elasticsearch(query, index=INDEX_NAME, size=10):\n",
    "    \"\"\"\n",
    "    Recherche dans Elasticsearch en utilisant une requête textuelle.\n",
    "    \"\"\"\n",
    "    body = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [\"metadata.*\", \"category\", \"file_path\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = es.search(index=index, body=body, size=size)\n",
    "    hits = response['hits']['hits']\n",
    "    return [hit['_source'] for hit in hits]\n",
    "\n",
    "\n",
    "def search_in_faiss(query, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Recherche vectorielle dans FAISS.\n",
    "    \"\"\"\n",
    "    query_vector = model.encode(query).astype(\"float32\")\n",
    "    distances, indices = index.search(np.array([query_vector]), top_k)\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        if idx != -1:\n",
    "            results.append({\n",
    "                \"distance\": dist,\n",
    "                \"metadata\": metadata[idx]\n",
    "            })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase Retrieval -generateur- Reponse generée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS(\n",
    "    embedding_function=hf_embeddings,  # Passer l'objet embeddings, pas la fonction\n",
    "    index=faiss_index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion à Hugging Face réussie.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\elasticsearch\\_sync\\client\\__init__.py:402: SecurityWarning: Connecting to 'https://localhost:9200' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n",
      "C:\\Users\\ec\\AppData\\Local\\Temp\\ipykernel_20064\\488494990.py:29: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
      "c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse générée : Repondez à la demande suivante : Texte du document 2 Texte du document 1\n",
      "\n",
      "Résultats FAISS :\n",
      "- Contenu : Texte du document 2\n",
      "  Source : doc2.pdf\n",
      "- Contenu : Texte du document 1\n",
      "  Source : doc1.pdf\n",
      "\n",
      "Résultats Elasticsearch :\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from huggingface_hub import login\n",
    "import faiss\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Connexion à Hugging Face\n",
    "login(token=\"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\")\n",
    "print(\"Connexion à Hugging Face réussie.\")\n",
    "\n",
    "# Étape 1 : Configurer Elasticsearch\n",
    "ES_HOST = \"https://localhost:9200\"\n",
    "ES_USERNAME = \"elastic\"\n",
    "ES_PASSWORD = \"4fHbHMK5UaiGY3tzqeTJ\"\n",
    "INDEX_NAME = \"geosearch_metadata\"\n",
    "\n",
    "es_client = Elasticsearch(\n",
    "    [ES_HOST],\n",
    "    basic_auth=(ES_USERNAME, ES_PASSWORD),\n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "# Charger les embeddings Hugging Face\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Charger l'index FAISS\n",
    "FAISS_INDEX_PATH = r\"C:\\FAISS\\faiss_index.bin\"\n",
    "faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "\n",
    "# Recréer l'index FAISS pour synchroniser avec les documents\n",
    "documents = [\n",
    "    Document(page_content=\"Texte du document 1\", metadata={\"source\": \"doc1.pdf\"}),\n",
    "    Document(page_content=\"Texte du document 2\", metadata={\"source\": \"doc2.pdf\"})\n",
    "]\n",
    "valid_vectors = [faiss_index.reconstruct(i) for i in range(len(documents))]\n",
    "new_faiss_index = faiss.IndexFlatL2(faiss_index.d)\n",
    "new_faiss_index.add(np.array(valid_vectors))\n",
    "faiss_index = new_faiss_index\n",
    "\n",
    "# Synchroniser docstore et mappage\n",
    "docstore = InMemoryDocstore({str(i): documents[i] for i in range(len(documents))})\n",
    "index_to_docstore_id = {i: str(i) for i in range(len(documents))}\n",
    "\n",
    "vectorstore = FAISS(\n",
    "    embedding_function=hf_embeddings,  # Passer l'objet HuggingFaceEmbeddings\n",
    "    index=faiss_index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id\n",
    ")\n",
    "\n",
    "# Classe ElasticsearchRetriever\n",
    "class ElasticsearchRetriever:\n",
    "    def __init__(self, client, index_name):\n",
    "        self.client = client\n",
    "        self.index_name = index_name\n",
    "\n",
    "    def retrieve(self, query, top_k=5):\n",
    "        response = self.client.search(\n",
    "            index=self.index_name,\n",
    "            body={\n",
    "                \"query\": {\"match\": {\"metadata\": query}},\n",
    "                \"size\": top_k\n",
    "            }\n",
    "        )\n",
    "        documents = []\n",
    "        for hit in response[\"hits\"][\"hits\"]:\n",
    "            doc = Document(\n",
    "                page_content=hit[\"_source\"].get(\"metadata\", \"\"),\n",
    "                metadata={\"source\": hit[\"_source\"].get(\"file_path\", \"Inconnu\")}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        return documents\n",
    "\n",
    "elasticsearch_retriever = ElasticsearchRetriever(es_client, INDEX_NAME)\n",
    "\n",
    "# Charger un modèle Hugging Face pour la génération\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "generation_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Chaîne combinée Elasticsearch + FAISS\n",
    "def combined_retrieval(query, top_k=5):\n",
    "    faiss_docs = vectorstore.similarity_search(query, k=top_k)\n",
    "    es_docs = elasticsearch_retriever.retrieve(query, top_k=top_k)\n",
    "    combined_docs = {\"faiss\": faiss_docs, \"elasticsearch\": es_docs}\n",
    "    return combined_docs\n",
    "\n",
    "# Répondre à une requête\n",
    "query = \"Impact du changement climatique sur les zones côtières\"\n",
    "retrieved_docs = combined_retrieval(query)\n",
    "\n",
    "# Générer une réponse\n",
    "faiss_content = \"\\n\".join([doc.page_content for doc in retrieved_docs[\"faiss\"]])\n",
    "if faiss_content.strip():\n",
    "    generation_input = f\"Répondez à la requête suivante en utilisant les documents : {faiss_content}\"\n",
    "    response = generation_pipeline(generation_input, max_length=200)[0][\"generated_text\"]\n",
    "else:\n",
    "    response = \"Aucun contenu trouvé dans les documents FAISS pour répondre à cette requête.\"\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Réponse générée :\", response)\n",
    "print(\"\\nRésultats FAISS :\")\n",
    "for doc in retrieved_docs[\"faiss\"]:\n",
    "    print(f\"- Contenu : {doc.page_content}\")\n",
    "    print(f\"  Source : {doc.metadata.get('source', 'Inconnu')}\")\n",
    "print(\"\\nRésultats Elasticsearch :\")\n",
    "for doc in retrieved_docs[\"elasticsearch\"]:\n",
    "    print(f\"- Contenu : {doc.page_content}\")\n",
    "    print(f\"  Source : {doc.metadata.get('source', 'Inconnu')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bases spécialisées pour les données volumineuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de la base de données et activation de PostGIS IMG et OSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base de donnée OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La base de données 'geosearch_data' existe déjà.\n",
      "Extensions PostGIS activées avec succès.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "# Connexion à PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Malika2000\",  \n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Création de la base de données pour GeoSearch\n",
    "try:\n",
    "    cursor.execute(\"CREATE DATABASE geosearch_data;\")\n",
    "    print(\"Base de données 'geosearch_data' créée avec succès.\")\n",
    "except psycopg2.errors.DuplicateDatabase:\n",
    "    print(\"La base de données 'geosearch_data' existe déjà.\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Connexion à la nouvelle base pour activer PostGIS\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"geosearch_data\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Malika2000\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Activation des extensions PostGIS\n",
    "try:\n",
    "    cursor.execute(\"CREATE EXTENSION IF NOT EXISTS postgis;\")\n",
    "    cursor.execute(\"CREATE EXTENSION IF NOT EXISTS postgis_topology;\")\n",
    "    print(\"Extensions PostGIS activées avec succès.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur : {e}\")\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des données géospatiales dans PostGIS\n",
    "- Chargement des fichiers GeoJSON OpenStreetMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importation réussie : Abidjan_amenity.geojson dans la table Abidjan_Abidjan_amenity\n",
      "Importation réussie : Abidjan_building.geojson dans la table Abidjan_Abidjan_building\n",
      "Importation réussie : Abidjan_highway.geojson dans la table Abidjan_Abidjan_highway\n",
      "Importation réussie : Bamako_amenity.geojson dans la table Bamako_Bamako_amenity\n",
      "Importation réussie : Bamako_building.geojson dans la table Bamako_Bamako_building\n",
      "Importation réussie : Bamako_highway.geojson dans la table Bamako_Bamako_highway\n",
      "Importation réussie : Cotonou_amenity.geojson dans la table Cotonou_Cotonou_amenity\n",
      "Importation réussie : Cotonou_building.geojson dans la table Cotonou_Cotonou_building\n",
      "Importation réussie : Cotonou_highway.geojson dans la table Cotonou_Cotonou_highway\n",
      "Importation réussie : Libreville_amenity.geojson dans la table Libreville_Libreville_amenity\n",
      "Importation réussie : Libreville_building.geojson dans la table Libreville_Libreville_building\n",
      "Importation réussie : Libreville_highway.geojson dans la table Libreville_Libreville_highway\n",
      "Importation réussie : New_York_amenity.geojson dans la table New_York_New_York_amenity\n",
      "Importation réussie : New_York_building.geojson dans la table New_York_New_York_building\n",
      "Importation réussie : New_York_highway.geojson dans la table New_York_New_York_highway\n",
      "Importation réussie : Paris_amenity.geojson dans la table Paris_Paris_amenity\n",
      "Importation réussie : Paris_building.geojson dans la table Paris_Paris_building\n",
      "Importation réussie : Paris_highway.geojson dans la table Paris_Paris_highway\n",
      "Importation réussie : Tokyo_amenity.geojson dans la table Tokyo_Tokyo_amenity\n",
      "Importation réussie : Tokyo_building.geojson dans la table Tokyo_Tokyo_building\n",
      "Importation réussie : Tokyo_highway.geojson dans la table Tokyo_Tokyo_highway\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Paramètres de connexion à la base de données PostgreSQL/PostGIS\n",
    "DB_NAME = \"geosearch_data\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"Malika2000\"\n",
    "BASE_DIR = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\osm_data\"\n",
    "\n",
    "# Fonction pour charger les GeoJSON dans PostGIS\n",
    "def import_geojson_to_postgis(directory):\n",
    "    for root, dirs, files in os.walk(directory):  # Parcours récursif des sous-dossiers\n",
    "        for file in files:\n",
    "            if file.endswith(\".geojson\"):  # Filtrer uniquement les fichiers GeoJSON\n",
    "                # Récupérer le nom de la ville depuis le dossier parent\n",
    "                city_name = os.path.basename(root)\n",
    "                # Nom de la table dans PostGIS basé sur le nom de la ville et du fichier\n",
    "                table_name = f\"{city_name}_{os.path.splitext(file)[0]}\"\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Commande ogr2ogr pour importer dans PostGIS\n",
    "                command = [\n",
    "                    \"ogr2ogr\",\n",
    "                    \"-f\", \"PostgreSQL\",\n",
    "                    f\"PG:dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD}\",\n",
    "                    file_path,\n",
    "                    \"-nln\", table_name,  # Spécifier le nom de la table\n",
    "                    \"-overwrite\"  # Écraser la table si elle existe déjà\n",
    "                ]\n",
    "                try:\n",
    "                    # Exécuter la commande\n",
    "                    subprocess.run(command, check=True)\n",
    "                    print(f\"Importation réussie : {file} dans la table {table_name}\")\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    print(f\"Erreur lors de l'importation de {file} : {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur inattendue pour {file} : {e}\")\n",
    "\n",
    "# Appeler la fonction pour charger les fichiers GeoJSON\n",
    "import_geojson_to_postgis(BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création de la carte pour abidjan...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\abidjan\\abidjan_map.html\n",
      "Création de la carte pour bamako...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\bamako\\bamako_map.html\n",
      "Création de la carte pour cotonou...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\cotonou\\cotonou_map.html\n",
      "Création de la carte pour libreville...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\libreville\\libreville_map.html\n",
      "Création de la carte pour paris...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\paris\\paris_map.html\n",
      "Création de la carte pour tokyo...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\tokyo\\tokyo_map.html\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Paramètres PostgreSQL/PostGIS\n",
    "DB_NAME = \"geosearch_data_finale\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"Malika2000\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "# Chemin de base pour les cartes\n",
    "BASE_DIR = r\"D:\\GeoSearch_Data\"\n",
    "\n",
    "# Fonction pour récupérer les données depuis PostGIS\n",
    "def fetch_data_from_postgis(city, layer):\n",
    "    table_name = f\"{city}_{city}_{layer}\"\n",
    "    query = f\"\"\"\n",
    "        SELECT ST_AsGeoJSON(wkb_geometry) AS geojson\n",
    "        FROM {table_name}\n",
    "        WHERE wkb_geometry IS NOT NULL\n",
    "        LIMIT 5000;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, host=DB_HOST, port=DB_PORT\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la récupération des données pour {table_name} : {e}\")\n",
    "        return []\n",
    "\n",
    "# Fonction pour ajouter une légende HTML\n",
    "def add_legend(map_object):\n",
    "    legend_html = '''\n",
    "     <div style=\"position: fixed; \n",
    "                 bottom: 50px; left: 50px; width: 250px; height: 150px; \n",
    "                 background-color: white; z-index:9999; font-size:14px;\n",
    "                 border:2px solid grey; padding: 10px;\">\n",
    "         <b>Légende :</b><br>\n",
    "         <i style=\"background: blue; width: 10px; height: 10px; display: inline-block;\"></i> Routes (highway)<br>\n",
    "         <i style=\"background: red; width: 10px; height: 10px; display: inline-block;\"></i> Bâtiments (building)<br>\n",
    "         <i style=\"background: green; width: 10px; height: 10px; display: inline-block;\"></i> Équipements (amenity)<br>\n",
    "     </div>\n",
    "     '''\n",
    "    map_object.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Fonction pour créer une carte et ajouter les données OSM\n",
    "def create_map_with_osm_data(city, base_dir):\n",
    "    try:\n",
    "        # Coordonnées par défaut pour centrer la carte\n",
    "        city_locations = {\n",
    "            \"abidjan\": [5.316667, -4.033333],\n",
    "            \"bamako\": [12.639232, -8.002889],\n",
    "            \"cotonou\": [6.370293, 2.391236],\n",
    "            \"libreville\": [0.416198, 9.467268],\n",
    "            \"paris\": [48.8566, 2.3522],\n",
    "            \"tokyo\": [35.6895, 139.6917],\n",
    "        }\n",
    "        city_location = city_locations.get(city.lower(), [0, 0])\n",
    "\n",
    "        # Créer une carte Folium\n",
    "        m = folium.Map(location=city_location, zoom_start=12)\n",
    "\n",
    "        # Ajouter les données OSM pour chaque type\n",
    "        for layer, color in zip([\"highway\", \"building\", \"amenity\"], [\"blue\", \"red\", \"green\"]):\n",
    "            data = fetch_data_from_postgis(city, layer)\n",
    "            if data:\n",
    "                feature_group = folium.FeatureGroup(name=f\"{layer.capitalize()} ({city})\")\n",
    "                for row in data:\n",
    "                    geojson = row[0]  # GeoJSON\n",
    "                    folium.GeoJson(\n",
    "                        geojson,\n",
    "                        style_function=lambda x, color=color: {'color': color, 'weight': 2},\n",
    "                    ).add_to(feature_group)\n",
    "                feature_group.add_to(m)\n",
    "\n",
    "        # Ajouter la légende\n",
    "        add_legend(m)\n",
    "\n",
    "        # Ajouter le contrôle des couches\n",
    "        folium.LayerControl().add_to(m)\n",
    "\n",
    "        # Sauvegarder la carte\n",
    "        output_path = os.path.join(base_dir, city, f\"{city}_map.html\")\n",
    "        if not os.path.exists(os.path.dirname(output_path)):\n",
    "            os.makedirs(os.path.dirname(output_path))\n",
    "        m.save(output_path)\n",
    "        print(f\"Carte générée et sauvegardée : {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création de la carte pour {city} : {e}\")\n",
    "\n",
    "# Générer les cartes pour toutes les villes\n",
    "def generate_maps_for_all_cities(base_dir):\n",
    "    cities = [\"abidjan\", \"bamako\", \"cotonou\", \"libreville\", \"paris\", \"tokyo\"]\n",
    "    for city in cities:\n",
    "        print(f\"Création de la carte pour {city}...\")\n",
    "        create_map_with_osm_data(city, base_dir)\n",
    "\n",
    "# Exécuter la génération des cartes\n",
    "generate_maps_for_all_cities(BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création de la carte pour abidjan...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\abidjan\\abidjan_map.html\n",
      "Création de la carte pour bamako...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\bamako\\bamako_map.html\n",
      "Création de la carte pour cotonou...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\cotonou\\cotonou_map.html\n",
      "Création de la carte pour libreville...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\libreville\\libreville_map.html\n",
      "Création de la carte pour paris...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\paris\\paris_map.html\n",
      "Création de la carte pour tokyo...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\tokyo\\tokyo_map.html\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Paramètres PostgreSQL/PostGIS\n",
    "DB_NAME = \"geosearch_data_finale\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"Malika2000\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "# Chemin de base pour les cartes\n",
    "BASE_DIR = r\"D:\\GeoSearch_Data\"\n",
    "\n",
    "# Fonction pour récupérer les données depuis PostGIS\n",
    "def fetch_data_from_postgis(city, layer):\n",
    "    table_name = f\"{city}_{city}_{layer}\"\n",
    "    query = f\"\"\"\n",
    "        SELECT ST_AsGeoJSON(wkb_geometry) AS geojson, name\n",
    "        FROM {table_name}\n",
    "        WHERE wkb_geometry IS NOT NULL\n",
    "        LIMIT 5000;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, host=DB_HOST, port=DB_PORT\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la récupération des données pour {table_name} : {e}\")\n",
    "        return []\n",
    "\n",
    "# Fonction pour créer une carte de base et ajouter des données OSM\n",
    "def create_map_with_osm_data(city, base_dir):\n",
    "    try:\n",
    "        # Définir un emplacement central pour chaque ville (coordonnées à personnaliser)\n",
    "        city_locations = {\n",
    "            \"abidjan\": [5.316667, -4.033333],\n",
    "            \"bamako\": [12.639232, -8.002889],\n",
    "            \"cotonou\": [6.370293, 2.391236],\n",
    "            \"libreville\": [0.416198, 9.467268],\n",
    "            \"paris\": [48.8566, 2.3522],\n",
    "            \"tokyo\": [35.6895, 139.6917],\n",
    "        }\n",
    "        city_location = city_locations.get(city.lower(), [0, 0])  # Par défaut : [0, 0]\n",
    "\n",
    "        # Créer une carte Folium centrée sur la ville\n",
    "        m = folium.Map(location=city_location, zoom_start=12)\n",
    "\n",
    "        # Ajouter les données OSM pour chaque type (highway, building, amenity)\n",
    "        for layer, color in zip([\"highway\", \"building\", \"amenity\"], [\"blue\", \"red\", \"green\"]):\n",
    "            data = fetch_data_from_postgis(city, layer)\n",
    "            if data:\n",
    "                for row in data:\n",
    "                    geojson = row[0]  # GeoJSON\n",
    "                    name = row[1] or \"Inconnu\"  # Nom ou défaut\n",
    "                    folium.GeoJson(\n",
    "                        geojson,\n",
    "                        name=f\"{layer} ({city})\",\n",
    "                        style_function=lambda x: {'color': color},\n",
    "                        tooltip=folium.Tooltip(text=name)\n",
    "                    ).add_to(m)\n",
    "\n",
    "        # Ajouter le contrôle des couches\n",
    "        folium.LayerControl().add_to(m)\n",
    "\n",
    "        # Sauvegarder la carte\n",
    "        output_path = os.path.join(base_dir, city, f\"{city}_map.html\")\n",
    "        if not os.path.exists(os.path.dirname(output_path)):\n",
    "            os.makedirs(os.path.dirname(output_path))\n",
    "        m.save(output_path)\n",
    "        print(f\"Carte générée et sauvegardée : {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création de la carte pour {city} : {e}\")\n",
    "\n",
    "# Créer les cartes pour toutes les villes\n",
    "def generate_maps_for_all_cities(base_dir):\n",
    "    cities = [\"abidjan\", \"bamako\", \"cotonou\", \"libreville\", \"paris\", \"tokyo\"]\n",
    "    for city in cities:\n",
    "        print(f\"Création de la carte pour {city}...\")\n",
    "        create_map_with_osm_data(city, base_dir)\n",
    "\n",
    "# Exécuter la génération des cartes\n",
    "generate_maps_for_all_cities(BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création de la carte pour abidjan...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\abidjan\\abidjan_map.html\n",
      "Création de la carte pour bamako...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\bamako\\bamako_map.html\n",
      "Création de la carte pour cotonou...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\cotonou\\cotonou_map.html\n",
      "Création de la carte pour libreville...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\libreville\\libreville_map.html\n",
      "Création de la carte pour paris...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\paris\\paris_map.html\n",
      "Création de la carte pour tokyo...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data\\tokyo\\tokyo_map.html\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Paramètres PostgreSQL/PostGIS\n",
    "DB_NAME = \"geosearch_data_finale\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"Malika2000\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "# Chemin de base pour les cartes\n",
    "BASE_DIR = r\"D:\\GeoSearch_Data\"\n",
    "\n",
    "# Fonction pour récupérer les données depuis PostGIS\n",
    "def fetch_data_from_postgis(city, layer):\n",
    "    table_name = f\"{city}_{city}_{layer}\"\n",
    "    query = f\"\"\"\n",
    "        SELECT ST_AsGeoJSON(wkb_geometry) AS geojson\n",
    "        FROM {table_name}\n",
    "        WHERE wkb_geometry IS NOT NULL\n",
    "        LIMIT 5000;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, host=DB_HOST, port=DB_PORT\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la récupération des données pour {table_name} : {e}\")\n",
    "        return []\n",
    "\n",
    "# Fonction pour ajouter une légende HTML\n",
    "def add_legend(map_object):\n",
    "    legend_html = '''\n",
    "     <div style=\"position: fixed; \n",
    "                 bottom: 50px; left: 50px; width: 250px; height: 150px; \n",
    "                 background-color: white; z-index:9999; font-size:14px;\n",
    "                 border:2px solid grey; padding: 10px;\">\n",
    "         <b>Légende :</b><br>\n",
    "         <i style=\"background: blue; width: 10px; height: 10px; display: inline-block;\"></i> Routes (highway)<br>\n",
    "         <i style=\"background: red; width: 10px; height: 10px; display: inline-block;\"></i> Bâtiments (building)<br>\n",
    "         <i style=\"background: green; width: 10px; height: 10px; display: inline-block;\"></i> Équipements (amenity)<br>\n",
    "     </div>\n",
    "     '''\n",
    "    map_object.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Fonction pour créer une carte et ajouter les données OSM\n",
    "def create_map_with_osm_data(city, base_dir):\n",
    "    try:\n",
    "        # Coordonnées par défaut pour centrer la carte\n",
    "        city_locations = {\n",
    "            \"abidjan\": [5.316667, -4.033333],\n",
    "            \"bamako\": [12.639232, -8.002889],\n",
    "            \"cotonou\": [6.370293, 2.391236],\n",
    "            \"libreville\": [0.416198, 9.467268],\n",
    "            \"paris\": [48.8566, 2.3522],\n",
    "            \"tokyo\": [35.6895, 139.6917],\n",
    "        }\n",
    "        city_location = city_locations.get(city.lower(), [0, 0])\n",
    "\n",
    "        # Créer une carte Folium\n",
    "        m = folium.Map(location=city_location, zoom_start=12)\n",
    "\n",
    "        # Ajouter les données OSM pour chaque type\n",
    "        for layer, color in zip([\"highway\", \"building\", \"amenity\"], [\"blue\", \"red\", \"green\"]):\n",
    "            data = fetch_data_from_postgis(city, layer)\n",
    "            if data:\n",
    "                feature_group = folium.FeatureGroup(name=f\"{layer.capitalize()} ({city})\")\n",
    "                for row in data:\n",
    "                    geojson = row[0]  # GeoJSON\n",
    "                    folium.GeoJson(\n",
    "                        geojson,\n",
    "                        style_function=lambda x, color=color: {'color': color, 'weight': 2},\n",
    "                    ).add_to(feature_group)\n",
    "                feature_group.add_to(m)\n",
    "\n",
    "        # Ajouter la légende\n",
    "        add_legend(m)\n",
    "\n",
    "        # Ajouter le contrôle des couches\n",
    "        folium.LayerControl().add_to(m)\n",
    "\n",
    "        # Sauvegarder la carte\n",
    "        output_path = os.path.join(base_dir, city, f\"{city}_map.html\")\n",
    "        if not os.path.exists(os.path.dirname(output_path)):\n",
    "            os.makedirs(os.path.dirname(output_path))\n",
    "        m.save(output_path)\n",
    "        print(f\"Carte générée et sauvegardée : {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création de la carte pour {city} : {e}\")\n",
    "\n",
    "# Générer les cartes pour toutes les villes\n",
    "def generate_maps_for_all_cities(base_dir):\n",
    "    cities = [\"abidjan\", \"bamako\", \"cotonou\", \"libreville\", \"paris\", \"tokyo\"]\n",
    "    for city in cities:\n",
    "        print(f\"Création de la carte pour {city}...\")\n",
    "        create_map_with_osm_data(city, base_dir)\n",
    "\n",
    "# Exécuter la génération des cartes\n",
    "generate_maps_for_all_cities(BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création de la carte pour abidjan...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_finale\\abidjan\\abidjan_map.html\n",
      "Création de la carte pour bamako...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_finale\\bamako\\bamako_map.html\n",
      "Création de la carte pour cotonou...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_finale\\cotonou\\cotonou_map.html\n",
      "Création de la carte pour libreville...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_finale\\libreville\\libreville_map.html\n",
      "Création de la carte pour paris...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_finale\\paris\\paris_map.html\n",
      "Création de la carte pour tokyo...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_finale\\tokyo\\tokyo_map.html\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Paramètres PostgreSQL/PostGIS\n",
    "DB_NAME = \"geosearch_data\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"Malika2000\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "# Chemin de base pour les cartes\n",
    "BASE_DIR = r\"D:\\GeoSearch_Data_finale\"\n",
    "\n",
    "# Fonction pour récupérer les données depuis PostGIS\n",
    "def fetch_data_from_postgis(city, layer):\n",
    "    table_name = f\"{city}_{city}_{layer}\"\n",
    "    query = f\"\"\"\n",
    "        SELECT ST_AsGeoJSON(wkb_geometry) AS geojson\n",
    "        FROM {table_name}\n",
    "        WHERE wkb_geometry IS NOT NULL\n",
    "        LIMIT 5000;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, host=DB_HOST, port=DB_PORT\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la récupération des données pour {table_name} : {e}\")\n",
    "        return []\n",
    "\n",
    "# Fonction pour ajouter une légende HTML\n",
    "def add_legend(map_object):\n",
    "    legend_html = '''\n",
    "     <div style=\"position: fixed; \n",
    "                 bottom: 50px; left: 50px; width: 250px; height: 150px; \n",
    "                 background-color: white; z-index:9999; font-size:14px;\n",
    "                 border:2px solid grey; padding: 10px;\">\n",
    "         <b>Légende :</b><br>\n",
    "         <i style=\"background: red; width: 10px; height: 10px; display: inline-block;\"></i> Routes (highway)<br>\n",
    "         <i style=\"background: blue; width: 10px; height: 10px; display: inline-block;\"></i> Bâtiments (building)<br>\n",
    "         <i style=\"background: green; width: 10px; height: 10px; display: inline-block;\"></i> Équipements (amenity)<br>\n",
    "     </div>\n",
    "     '''\n",
    "    map_object.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Fonction pour créer une carte et ajouter les données OSM\n",
    "def create_map_with_osm_data(city, base_dir):\n",
    "    try:\n",
    "        # Coordonnées par défaut pour centrer la carte\n",
    "        city_locations = {\n",
    "            \"abidjan\": [5.316667, -4.033333],\n",
    "            \"bamako\": [12.639232, -8.002889],\n",
    "            \"cotonou\": [6.370293, 2.391236],\n",
    "            \"libreville\": [0.416198, 9.467268],\n",
    "            \"paris\": [48.8566, 2.3522],\n",
    "            \"tokyo\": [35.6895, 139.6917],\n",
    "        }\n",
    "        city_location = city_locations.get(city.lower(), [0, 0])\n",
    "\n",
    "        # Créer une carte Folium\n",
    "        m = folium.Map(location=city_location, zoom_start=12)\n",
    "\n",
    "        # Ajouter les données OSM pour chaque type\n",
    "        for layer, color in zip([\"highway\", \"building\", \"amenity\"], [\"red\", \"blue\", \"green\"]):\n",
    "            data = fetch_data_from_postgis(city, layer)\n",
    "            if data:\n",
    "                feature_group = folium.FeatureGroup(name=f\"{layer.capitalize()} ({city})\")\n",
    "                for row in data:\n",
    "                    geojson = row[0]  # GeoJSON\n",
    "                    folium.GeoJson(\n",
    "                        geojson,\n",
    "                        style_function=lambda x, color=color: {'color': color, 'weight': 2},\n",
    "                    ).add_to(feature_group)\n",
    "                feature_group.add_to(m)\n",
    "\n",
    "        # Ajouter la légende\n",
    "        add_legend(m)\n",
    "\n",
    "        # Ajouter le contrôle des couches\n",
    "        folium.LayerControl().add_to(m)\n",
    "\n",
    "        # Sauvegarder la carte\n",
    "        output_path = os.path.join(base_dir, city, f\"{city}_map.html\")\n",
    "        if not os.path.exists(os.path.dirname(output_path)):\n",
    "            os.makedirs(os.path.dirname(output_path))\n",
    "        m.save(output_path)\n",
    "        print(f\"Carte générée et sauvegardée : {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création de la carte pour {city} : {e}\")\n",
    "\n",
    "# Générer les cartes pour toutes les villes\n",
    "def generate_maps_for_all_cities(base_dir):\n",
    "    cities = [\"abidjan\", \"bamako\", \"cotonou\", \"libreville\", \"paris\", \"tokyo\"]\n",
    "    for city in cities:\n",
    "        print(f\"Création de la carte pour {city}...\")\n",
    "        create_map_with_osm_data(city, base_dir)\n",
    "\n",
    "# Exécuter la génération des cartes\n",
    "generate_maps_for_all_cities(BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création de la carte pour abidjan...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\abidjan\\abidjan_map.html\n",
      "Création de la carte pour bamako...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\bamako\\bamako_map.html\n",
      "Création de la carte pour cotonou...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\cotonou\\cotonou_map.html\n",
      "Création de la carte pour libreville...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\libreville\\libreville_map.html\n",
      "Création de la carte pour paris...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\paris\\paris_map.html\n",
      "Création de la carte pour tokyo...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\tokyo\\tokyo_map.html\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Paramètres PostgreSQL/PostGIS\n",
    "DB_NAME = \"geosearch_data\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"Malika2000\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "# Chemin de base pour les cartes\n",
    "BASE_DIR = r\"D:\\GeoSearch_Data_true\"\n",
    "\n",
    "# Fonction pour récupérer les données depuis PostGIS\n",
    "def fetch_data_from_postgis(city, layer):\n",
    "    table_name = f\"{city}_{city}_{layer}\"\n",
    "    query = f\"\"\"\n",
    "        SELECT ST_AsGeoJSON(wkb_geometry) AS geojson\n",
    "        FROM {table_name}\n",
    "        WHERE wkb_geometry IS NOT NULL\n",
    "        LIMIT 5000;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, host=DB_HOST, port=DB_PORT\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la récupération des données pour {table_name} : {e}\")\n",
    "        return []\n",
    "\n",
    "# Fonction pour ajouter une légende HTML\n",
    "def add_legend(map_object):\n",
    "    legend_html = '''\n",
    "     <div style=\"position: fixed; \n",
    "                 bottom: 50px; left: 50px; width: 250px; height: 150px; \n",
    "                 background-color: white; z-index:9999; font-size:14px;\n",
    "                 border:2px solid grey; padding: 10px;\">\n",
    "         <b>Légende :</b><br>\n",
    "         <i style=\"background: red; width: 10px; height: 10px; display: inline-block;\"></i> Routes (highway)<br>\n",
    "         <i style=\"background: blue; width: 10px; height: 10px; display: inline-block;\"></i> Bâtiments (building)<br>\n",
    "         <i style=\"background: green; width: 10px; height: 10px; display: inline-block;\"></i> Équipements (amenity)<br>\n",
    "     </div>\n",
    "     '''\n",
    "    map_object.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Fonction pour créer une carte et ajouter les données OSM\n",
    "def create_map_with_osm_data(city, base_dir):\n",
    "    try:\n",
    "        # Coordonnées par défaut pour centrer la carte\n",
    "        city_locations = {\n",
    "            \"abidjan\": [5.316667, -4.033333],\n",
    "            \"bamako\": [12.639232, -8.002889],\n",
    "            \"cotonou\": [6.370293, 2.391236],\n",
    "            \"libreville\": [0.416198, 9.467268],\n",
    "            \"paris\": [48.8566, 2.3522],\n",
    "            \"tokyo\": [35.6895, 139.6917],\n",
    "        }\n",
    "        city_location = city_locations.get(city.lower(), [0, 0])\n",
    "\n",
    "        # Créer une carte Folium\n",
    "        m = folium.Map(location=city_location, zoom_start=12)\n",
    "\n",
    "        # Ajouter les données OSM pour chaque type\n",
    "        for layer, color in zip([\"highway\", \"building\", \"amenity\"], [\"red\", \"blue\", \"green\"]):\n",
    "            data = fetch_data_from_postgis(city, layer)\n",
    "            if data:\n",
    "                feature_group = folium.FeatureGroup(name=f\"{layer.capitalize()} ({city})\")\n",
    "                for row in data:\n",
    "                    geojson = row[0]  # GeoJSON\n",
    "                    folium.GeoJson(\n",
    "                        geojson,\n",
    "                        style_function=lambda x, color=color: {\"color\": color, \"weight\": 2},\n",
    "                        marker=None,  # Supprime les marqueurs bleus\n",
    "                    ).add_to(feature_group)\n",
    "                feature_group.add_to(m)\n",
    "\n",
    "        # Ajouter la légende\n",
    "        add_legend(m)\n",
    "\n",
    "        # Ajouter le contrôle des couches\n",
    "        folium.LayerControl().add_to(m)\n",
    "\n",
    "        # Sauvegarder la carte\n",
    "        output_path = os.path.join(base_dir, city, f\"{city}_map.html\")\n",
    "        if not os.path.exists(os.path.dirname(output_path)):\n",
    "            os.makedirs(os.path.dirname(output_path))\n",
    "        m.save(output_path)\n",
    "        print(f\"Carte générée et sauvegardée : {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création de la carte pour {city} : {e}\")\n",
    "\n",
    "# Générer les cartes pour toutes les villes\n",
    "def generate_maps_for_all_cities(base_dir):\n",
    "    cities = [\"abidjan\", \"bamako\", \"cotonou\", \"libreville\", \"paris\", \"tokyo\"]\n",
    "    for city in cities:\n",
    "        print(f\"Création de la carte pour {city}...\")\n",
    "        create_map_with_osm_data(city, base_dir)\n",
    "\n",
    "# Exécuter la génération des cartes\n",
    "generate_maps_for_all_cities(BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création de la carte pour abidjan...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\abidjan\\abidjan_map.html\n",
      "Création de la carte pour bamako...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\bamako\\bamako_map.html\n",
      "Création de la carte pour cotonou...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\cotonou\\cotonou_map.html\n",
      "Création de la carte pour libreville...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\libreville\\libreville_map.html\n",
      "Création de la carte pour paris...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\paris\\paris_map.html\n",
      "Création de la carte pour tokyo...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\tokyo\\tokyo_map.html\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Paramètres PostgreSQL/PostGIS\n",
    "DB_NAME = \"geosearch_data\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"Malika2000\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "# Chemin de base pour les cartes\n",
    "BASE_DIR = r\"D:\\GeoSearch_Data_true\"\n",
    "\n",
    "# Fonction pour récupérer les données depuis PostGIS\n",
    "def fetch_data_from_postgis(city, layer):\n",
    "    table_name = f\"{city}_{city}_{layer}\"\n",
    "    query = f\"\"\"\n",
    "        SELECT ST_AsGeoJSON(wkb_geometry) AS geojson\n",
    "        FROM {table_name}\n",
    "        WHERE wkb_geometry IS NOT NULL\n",
    "        LIMIT 5000;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, host=DB_HOST, port=DB_PORT\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la récupération des données pour {table_name} : {e}\")\n",
    "        return []\n",
    "\n",
    "# Fonction pour ajouter une légende HTML\n",
    "def add_legend(map_object):\n",
    "    legend_html = '''\n",
    "     <div style=\"position: fixed; \n",
    "                 bottom: 50px; left: 50px; width: 250px; height: 150px; \n",
    "                 background-color: white; z-index:9999; font-size:14px;\n",
    "                 border:2px solid grey; padding: 10px;\">\n",
    "         <b>Légende :</b><br>\n",
    "         <i style=\"background: red; width: 10px; height: 10px; display: inline-block;\"></i> Routes (highway)<br>\n",
    "         <i style=\"background: rgba(70, 130, 180, 0.5); width: 10px; height: 10px; display: inline-block;\"></i> Bâtiments (building)<br>\n",
    "         <i style=\"background: green; width: 10px; height: 10px; display: inline-block;\"></i> Équipements (amenity)<br>\n",
    "     </div>\n",
    "     '''\n",
    "    map_object.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Fonction pour créer une carte et ajouter les données OSM\n",
    "def create_map_with_osm_data(city, base_dir):\n",
    "    try:\n",
    "        # Coordonnées par défaut pour centrer la carte\n",
    "        city_locations = {\n",
    "            \"abidjan\": [5.316667, -4.033333],\n",
    "            \"bamako\": [12.639232, -8.002889],\n",
    "            \"cotonou\": [6.370293, 2.391236],\n",
    "            \"libreville\": [0.416198, 9.467268],\n",
    "            \"paris\": [48.8566, 2.3522],\n",
    "            \"tokyo\": [35.6895, 139.6917],\n",
    "        }\n",
    "        city_location = city_locations.get(city.lower(), [0, 0])\n",
    "\n",
    "        # Créer une carte Folium\n",
    "        m = folium.Map(location=city_location, zoom_start=12)\n",
    "\n",
    "        # Ajouter les données OSM pour chaque type\n",
    "        for layer, color in zip([\"highway\", \"building\", \"amenity\"], [\"red\", \"rgba(70, 130, 180, 0.5)\", \"green\"]):\n",
    "            data = fetch_data_from_postgis(city, layer)\n",
    "            if data:\n",
    "                feature_group = folium.FeatureGroup(name=f\"{layer.capitalize()} ({city})\")\n",
    "                for row in data:\n",
    "                    geojson = row[0]  # GeoJSON\n",
    "                    folium.GeoJson(\n",
    "                        geojson,\n",
    "                        style_function=lambda x, color=color: {\"color\": color, \"weight\": 2, \"fillOpacity\": 0.5} if layer == \"building\" else {\"color\": color, \"weight\": 2},\n",
    "                        marker=None,  # Désactiver les marqueurs\n",
    "                    ).add_to(feature_group)\n",
    "                feature_group.add_to(m)\n",
    "\n",
    "        # Ajouter la légende\n",
    "        add_legend(m)\n",
    "\n",
    "        # Ajouter le contrôle des couches\n",
    "        folium.LayerControl().add_to(m)\n",
    "\n",
    "        # Sauvegarder la carte\n",
    "        output_path = os.path.join(base_dir, city, f\"{city}_map.html\")\n",
    "        if not os.path.exists(os.path.dirname(output_path)):\n",
    "            os.makedirs(os.path.dirname(output_path))\n",
    "        m.save(output_path)\n",
    "        print(f\"Carte générée et sauvegardée : {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création de la carte pour {city} : {e}\")\n",
    "\n",
    "# Générer les cartes pour toutes les villes\n",
    "def generate_maps_for_all_cities(base_dir):\n",
    "    cities = [\"abidjan\", \"bamako\", \"cotonou\", \"libreville\", \"paris\", \"tokyo\"]\n",
    "    for city in cities:\n",
    "        print(f\"Création de la carte pour {city}...\")\n",
    "        create_map_with_osm_data(city, base_dir)\n",
    "\n",
    "# Exécuter la génération des cartes\n",
    "generate_maps_for_all_cities(BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création de la carte filtrée pour abidjan...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\abidjan\\abidjan_map.html\n",
      "Création de la carte filtrée pour bamako...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\bamako\\bamako_map.html\n",
      "Création de la carte filtrée pour cotonou...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\cotonou\\cotonou_map.html\n",
      "Création de la carte filtrée pour libreville...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\libreville\\libreville_map.html\n",
      "Création de la carte filtrée pour paris...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\paris\\paris_map.html\n",
      "Création de la carte filtrée pour tokyo...\n",
      "Carte générée et sauvegardée : D:\\GeoSearch_Data_true\\tokyo\\tokyo_map.html\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import psycopg2\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Paramètres PostgreSQL/PostGIS\n",
    "DB_NAME = \"geosearch_data\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"Malika2000\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "# Chemin de base pour les cartes\n",
    "BASE_DIR = r\"D:\\GeoSearch_Data_true\"\n",
    "\n",
    "# Fonction pour récupérer les données filtrées depuis PostGIS\n",
    "def fetch_filtered_data(city, layer):\n",
    "    table_name = f\"{city}_{city}_{layer}\"\n",
    "    query = f\"\"\"\n",
    "        SELECT ST_AsGeoJSON(wkb_geometry) AS geojson\n",
    "        FROM {table_name}\n",
    "        WHERE ST_GeometryType(wkb_geometry) IN ('ST_LineString', 'ST_Polygon')\n",
    "        LIMIT 5000;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, host=DB_HOST, port=DB_PORT\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la récupération des données pour {table_name} : {e}\")\n",
    "        return []\n",
    "\n",
    "# Fonction pour ajouter une légende HTML\n",
    "def add_legend(map_object):\n",
    "    legend_html = '''\n",
    "     <div style=\"position: fixed; \n",
    "                 bottom: 50px; left: 50px; width: 250px; height: 150px; \n",
    "                 background-color: white; z-index:9999; font-size:14px;\n",
    "                 border:2px solid grey; padding: 10px;\">\n",
    "         <b>Légende :</b><br>\n",
    "         <i style=\"background: red; width: 10px; height: 10px; display: inline-block;\"></i> Routes (highway)<br>\n",
    "         <i style=\"background: rgba(70, 130, 180, 0.5); width: 10px; height: 10px; display: inline-block;\"></i> Bâtiments (building)<br>\n",
    "         <i style=\"background: green; width: 10px; height: 10px; display: inline-block;\"></i> Équipements (amenity)<br>\n",
    "     </div>\n",
    "     '''\n",
    "    map_object.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Fonction pour créer une carte et ajouter les données filtrées\n",
    "def create_filtered_map(city, base_dir):\n",
    "    try:\n",
    "        # Coordonnées par défaut pour centrer la carte\n",
    "        city_locations = {\n",
    "            \"abidjan\": [5.316667, -4.033333],\n",
    "            \"bamako\": [12.639232, -8.002889],\n",
    "            \"cotonou\": [6.370293, 2.391236],\n",
    "            \"libreville\": [0.416198, 9.467268],\n",
    "            \"paris\": [48.8566, 2.3522],\n",
    "            \"tokyo\": [35.6895, 139.6917],\n",
    "        }\n",
    "        city_location = city_locations.get(city.lower(), [0, 0])\n",
    "\n",
    "        # Créer une carte Folium\n",
    "        m = folium.Map(location=city_location, zoom_start=12)\n",
    "\n",
    "        # Ajouter les données filtrées pour chaque type\n",
    "        for layer, color in zip([\"highway\", \"building\", \"amenity\"], [\"red\", \"rgba(70, 130, 180, 0.5)\", \"green\"]):\n",
    "            data = fetch_filtered_data(city, layer)\n",
    "            if data:\n",
    "                feature_group = folium.FeatureGroup(name=f\"{layer.capitalize()} ({city})\")\n",
    "                for row in data:\n",
    "                    geojson = json.loads(row[0])  # Charger le GeoJSON\n",
    "                    folium.GeoJson(\n",
    "                        geojson,\n",
    "                        style_function=lambda x, color=color: {\"color\": color, \"weight\": 2, \"fillOpacity\": 0.5} if layer == \"building\" else {\"color\": color, \"weight\": 2},\n",
    "                    ).add_to(feature_group)\n",
    "                feature_group.add_to(m)\n",
    "\n",
    "        # Ajouter la légende\n",
    "        add_legend(m)\n",
    "\n",
    "        # Ajouter le contrôle des couches\n",
    "        folium.LayerControl().add_to(m)\n",
    "\n",
    "        # Sauvegarder la carte\n",
    "        output_path = os.path.join(base_dir, city, f\"{city}_map.html\")\n",
    "        if not os.path.exists(os.path.dirname(output_path)):\n",
    "            os.makedirs(os.path.dirname(output_path))\n",
    "        m.save(output_path)\n",
    "        print(f\"Carte générée et sauvegardée : {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création de la carte pour {city} : {e}\")\n",
    "\n",
    "# Générer les cartes pour toutes les villes\n",
    "def generate_filtered_maps(base_dir):\n",
    "    cities = [\"abidjan\", \"bamako\", \"cotonou\", \"libreville\", \"paris\", \"tokyo\"]\n",
    "    for city in cities:\n",
    "        print(f\"Création de la carte filtrée pour {city}...\")\n",
    "        create_filtered_map(city, base_dir)\n",
    "\n",
    "# Exécuter la génération des cartes\n",
    "generate_filtered_maps(BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating map for abidjan...\n",
      "Map for abidjan saved at D:\\GeoSearch_Data_true\\abidjan\\abidjan_map.html\n",
      "Creating map for bamako...\n",
      "Map for bamako saved at D:\\GeoSearch_Data_true\\bamako\\bamako_map.html\n",
      "Creating map for cotonou...\n",
      "Map for cotonou saved at D:\\GeoSearch_Data_true\\cotonou\\cotonou_map.html\n",
      "Creating map for libreville...\n",
      "Map for libreville saved at D:\\GeoSearch_Data_true\\libreville\\libreville_map.html\n",
      "Creating map for paris...\n",
      "Map for paris saved at D:\\GeoSearch_Data_true\\paris\\paris_map.html\n",
      "Creating map for tokyo...\n",
      "Map for tokyo saved at D:\\GeoSearch_Data_true\\tokyo\\tokyo_map.html\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import psycopg2\n",
    "import os\n",
    "import json\n",
    "\n",
    "# PostgreSQL/PostGIS connection parameters\n",
    "DB_NAME = \"geosearch_data\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"Malika2000\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "# Base directory for saving the maps\n",
    "BASE_DIR = r\"D:\\GeoSearch_Data_true\"\n",
    "\n",
    "# Fetch filtered data from PostGIS\n",
    "def fetch_filtered_data(city, layer):\n",
    "    table_name = f\"{city}_{city}_{layer}\"\n",
    "    query = f\"\"\"\n",
    "        SELECT ST_AsGeoJSON(wkb_geometry) AS geojson\n",
    "        FROM {table_name}\n",
    "        WHERE ST_GeometryType(wkb_geometry) IN ('ST_LineString', 'ST_Polygon')\n",
    "        LIMIT 5000;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, host=DB_HOST, port=DB_PORT\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Add HTML legend to the map\n",
    "def add_legend(map_object):\n",
    "    legend_html = '''\n",
    "    <div style=\"position: fixed; \n",
    "                bottom: 50px; left: 50px; width: 250px; height: 150px; \n",
    "                background-color: white; z-index:9999; font-size:14px;\n",
    "                border:2px solid grey; padding: 10px;\">\n",
    "        <b>Legend:</b><br>\n",
    "        <i style=\"background: red; width: 10px; height: 10px; display: inline-block;\"></i> Highways (Red)<br>\n",
    "        <i style=\"background: rgba(70, 130, 180, 0.5); width: 10px; height: 10px; display: inline-block;\"></i> Buildings (Blue)<br>\n",
    "        <i style=\"background: green; width: 10px; height: 10px; display: inline-block;\"></i> Amenities (Green)<br>\n",
    "    </div>\n",
    "    '''\n",
    "    map_object.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Create interactive map with filtered data\n",
    "def create_filtered_map(city, base_dir):\n",
    "    try:\n",
    "        # Default city locations for centering the map\n",
    "        city_locations = {\n",
    "            \"abidjan\": [5.316667, -4.033333],\n",
    "            \"bamako\": [12.639232, -8.002889],\n",
    "            \"cotonou\": [6.370293, 2.391236],\n",
    "            \"libreville\": [0.416198, 9.467268],\n",
    "            \"paris\": [48.8566, 2.3522],\n",
    "            \"tokyo\": [35.6895, 139.6917],\n",
    "        }\n",
    "        city_location = city_locations.get(city.lower(), [0, 0])\n",
    "\n",
    "        # Create a Folium map object\n",
    "        m = folium.Map(location=city_location, zoom_start=12, tiles=\"OpenStreetMap\")\n",
    "\n",
    "        # Add layers for each category with filtered data\n",
    "        for layer, color in zip([\"highway\", \"building\", \"amenity\"], [\"red\", \"blue\", \"green\"]):\n",
    "            data = fetch_filtered_data(city, layer)\n",
    "            if data:\n",
    "                feature_group = folium.FeatureGroup(name=f\"{layer.capitalize()} ({city})\")\n",
    "                for row in data:\n",
    "                    geojson = json.loads(row[0])  # Parse GeoJSON data\n",
    "                    folium.GeoJson(\n",
    "                        geojson,\n",
    "                        style_function=lambda x, color=color: {\n",
    "                            \"color\": color, \n",
    "                            \"weight\": 2, \n",
    "                            \"fillOpacity\": 0.5 if layer == \"building\" else 0\n",
    "                        },\n",
    "                    ).add_to(feature_group)\n",
    "                feature_group.add_to(m)\n",
    "\n",
    "        # Add legend and layer control\n",
    "        add_legend(m)\n",
    "        folium.LayerControl().add_to(m)\n",
    "\n",
    "        # Save the map to file\n",
    "        output_path = os.path.join(base_dir, city, f\"{city}_map.html\")\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        m.save(output_path)\n",
    "        print(f\"Map for {city} saved at {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating map for {city}: {e}\")\n",
    "\n",
    "# Generate maps for all cities\n",
    "def generate_filtered_maps(base_dir):\n",
    "    cities = [\"abidjan\", \"bamako\", \"cotonou\", \"libreville\", \"paris\", \"tokyo\"]\n",
    "    for city in cities:\n",
    "        print(f\"Creating map for {city}...\")\n",
    "        create_filtered_map(city, base_dir)\n",
    "\n",
    "# Execute the map generation\n",
    "generate_filtered_maps(BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger les images et leurs métadonnées dans PostGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Abidjan\\Abidjan_2023-10-01.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Abidjan\\Abidjan_2023-10-02.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Abidjan\\Abidjan_2023-10-03.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Abidjan\\Abidjan_2023-10-04.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Abidjan\\Abidjan_2023-10-05.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Abidjan\\Abidjan_2023-10-06.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Abidjan\\Abidjan_2023-10-07.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Abidjan\\Abidjan_2023-10-08.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Abidjan\\Abidjan_2023-10-09.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Abidjan\\Abidjan_2023-10-10.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Bamako\\Bamako_2023-10-01.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Bamako\\Bamako_2023-10-02.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Bamako\\Bamako_2023-10-03.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Bamako\\Bamako_2023-10-04.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Bamako\\Bamako_2023-10-05.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Bamako\\Bamako_2023-10-06.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Bamako\\Bamako_2023-10-07.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Bamako\\Bamako_2023-10-08.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Bamako\\Bamako_2023-10-09.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Bamako\\Bamako_2023-10-10.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Cotonou\\Cotonou_2023-10-01.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Cotonou\\Cotonou_2023-10-02.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Cotonou\\Cotonou_2023-10-03.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Cotonou\\Cotonou_2023-10-04.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Cotonou\\Cotonou_2023-10-05.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Cotonou\\Cotonou_2023-10-06.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Cotonou\\Cotonou_2023-10-07.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Cotonou\\Cotonou_2023-10-08.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Cotonou\\Cotonou_2023-10-09.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Cotonou\\Cotonou_2023-10-10.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Libreville\\Libreville_2023-10-01.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Libreville\\Libreville_2023-10-02.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Libreville\\Libreville_2023-10-03.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Libreville\\Libreville_2023-10-04.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Libreville\\Libreville_2023-10-05.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Libreville\\Libreville_2023-10-06.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Libreville\\Libreville_2023-10-07.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Libreville\\Libreville_2023-10-08.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Libreville\\Libreville_2023-10-09.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Libreville\\Libreville_2023-10-10.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\New York\\New York_2023-10-01.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\New York\\New York_2023-10-02.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\New York\\New York_2023-10-03.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\New York\\New York_2023-10-04.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\New York\\New York_2023-10-05.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\New York\\New York_2023-10-06.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\New York\\New York_2023-10-07.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\New York\\New York_2023-10-08.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\New York\\New York_2023-10-09.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\New York\\New York_2023-10-10.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Paris\\Paris_2023-10-01.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Paris\\Paris_2023-10-02.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Paris\\Paris_2023-10-03.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Paris\\Paris_2023-10-04.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Paris\\Paris_2023-10-05.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Paris\\Paris_2023-10-06.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Paris\\Paris_2023-10-07.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Paris\\Paris_2023-10-08.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Paris\\Paris_2023-10-09.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Paris\\Paris_2023-10-10.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Tokyo\\Tokyo_2023-10-01.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Tokyo\\Tokyo_2023-10-02.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Tokyo\\Tokyo_2023-10-03.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Tokyo\\Tokyo_2023-10-04.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Tokyo\\Tokyo_2023-10-05.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Tokyo\\Tokyo_2023-10-06.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Tokyo\\Tokyo_2023-10-07.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Tokyo\\Tokyo_2023-10-08.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Tokyo\\Tokyo_2023-10-09.json\n",
      "Fichier JSON inséré : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\\Tokyo\\Tokyo_2023-10-10.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, String\n",
    "from geoalchemy2 import Geometry\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# Configuration de la base\n",
    "DB_NAME = \"geosearch_data\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"Malika2000\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "IMG_DIR = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte_Donnee\\IMG_SentinelHub\"\n",
    "\n",
    "# Connexion à PostGIS\n",
    "engine = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "metadata = MetaData()\n",
    "\n",
    "# Définition de la table\n",
    "images_table = Table(\n",
    "    'satellites_images', metadata,\n",
    "    Column('id', String, primary_key=True),\n",
    "    Column('city', String),\n",
    "    Column('file_path', String),\n",
    "    Column('geometry', Geometry('POLYGON', srid=4326))\n",
    ")\n",
    "metadata.create_all(engine)\n",
    "\n",
    "# Charger les fichiers JSON et insérer dans la base\n",
    "with engine.connect() as conn:\n",
    "    for city in os.listdir(IMG_DIR):\n",
    "        city_path = os.path.join(IMG_DIR, city)\n",
    "        if os.path.isdir(city_path):\n",
    "            for file in os.listdir(city_path):\n",
    "                if file.endswith('.json'):\n",
    "                    json_path = os.path.join(city_path, file)\n",
    "                    try:\n",
    "                        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                            data = json.load(f)\n",
    "\n",
    "                        # Construire la géométrie POLYGON WKT\n",
    "                        bbox = data['bbox']\n",
    "                        polygon_wkt = (\n",
    "                            f\"SRID=4326;POLYGON(({bbox['min_lon']} {bbox['min_lat']}, \"\n",
    "                            f\"{bbox['max_lon']} {bbox['min_lat']}, \"\n",
    "                            f\"{bbox['max_lon']} {bbox['max_lat']}, \"\n",
    "                            f\"{bbox['min_lon']} {bbox['max_lat']}, \"\n",
    "                            f\"{bbox['min_lon']} {bbox['min_lat']}))\"\n",
    "                        )\n",
    "\n",
    "                        # Insérer les données dans une transaction\n",
    "                        with conn.begin():  # Démarrer une transaction\n",
    "                            stmt = images_table.insert().values(\n",
    "                                id=f\"{data['city']}_{data['date']}\",\n",
    "                                city=data['city'],\n",
    "                                file_path=os.path.join(city_path, data['image_path']),\n",
    "                                geometry=polygon_wkt\n",
    "                            )\n",
    "                            conn.execute(stmt)\n",
    "                            print(f\"Fichier JSON inséré : {json_path}\")\n",
    "\n",
    "                    except SQLAlchemyError as e:\n",
    "                        print(f\"Erreur SQL lors de l'insertion de {json_path} : {str(e)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erreur lors du traitement de {json_path} : {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carte générée : Abidjan_satellite_map.html\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "from shapely.geometry import shape\n",
    "\n",
    "# Connexion à la base de données\n",
    "DB_NAME = \"geosearch_data\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"Malika2000\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "engine = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "\n",
    "# Charger les données au format GeoJSON\n",
    "def fetch_images_for_city(city):\n",
    "    query = f\"\"\"\n",
    "    SELECT id, city, file_path, ST_AsGeoJSON(geometry) AS geojson\n",
    "    FROM satellite_images\n",
    "    WHERE city = '{city}';\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, con=engine)\n",
    "\n",
    "    if not df.empty:\n",
    "        # Convertir GeoJSON en objets géométriques Shapely\n",
    "        df['geometry'] = df['geojson'].apply(lambda x: shape(json.loads(x)))\n",
    "        gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "        gdf.set_crs(epsg=4326, inplace=True)\n",
    "        return gdf\n",
    "    return gpd.GeoDataFrame()  # Retourne un DataFrame vide si aucune donnée\n",
    "\n",
    "# Créer une carte interactive\n",
    "def create_city_map(city):\n",
    "    gdf = fetch_images_for_city(city)\n",
    "\n",
    "    if gdf.empty:\n",
    "        print(f\"Aucune donnée trouvée pour la ville : {city}\")\n",
    "        return\n",
    "\n",
    "    # Créez une carte centrée sur le premier élément\n",
    "    centroid = gdf.geometry[0].centroid\n",
    "    city_map = folium.Map(location=[centroid.y, centroid.x], zoom_start=12)\n",
    "\n",
    "    # Ajouter les polygones des images satellites\n",
    "    for _, row in gdf.iterrows():\n",
    "        folium.GeoJson(\n",
    "            row.geometry.__geo_interface__,\n",
    "            name=row.file_path,\n",
    "            tooltip=f\"ID: {row.id}, Path: {row.file_path}\"\n",
    "        ).add_to(city_map)\n",
    "\n",
    "    # Ajouter un contrôle pour afficher les couches\n",
    "    folium.LayerControl().add_to(city_map)\n",
    "\n",
    "    # Sauvegarder la carte\n",
    "    map_file = f\"{city}_satellite_map.html\"\n",
    "    city_map.save(map_file)\n",
    "    print(f\"Carte générée : {map_file}\")\n",
    "\n",
    "# Générer une carte pour Abidjan\n",
    "create_city_map(\"Abidjan\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application Streamlit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 12:34:24.296 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 12:34:24.473 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-12-03 12:34:24.473 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "if not os.path.exists(FAISS_INDEX_PATH):\n",
    "    st.sidebar.error(f\"Fichier FAISS introuvable à l'emplacement : {FAISS_INDEX_PATH}\")\n",
    "else:\n",
    "    st.sidebar.success(f\"Fichier FAISS trouvé à : {FAISS_INDEX_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index FAISS sauvegardé dans : C:\\FAISS\\faiss_index\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chemin vers les vecteurs (assurez-vous d'avoir ce fichier)\n",
    "VECTOR_FILE = r\"C:\\FAISS\\vectors.npy\"\n",
    "FAISS_INDEX_PATH = r\"C:\\FAISS\\faiss_index\"\n",
    "\n",
    "# Charger les vecteurs\n",
    "vectors = np.load(VECTOR_FILE)\n",
    "dimension = vectors.shape[1]\n",
    "\n",
    "# Créer un index FAISS\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(vectors)\n",
    "\n",
    "# Sauvegarder l'index FAISS\n",
    "faiss.write_index(index, FAISS_INDEX_PATH)\n",
    "print(f\"Index FAISS sauvegardé dans : {FAISS_INDEX_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier trouvé à : C:\\FAISS\\faiss_index.bin\n",
      "Index FAISS chargé avec succès.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "\n",
    "FAISS_INDEX_PATH = r\"C:\\FAISS\\faiss_index.bin\"\n",
    "\n",
    "if not os.path.exists(FAISS_INDEX_PATH):\n",
    "    print(f\"Erreur : Le fichier {FAISS_INDEX_PATH} est introuvable.\")\n",
    "else:\n",
    "    print(f\"Fichier trouvé à : {FAISS_INDEX_PATH}\")\n",
    "    try:\n",
    "        # Chargement de l'index pour vérifier son intégrité\n",
    "        index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "        print(\"Index FAISS chargé avec succès.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement de l'index FAISS : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index FAISS chargé avec succès : 19 vecteurs.\n",
      "Métadonnées chargées avec succès : 19 entrées.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import json\n",
    "import os\n",
    "\n",
    "FAISS_INDEX_PATH = r\"C:\\FAISS\\faiss_index.bin\"\n",
    "METADATA_FILE = r\"C:\\FAISS\\metadata_faiss.json\"\n",
    "\n",
    "# Test du fichier FAISS\n",
    "if os.path.exists(FAISS_INDEX_PATH):\n",
    "    index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "    print(f\"Index FAISS chargé avec succès : {index.ntotal} vecteurs.\")\n",
    "else:\n",
    "    print(f\"Erreur : Fichier FAISS introuvable à {FAISS_INDEX_PATH}\")\n",
    "\n",
    "# Test du fichier de métadonnées\n",
    "if os.path.exists(METADATA_FILE):\n",
    "    with open(METADATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "    print(f\"Métadonnées chargées avec succès : {len(metadata)} entrées.\")\n",
    "else:\n",
    "    print(f\"Erreur : Fichier de métadonnées introuvable à {METADATA_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier FAISS existe : C:\\FAISS\\faiss_index.bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "FAISS_INDEX_PATH = r\"C:\\FAISS\\faiss_index.bin\"\n",
    "if os.path.isfile(FAISS_INDEX_PATH):\n",
    "    print(f\"Le fichier FAISS existe : {FAISS_INDEX_PATH}\")\n",
    "else:\n",
    "    print(f\"Erreur : Le fichier FAISS est introuvable : {FAISS_INDEX_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index FAISS chargé avec succès. Nombre de vecteurs : 19\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "FAISS_INDEX_PATH = r\"C:\\FAISS\\faiss_index.bin\"\n",
    "\n",
    "try:\n",
    "    index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "    print(f\"Index FAISS chargé avec succès. Nombre de vecteurs : {index.ntotal}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement de l'index FAISS : {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
