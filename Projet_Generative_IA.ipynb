{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJET IA MOTEUR DE RECHERCHE  GEOSEARCH (GEOSPATIALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "Le projet GeoSearch est une initiative visant à développer un moteur de recherche intelligent spécialisé dans les sciences géospatiales. Dans le cadre de ce projet, j'utiliserai une Intelligence Artificielle Générative pour générer des réponses aux requêtes des utilisateurs, en utilisant une base de connaissances scientifiques enrichie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GeoSearch Diagram](./IM3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (3.10.10)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (0.3.15)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (0.1.140)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-community in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (0.3.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.6 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (0.3.7)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (0.3.15)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (0.1.140)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.17.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (2.23.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openai in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (1.54.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pypdf in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (5.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: chromadb in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (0.5.18)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (2.9.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (0.115.4)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (0.49b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (1.28.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (0.20.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (4.67.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (1.67.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (0.13.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (3.10.11)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (0.27.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from build>=1.0.3->chromadb) (24.1)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.41.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.36.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.28.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.49b0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.49b0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-instrumentation==0.49b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.49b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.26.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.10.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tiktoken in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: yt_dlp in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (2024.11.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pydub in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (0.25.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installation des packages necessaire à notre projet \n",
    "\n",
    "%pip install langchain\n",
    "%pip install langchain-community\n",
    "%pip install openai\n",
    "%pip install pypdf  \n",
    "%pip install chromadb  \n",
    "%pip install tiktoken  \n",
    "%pip install beautifulsoup4 \n",
    "%pip install yt_dlp  \n",
    "%pip install pydub "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecte de donnée dans le cadre de l'application de notre projet ( Data collection sources )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construire un prototype fonctionnel de GeoSearch en utilisant des outils open source et gratuits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecte et Pretraitement des  Données Documentaires sur le sol , la géologie , le cadastre , etude circulation  et les Infrastructures dans les Villes Sélectionnées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction des PDF et Normalisation de ces documents en JSON, incluant les champs importants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche de PDF pour : Abidjan études de sol géologie PDF\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_3.json\n",
      "Recherche de PDF pour : Abidjan rapport infrastructure PDF\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_3.json\n",
      "Recherche de PDF pour : Abidjan document cadastral PDF\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_1.pdf\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_2.json\n",
      "Recherche de PDF pour : Abidjan étude circulation PDF\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_2.json\n",
      "Pas un PDF valide ou erreur HTTP : https://www.afdb.org/fileadmin/uploads/afdb/Documents/Environmental-and-Social-Assessments/Cote_Ivoire-PTUA-Summary_ESIA_August_2016-FR.pdf\n",
      "Recherche de PDF pour : Abidjan plans réseaux souterrains PDF\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Abidjan_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Abidjan_doc_3.json\n",
      "Recherche de PDF pour : Bamako études de sol géologie PDF\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_3.json\n",
      "Recherche de PDF pour : Bamako rapport infrastructure PDF\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_3.json\n",
      "Recherche de PDF pour : Bamako document cadastral PDF\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_3.json\n",
      "Recherche de PDF pour : Bamako étude circulation PDF\n",
      "Pas un PDF valide ou erreur HTTP : https://www.afdb.org/fileadmin/uploads/afdb/Documents/Publications/etude_securite_routiere_Bamako_2018.pdf\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_3.json\n",
      "Recherche de PDF pour : Bamako plans réseaux souterrains PDF\n",
      "PDF téléchargé : pdf_data\\Bamako_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Bamako_doc_1.json\n",
      "Erreur lors du téléchargement de https://mirror.unhabitat.org/downloads/docs/BamakoCDS_ReportFrench.pdf : HTTPSConnectionPool(host='mirror.unhabitat.org', port=443): Max retries exceeded with url: /downloads/docs/BamakoCDS_ReportFrench.pdf (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "Pas un PDF valide ou erreur HTTP : https://www.afdb.org/sites/default/files/documents/environmental-and-social-assessments/mali_-_projet_dalimentation_en_eau_potable_de_la_ville_de_bamako_a_partir_de_kabala_-_resume_pges.pdf\n",
      "Recherche de PDF pour : Libreville études de sol géologie PDF\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_3.json\n",
      "Recherche de PDF pour : Libreville rapport infrastructure PDF\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_3.json\n",
      "Recherche de PDF pour : Libreville document cadastral PDF\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_3.json\n",
      "Recherche de PDF pour : Libreville étude circulation PDF\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_3.json\n",
      "Recherche de PDF pour : Libreville plans réseaux souterrains PDF\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Libreville_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Libreville_doc_3.json\n",
      "Recherche de PDF pour : Cotonou études de sol géologie PDF\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_3.json\n",
      "Recherche de PDF pour : Cotonou rapport infrastructure PDF\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_2.json\n",
      "Pas un PDF valide ou erreur HTTP : https://www.afdb.org/sites/default/files/documents/projects-and-operations/benin_-_modernisation_et_extension_du_port_autonome_de_cotonou_-_note_de_synthese_de_projet.pdf\n",
      "Recherche de PDF pour : Cotonou document cadastral PDF\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_3.json\n",
      "Recherche de PDF pour : Cotonou étude circulation PDF\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_3.json\n",
      "Recherche de PDF pour : Cotonou plans réseaux souterrains PDF\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Cotonou_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Cotonou_doc_3.json\n",
      "Recherche de PDF pour : Paris études de sol géologie PDF\n",
      "PDF téléchargé : pdf_data\\Paris_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Paris_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Paris_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_3.json\n",
      "Recherche de PDF pour : Paris rapport infrastructure PDF\n",
      "Pas un PDF valide ou erreur HTTP : https://www.un.org/pga/71/wp-content/uploads/sites/40/2017/02/New-Climate-Economy-Report-2016-Executive-Summary.pdf\n",
      "Pas un PDF valide ou erreur HTTP : https://unfccc.int/sites/default/files/resource/cma2021_08rev01_adv.pdf\n",
      "Pas un PDF valide ou erreur HTTP : https://unfccc.int/files/meetings/paris_nov_2015/application/pdf/paris_agreement_english_.pdf\n",
      "Recherche de PDF pour : Paris document cadastral PDF\n",
      "PDF téléchargé : pdf_data\\Paris_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Paris_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Paris_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_3.json\n",
      "Recherche de PDF pour : Paris étude circulation PDF\n",
      "PDF téléchargé : pdf_data\\Paris_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Paris_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Paris_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_3.json\n",
      "Recherche de PDF pour : Paris plans réseaux souterrains PDF\n",
      "PDF téléchargé : pdf_data\\Paris_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Paris_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Paris_doc_2.json\n",
      "Recherche de PDF pour : New York soil studies geology PDF\n",
      "PDF téléchargé : pdf_data\\New_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_1.json\n",
      "PDF téléchargé : pdf_data\\New_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_2.json\n",
      "PDF téléchargé : pdf_data\\New_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_3.json\n",
      "Recherche de PDF pour : New York infrastructure report PDF\n",
      "Pas un PDF valide ou erreur HTTP : https://infrastructurereportcard.org/wp-content/uploads/2024/04/NY-2024-State-Fact-Sheet-ASCE.pdf\n",
      "Pas un PDF valide ou erreur HTTP : https://infrastructurereportcard.org/wp-content/uploads/2017/01/IRC_Brochure-NY2022.pdf\n",
      "PDF téléchargé : pdf_data\\New_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_3.json\n",
      "Recherche de PDF pour : New York cadastral document PDF\n",
      "PDF téléchargé : pdf_data\\New_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_1.json\n",
      "PDF téléchargé : pdf_data\\New_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_2.json\n",
      "Recherche de PDF pour : New York traffic study PDF\n",
      "PDF téléchargé : pdf_data\\New_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_1.json\n",
      "PDF téléchargé : pdf_data\\New_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_2.json\n",
      "PDF téléchargé : pdf_data\\New_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_3.json\n",
      "Recherche de PDF pour : New York underground network plans PDF\n",
      "PDF téléchargé : pdf_data\\New_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_1.json\n",
      "PDF téléchargé : pdf_data\\New_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\New_doc_2.json\n",
      "Recherche de PDF pour : Tokyo soil studies geology PDF\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_3.json\n",
      "Recherche de PDF pour : Tokyo infrastructure report PDF\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_3.json\n",
      "Recherche de PDF pour : Tokyo cadastral document PDF\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_2.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_2.json\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_3.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_3.json\n",
      "Recherche de PDF pour : Tokyo traffic study PDF\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_1.json\n",
      "Recherche de PDF pour : Tokyo underground network plans PDF\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_1.pdf\n",
      "Données normalisées sauvegardées : normalized_data\\Tokyo_doc_1.json\n",
      "PDF téléchargé : pdf_data\\Tokyo_doc_2.pdf\n",
      "Pas un PDF valide ou erreur HTTP : https://www.jreast.co.jp/e/press/2014/pdf/20140703.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF pour l'extraction de texte\n",
    "import json\n",
    "\n",
    "# Répertoires pour sauvegarder les PDF et les fichiers normalisés\n",
    "PDF_DIR = \"pdf_data\"\n",
    "NORMALIZED_DIR = \"normalized_data\"\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "os.makedirs(NORMALIZED_DIR, exist_ok=True)\n",
    "\n",
    "# Liste des recherches pour chaque ville\n",
    "cities_keywords = [\n",
    "    # Abidjan\n",
    "    \"Abidjan études de sol géologie PDF\",\n",
    "    \"Abidjan rapport infrastructure PDF\",\n",
    "    \"Abidjan document cadastral PDF\",\n",
    "    \"Abidjan étude circulation PDF\",\n",
    "    \"Abidjan plans réseaux souterrains PDF\",\n",
    "    # Bamako\n",
    "    \"Bamako études de sol géologie PDF\",\n",
    "    \"Bamako rapport infrastructure PDF\",\n",
    "    \"Bamako document cadastral PDF\",\n",
    "    \"Bamako étude circulation PDF\",\n",
    "    \"Bamako plans réseaux souterrains PDF\",\n",
    "    # Libreville\n",
    "    \"Libreville études de sol géologie PDF\",\n",
    "    \"Libreville rapport infrastructure PDF\",\n",
    "    \"Libreville document cadastral PDF\",\n",
    "    \"Libreville étude circulation PDF\",\n",
    "    \"Libreville plans réseaux souterrains PDF\",\n",
    "    # Cotonou\n",
    "    \"Cotonou études de sol géologie PDF\",\n",
    "    \"Cotonou rapport infrastructure PDF\",\n",
    "    \"Cotonou document cadastral PDF\",\n",
    "    \"Cotonou étude circulation PDF\",\n",
    "    \"Cotonou plans réseaux souterrains PDF\",\n",
    "    # Paris\n",
    "    \"Paris études de sol géologie PDF\",\n",
    "    \"Paris rapport infrastructure PDF\",\n",
    "    \"Paris document cadastral PDF\",\n",
    "    \"Paris étude circulation PDF\",\n",
    "    \"Paris plans réseaux souterrains PDF\",\n",
    "    # New York\n",
    "    \"New York soil studies geology PDF\",\n",
    "    \"New York infrastructure report PDF\",\n",
    "    \"New York cadastral document PDF\",\n",
    "    \"New York traffic study PDF\",\n",
    "    \"New York underground network plans PDF\",\n",
    "    # Tokyo\n",
    "    \"Tokyo soil studies geology PDF\",\n",
    "    \"Tokyo infrastructure report PDF\",\n",
    "    \"Tokyo cadastral document PDF\",\n",
    "    \"Tokyo traffic study PDF\",\n",
    "    \"Tokyo underground network plans PDF\",\n",
    "]\n",
    "\n",
    "\n",
    "# Fonction pour télécharger un fichier PDF\n",
    "def download_pdf(url, city_name, count):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200 and \"application/pdf\" in response.headers.get(\"Content-Type\", \"\"):\n",
    "            filename = os.path.join(PDF_DIR, f\"{city_name}_doc_{count}.pdf\")\n",
    "            with open(filename, \"wb\") as f:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "            print(f\"PDF téléchargé : {filename}\")\n",
    "            return filename\n",
    "        else:\n",
    "            print(f\"Pas un PDF valide ou erreur HTTP : {url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du téléchargement de {url} : {e}\")\n",
    "        return None\n",
    "\n",
    "# Fonction pour extraire le contenu textuel des PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        content = \"\"\n",
    "        for page in doc:\n",
    "            content += page.get_text()\n",
    "        doc.close()\n",
    "        return content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'extraction du texte de {pdf_path} : {e}\")\n",
    "        return None\n",
    "\n",
    "# Fonction pour normaliser les données et les sauvegarder en JSON\n",
    "def normalize_and_save_pdf(pdf_path, city_name):\n",
    "    try:\n",
    "        content = extract_text_from_pdf(pdf_path)\n",
    "        if content:\n",
    "            # Créer un dictionnaire structuré\n",
    "            normalized_data = {\n",
    "                \"city\": city_name,\n",
    "                \"file_name\": os.path.basename(pdf_path),\n",
    "                \"content\": content,\n",
    "            }\n",
    "            # Sauvegarder en JSON\n",
    "            json_path = os.path.join(NORMALIZED_DIR, os.path.basename(pdf_path).replace(\".pdf\", \".json\"))\n",
    "            with open(json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "                json.dump(normalized_data, json_file, ensure_ascii=False, indent=4)\n",
    "            print(f\"Données normalisées sauvegardées : {json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la normalisation de {pdf_path} : {e}\")\n",
    "\n",
    "# Fonction pour rechercher et télécharger des PDF\n",
    "def search_and_download_pdfs(cities_keywords, max_results=3):\n",
    "    base_url = \"https://html.duckduckgo.com/html/\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    for query in cities_keywords:\n",
    "        city_name = query.split()[0]  # Utiliser le premier mot (nom de la ville) pour nommer les fichiers\n",
    "        print(f\"Recherche de PDF pour : {query}\")\n",
    "        try:\n",
    "            data = {\"q\": query, \"t\": \"h_\", \"ia\": \"web\"}\n",
    "            response = requests.post(base_url, data=data, headers=headers)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            links_found = 0\n",
    "            downloaded_links = set()\n",
    "\n",
    "            # Parcourir les liens pour trouver des PDF\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                href = link[\"href\"]\n",
    "                if \".pdf\" in href.lower() and links_found < max_results:\n",
    "                    pdf_url = href if href.startswith(\"http\") else f\"https://{href}\"\n",
    "\n",
    "                    if pdf_url not in downloaded_links:\n",
    "                        pdf_path = download_pdf(pdf_url, city_name, links_found + 1)\n",
    "                        if pdf_path:\n",
    "                            normalize_and_save_pdf(pdf_path, city_name)\n",
    "                        downloaded_links.add(pdf_url)\n",
    "                        links_found += 1\n",
    "\n",
    "            if links_found == 0:\n",
    "                print(f\"Aucun PDF trouvé pour {city_name}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la recherche pour {query} : {e}\")\n",
    "\n",
    "# Lancer la recherche, le téléchargement et la normalisation\n",
    "search_and_download_pdfs(cities_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " validation et le nettoyage des données normalisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation et nettoyage des fichiers normalisés...\n",
      "Document non pertinent : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\New_doc_1.json\n",
      "Fichier supprimé : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\New_doc_1.json\n",
      "Enregistrement des fichiers PDF invalides...\n",
      "Liste des fichiers invalides enregistrée dans C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\invalid_pdfs.log\n",
      "Processus terminé.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Chemin vers le répertoire des données normalisées\n",
    "NORMALIZED_DIR = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\"\n",
    "LOG_FILE = os.path.join(NORMALIZED_DIR, \"invalid_pdfs.log\")\n",
    "\n",
    "# Fonction pour valider les données normalisées\n",
    "def validate_normalized_data(json_path, keywords):\n",
    "    \"\"\"\n",
    "    Valide si le contenu d'un document normalisé est pertinent.\n",
    "    \n",
    "    :param json_path: Chemin vers le fichier JSON normalisé.\n",
    "    :param keywords: Liste de mots-clés pour vérifier la pertinence.\n",
    "    :return: True si le document est valide, sinon False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        content = data.get(\"content\", \"\").strip()\n",
    "        \n",
    "        # Vérifier si le contenu est vide\n",
    "        if not content:\n",
    "            print(f\"Document vide détecté : {json_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Vérifier la pertinence avec les mots-clés\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in content.lower():\n",
    "                return True\n",
    "        \n",
    "        print(f\"Document non pertinent : {json_path}\")\n",
    "        return False\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la validation du fichier {json_path} : {e}\")\n",
    "        return False\n",
    "\n",
    "# Fonction pour nettoyer les données normalisées\n",
    "def clean_normalized_data(normalized_dir, keywords):\n",
    "    \"\"\"\n",
    "    Parcourt les fichiers normalisés et supprime les documents invalides.\n",
    "    \n",
    "    :param normalized_dir: Répertoire contenant les fichiers JSON normalisés.\n",
    "    :param keywords: Liste de mots-clés pour la validation.\n",
    "    \"\"\"\n",
    "    for json_file in os.listdir(normalized_dir):\n",
    "        json_path = os.path.join(normalized_dir, json_file)\n",
    "        if not validate_normalized_data(json_path, keywords):\n",
    "            os.remove(json_path)\n",
    "            print(f\"Fichier supprimé : {json_path}\")\n",
    "\n",
    "# Fonction pour enregistrer les fichiers PDF invalides\n",
    "def log_invalid_pdfs(normalized_dir, log_file):\n",
    "    \"\"\"\n",
    "    Enregistre les chemins des fichiers PDF invalides dans un fichier de log.\n",
    "    \n",
    "    :param normalized_dir: Répertoire contenant les fichiers JSON normalisés.\n",
    "    :param log_file: Chemin vers le fichier de log.\n",
    "    \"\"\"\n",
    "    invalid_files = []\n",
    "    \n",
    "    for json_file in os.listdir(normalized_dir):\n",
    "        json_path = os.path.join(normalized_dir, json_file)\n",
    "        try:\n",
    "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                content = data.get(\"content\", \"\").strip()\n",
    "                if not content:\n",
    "                    invalid_files.append(data.get(\"file_name\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la lecture du fichier {json_path} : {e}\")\n",
    "    \n",
    "    # Sauvegarder les fichiers invalides dans un log\n",
    "    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for invalid_file in invalid_files:\n",
    "            f.write(invalid_file + \"\\n\")\n",
    "    \n",
    "    print(f\"Liste des fichiers invalides enregistrée dans {log_file}\")\n",
    "\n",
    "# Liste des mots-clés pour valider la pertinence\n",
    "keywords = [\"géologie\", \"infrastructure\", \"cadastral\", \"circulation\", \"réseaux\"]\n",
    "\n",
    "# Lancer le nettoyage et la validation\n",
    "print(\"Validation et nettoyage des fichiers normalisés...\")\n",
    "clean_normalized_data(NORMALIZED_DIR, keywords)\n",
    "\n",
    "# Enregistrer les fichiers invalides\n",
    "print(\"Enregistrement des fichiers PDF invalides...\")\n",
    "log_invalid_pdfs(NORMALIZED_DIR, LOG_FILE)\n",
    "\n",
    "print(\"Processus terminé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Fractionnement des JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractionnement des fichiers JSON...\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Abidjan_doc_1.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Abidjan_doc_2.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Abidjan_doc_3.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Bamako_doc_1.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Bamako_doc_2.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Bamako_doc_3.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Cotonou_doc_1.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Cotonou_doc_2.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Cotonou_doc_3.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Libreville_doc_1.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Libreville_doc_2.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Libreville_doc_3.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\New_doc_2.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\New_doc_3.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Paris_doc_1.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Paris_doc_2.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Paris_doc_3.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Tokyo_doc_1.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Tokyo_doc_2.json\n",
      "Fractionnement terminé pour : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\\Tokyo_doc_3.json\n",
      "Processus terminé.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Répertoires\n",
    "NORMALIZED_DIR = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\normalized_data\"\n",
    "FRACTIONED_DIR = os.path.join(NORMALIZED_DIR, \"fractioned_data\")\n",
    "os.makedirs(FRACTIONED_DIR, exist_ok=True)\n",
    "\n",
    "# Fonction pour fractionner le contenu textuel\n",
    "def split_content_into_segments(content, max_length=500):\n",
    "    \"\"\"\n",
    "    Fractionne le contenu textuel en segments de taille limitée.\n",
    "    \n",
    "    :param content: Texte complet à fractionner.\n",
    "    :param max_length: Nombre maximal de caractères par segment.\n",
    "    :return: Liste de segments.\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    words = content.split()\n",
    "    segment = []\n",
    "\n",
    "    for word in words:\n",
    "        if len(\" \".join(segment + [word])) <= max_length:\n",
    "            segment.append(word)\n",
    "        else:\n",
    "            segments.append(\" \".join(segment))\n",
    "            segment = [word]\n",
    "\n",
    "    if segment:\n",
    "        segments.append(\" \".join(segment))\n",
    "    \n",
    "    return segments\n",
    "\n",
    "# Fonction pour fractionner un fichier JSON\n",
    "def fractionate_json(json_path, output_dir, max_length=500):\n",
    "    \"\"\"\n",
    "    Fractionne le contenu d'un JSON en segments et les sauvegarde individuellement.\n",
    "    \n",
    "    :param json_path: Chemin vers le fichier JSON normalisé.\n",
    "    :param output_dir: Répertoire pour sauvegarder les segments fractionnés.\n",
    "    :param max_length: Longueur maximale d'un segment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        content = data.get(\"content\", \"\").strip()\n",
    "        if not content:\n",
    "            print(f\"Contenu vide dans : {json_path}\")\n",
    "            return\n",
    "\n",
    "        # Fractionner le contenu\n",
    "        segments = split_content_into_segments(content, max_length)\n",
    "\n",
    "        for i, segment in enumerate(segments):\n",
    "            segment_data = {\n",
    "                \"city\": data.get(\"city\"),\n",
    "                \"file_name\": data.get(\"file_name\"),\n",
    "                \"segment_id\": i + 1,\n",
    "                \"segment_content\": segment,\n",
    "                \"document_type\": data.get(\"document_type\", \"unknown\"),\n",
    "            }\n",
    "\n",
    "            # Sauvegarder chaque segment dans un fichier JSON séparé\n",
    "            segment_file = os.path.join(\n",
    "                output_dir, f\"{os.path.basename(json_path).replace('.json', '')}_segment_{i + 1}.json\"\n",
    "            )\n",
    "            with open(segment_file, \"w\", encoding=\"utf-8\") as segment_f:\n",
    "                json.dump(segment_data, segment_f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Fractionnement terminé pour : {json_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du fractionnement de {json_path} : {e}\")\n",
    "\n",
    "# Fonction pour parcourir tous les fichiers JSON et les fractionner\n",
    "def process_all_json_files(normalized_dir, fractioned_dir, max_length=500):\n",
    "    \"\"\"\n",
    "    Parcourt tous les fichiers JSON normalisés et les fractionne.\n",
    "    \n",
    "    :param normalized_dir: Répertoire contenant les fichiers JSON normalisés.\n",
    "    :param fractioned_dir: Répertoire pour sauvegarder les segments fractionnés.\n",
    "    :param max_length: Longueur maximale d'un segment.\n",
    "    \"\"\"\n",
    "    for json_file in os.listdir(normalized_dir):\n",
    "        json_path = os.path.join(normalized_dir, json_file)\n",
    "        if json_file.endswith(\".json\"):\n",
    "            fractionate_json(json_path, fractioned_dir, max_length)\n",
    "\n",
    "# Exécution du fractionnement\n",
    "print(\"Fractionnement des fichiers JSON...\")\n",
    "process_all_json_files(NORMALIZED_DIR, FRACTIONED_DIR)\n",
    "print(\"Processus terminé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecte et Traitement des images satellitaires acquises sur SentilHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: shapely in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (2.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from shapely) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wheel in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (0.45.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pipwin in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: docopt in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (0.6.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (2.32.3)\n",
      "Requirement already satisfied: pyprind in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (2.11.3)\n",
      "Requirement already satisfied: six in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9.0 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (4.12.3)\n",
      "Requirement already satisfied: js2py in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (0.74)\n",
      "Requirement already satisfied: packaging in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (24.1)\n",
      "Requirement already satisfied: pySmartDL>=1.3.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from pipwin) (1.3.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from beautifulsoup4>=4.9.0->pipwin) (2.6)\n",
      "Requirement already satisfied: tzlocal>=1.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from js2py->pipwin) (5.2)\n",
      "Requirement already satisfied: pyjsparser>=2.5.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from js2py->pipwin) (2.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests->pipwin) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests->pipwin) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests->pipwin) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from requests->pipwin) (2024.8.30)\n",
      "Requirement already satisfied: tzdata in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from tzlocal>=1.2->js2py->pipwin) (2024.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Scripts\\pipwin.exe\\__main__.py\", line 4, in <module>\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\pipwin\\command.py\", line 28, in <module>\n",
      "    from . import pipwin, __version__\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\pipwin\\pipwin.py\", line 13, in <module>\n",
      "    import js2py\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\__init__.py\", line 72, in <module>\n",
      "    from .base import PyJsException\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\base.py\", line 2965, in <module>\n",
      "    @Js\n",
      "     ^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\base.py\", line 165, in Js\n",
      "    return PyJsFunction(val, FunctionPrototype)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\base.py\", line 1377, in __init__\n",
      "    cand = fix_js_args(func)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\utils\\injector.py\", line 27, in fix_js_args\n",
      "    code = append_arguments(six.get_function_code(func), ('this', 'arguments'))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\utils\\injector.py\", line 121, in append_arguments\n",
      "    arg = name_translations[inst.arg]\n",
      "          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shapely==2.0.1Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached shapely-2.0.1.tar.gz (275 kB)\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [3 lines of output]\n",
      "      <string>:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "      Could not find geos-config executable. Either append the path to geos-config to PATH or manually provide the include_dirs, library_dirs, libraries and other link args for compiling against a GEOS version >=3.5.\n",
      "      ERROR: Cython is required to build shapely from source.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Scripts\\pipwin.exe\\__main__.py\", line 4, in <module>\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\pipwin\\command.py\", line 28, in <module>\n",
      "    from . import pipwin, __version__\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\pipwin\\pipwin.py\", line 13, in <module>\n",
      "    import js2py\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\__init__.py\", line 72, in <module>\n",
      "    from .base import PyJsException\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\base.py\", line 2965, in <module>\n",
      "    @Js\n",
      "     ^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\base.py\", line 165, in Js\n",
      "    return PyJsFunction(val, FunctionPrototype)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\base.py\", line 1377, in __init__\n",
      "    cand = fix_js_args(func)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\utils\\injector.py\", line 27, in fix_js_args\n",
      "    code = append_arguments(six.get_function_code(func), ('this', 'arguments'))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\js2py\\utils\\injector.py\", line 121, in append_arguments\n",
      "    arg = name_translations[inst.arg]\n",
      "          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 3\n"
     ]
    }
   ],
   "source": [
    "# Importation des library pour la collecte et traitement des données \n",
    "%pip install requests shapely\n",
    "%pip install wheel\n",
    "%pip install pipwin\n",
    "%pipwin install GDAL==3.6.0\n",
    "%pip install shapely==2.0.1 --no-build-isolation\n",
    "%pipwin install rasterio==1.3.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Collecte de donnée géospatiale ( image sentinel ) sur la plateforme SentinelHub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-01.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-01.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-02.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-02.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-03.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-03.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-04.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-04.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-05.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-05.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-06.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-06.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-07.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-07.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-08.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-08.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-09.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-09.json\n",
      "Image satellite téléchargée pour Abidjan : images/Abidjan/Abidjan_2023-10-10.jpg\n",
      "Métadonnées sauvegardées pour Abidjan : images/Abidjan/Abidjan_2023-10-10.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-01.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-01.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-02.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-02.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-03.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-03.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-04.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-04.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-05.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-05.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-06.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-06.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-07.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-07.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-08.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-08.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-09.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-09.json\n",
      "Image satellite téléchargée pour Cotonou : images/Cotonou/Cotonou_2023-10-10.jpg\n",
      "Métadonnées sauvegardées pour Cotonou : images/Cotonou/Cotonou_2023-10-10.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-01.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-01.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-02.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-02.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-03.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-03.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-04.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-04.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-05.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-05.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-06.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-06.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-07.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-07.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-08.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-08.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-09.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-09.json\n",
      "Image satellite téléchargée pour Bamako : images/Bamako/Bamako_2023-10-10.jpg\n",
      "Métadonnées sauvegardées pour Bamako : images/Bamako/Bamako_2023-10-10.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-01.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-01.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-02.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-02.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-03.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-03.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-04.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-04.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-05.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-05.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-06.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-06.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-07.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-07.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-08.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-08.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-09.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-09.json\n",
      "Image satellite téléchargée pour Libreville : images/Libreville/Libreville_2023-10-10.jpg\n",
      "Métadonnées sauvegardées pour Libreville : images/Libreville/Libreville_2023-10-10.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-01.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-01.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-02.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-02.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-03.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-03.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-04.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-04.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-05.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-05.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-06.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-06.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-07.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-07.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-08.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-08.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-09.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-09.json\n",
      "Image satellite téléchargée pour Paris : images/Paris/Paris_2023-10-10.jpg\n",
      "Métadonnées sauvegardées pour Paris : images/Paris/Paris_2023-10-10.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-01.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-01.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-02.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-02.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-03.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-03.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-04.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-04.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-05.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-05.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-06.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-06.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-07.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-07.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-08.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-08.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-09.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-09.json\n",
      "Image satellite téléchargée pour New York : images/New York/New York_2023-10-10.jpg\n",
      "Métadonnées sauvegardées pour New York : images/New York/New York_2023-10-10.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-01.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-01.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-02.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-02.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-03.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-03.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-04.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-04.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-05.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-05.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-06.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-06.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-07.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-07.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-08.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-08.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-09.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-09.json\n",
      "Image satellite téléchargée pour Tokyo : images/Tokyo/Tokyo_2023-10-10.jpg\n",
      "Métadonnées sauvegardées pour Tokyo : images/Tokyo/Tokyo_2023-10-10.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.geometry import mapping, box\n",
    "import os\n",
    "\n",
    "# Informations pour l'accès à l'API Sentinel Hub\n",
    "CLIENT_ID = 'c576337e-156f-4bfb-bec1-da8c343c7a00'\n",
    "CLIENT_SECRET = 'hJ1yQ9PI07MJw4R9fVpooFvpwkL59U6x'\n",
    "\n",
    "def get_sentinel_token():\n",
    "    \"\"\"\n",
    "    Obtient un jeton d'authentification pour l'API Sentinel Hub.\n",
    "    \"\"\"\n",
    "    url = \"https://services.sentinel-hub.com/oauth/token\"\n",
    "    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "    data = {\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': CLIENT_ID,\n",
    "        'client_secret': CLIENT_SECRET\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=data)\n",
    "    response.raise_for_status()  # Gérer les erreurs d'authentification\n",
    "    return response.json().get(\"access_token\")\n",
    "\n",
    "def fetch_satellite_image(city_name, latitude, longitude, date='2023-10-01', resolution=10, bbox_size=0.05):\n",
    "    \"\"\"\n",
    "    Télécharge une image satellite pour une position géographique donnée\n",
    "    et crée un fichier JSON contenant les métadonnées de l'image.\n",
    "    \"\"\"\n",
    "    # Création de la bbox autour du point central (ville)\n",
    "    bbox = box(\n",
    "        longitude - bbox_size, latitude - bbox_size,\n",
    "        longitude + bbox_size, latitude + bbox_size\n",
    "    )\n",
    "\n",
    "    # Ajouter l'evalscript pour l'imagerie RGB\n",
    "    evalscript = \"\"\"\n",
    "    // Evalscript pour une image RGB\n",
    "    // Sélectionne les canaux R, G, B pour l'affichage des couleurs naturelles\n",
    "    function setup() {\n",
    "        return {\n",
    "            input: [\"B04\", \"B03\", \"B02\"],\n",
    "            output: { bands: 3 }\n",
    "        };\n",
    "    }\n",
    "    function evaluatePixel(sample) {\n",
    "        return [sample.B04, sample.B03, sample.B02];\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    # Préparer la demande à l'API Sentinel Hub\n",
    "    url = \"https://services.sentinel-hub.com/api/v1/process\"\n",
    "    token = get_sentinel_token()\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"input\": {\n",
    "            \"bounds\": {\"geometry\": mapping(bbox)},\n",
    "            \"data\": [\n",
    "                {\n",
    "                    \"type\": \"S2L2A\",\n",
    "                    \"dataFilter\": {\n",
    "                        \"timeRange\": {\n",
    "                            \"from\": f\"{date}T00:00:00Z\",\n",
    "                            \"to\": f\"{date}T23:59:59Z\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"output\": {\n",
    "            \"width\": int(1000 / resolution),\n",
    "            \"height\": int(1000 / resolution),\n",
    "            \"responses\": [\n",
    "                {\"identifier\": \"default\", \"format\": {\"type\": \"image/jpeg\"}}\n",
    "            ]\n",
    "        },\n",
    "        \"evalscript\": evalscript  # Ajouter l'evalscript ici\n",
    "    }\n",
    "\n",
    "    # Envoyer la demande et gérer les erreurs éventuelles\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        # Créer le répertoire pour la ville\n",
    "        directory = f\"images/{city_name}\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Sauvegarder l'image\n",
    "        image_filename = f\"{directory}/{city_name}_{date}.jpg\"\n",
    "        with open(image_filename, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Image satellite téléchargée pour {city_name} : {image_filename}\")\n",
    "\n",
    "        # Créer et sauvegarder les métadonnées dans un fichier JSON\n",
    "        metadata = {\n",
    "            \"city\": city_name,\n",
    "            \"date\": date,\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"resolution\": resolution,\n",
    "            \"bbox\": {\n",
    "                \"min_lat\": latitude - bbox_size,\n",
    "                \"min_lon\": longitude - bbox_size,\n",
    "                \"max_lat\": latitude + bbox_size,\n",
    "                \"max_lon\": longitude + bbox_size\n",
    "            },\n",
    "            \"image_path\": image_filename\n",
    "        }\n",
    "        metadata_filename = f\"{directory}/{city_name}_{date}.json\"\n",
    "        with open(metadata_filename, \"w\", encoding=\"utf-8\") as metadata_file:\n",
    "            json.dump(metadata, metadata_file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Métadonnées sauvegardées pour {city_name} : {metadata_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Erreur pour {city_name} à la date {date}: {response.text}\")\n",
    "\n",
    "def collect_images_for_cities(cities, dates):\n",
    "    \"\"\"\n",
    "    Télécharge des images satellite pour une liste de villes et de dates données.\n",
    "    \"\"\"\n",
    "    for city in cities:\n",
    "        for date in dates:\n",
    "            fetch_satellite_image(city[\"name\"], city[\"latitude\"], city[\"longitude\"], date=date)\n",
    "\n",
    "# Liste des villes demandées\n",
    "cities = [\n",
    "    {\"name\": \"Abidjan\", \"latitude\": 5.347, \"longitude\": -4.0244},\n",
    "    {\"name\": \"Cotonou\", \"latitude\": 6.3703, \"longitude\": 2.3912},\n",
    "    {\"name\": \"Bamako\", \"latitude\": 12.6392, \"longitude\": -8.0029},\n",
    "    {\"name\": \"Libreville\", \"latitude\": 0.4162, \"longitude\": 9.4673},\n",
    "    {\"name\": \"Paris\", \"latitude\": 48.8566, \"longitude\": 2.3522},\n",
    "    {\"name\": \"New York\", \"latitude\": 40.7128, \"longitude\": -74.0060},\n",
    "    {\"name\": \"Tokyo\", \"latitude\": 35.6895, \"longitude\": 139.6917}\n",
    "]\n",
    "\n",
    "# Liste de dates pour la collecte d'images\n",
    "start_date = datetime(2023, 10, 1)\n",
    "end_date = datetime(2023, 10, 10)\n",
    "dates = [(start_date + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range((end_date - start_date).days + 1)]\n",
    "\n",
    "# Collecte des images\n",
    "collect_images_for_cities(cities, dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecte de la donnée  sur OPenstreetMap (OSM) pour plusieurs villes à partir d'un API open acces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [5.3, -4.05, 5.4, -4.0]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Abidjan_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Abidjan_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [5.3, -4.05, 5.4, -4.0]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Abidjan_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Abidjan_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [5.3, -4.05, 5.4, -4.0]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Abidjan_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Abidjan_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [6.34, 2.36, 6.38, 2.4]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Cotonou_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Cotonou_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [6.34, 2.36, 6.38, 2.4]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Cotonou_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Cotonou_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [6.34, 2.36, 6.38, 2.4]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Cotonou_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Cotonou_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [12.6, -8.05, 12.65, -7.95]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Bamako_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Bamako_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [12.6, -8.05, 12.65, -7.95]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Bamako_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Bamako_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [12.6, -8.05, 12.65, -7.95]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Bamako_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Bamako_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [0.37, 9.4, 0.46, 9.5]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Libreville_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Libreville_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [0.37, 9.4, 0.46, 9.5]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Libreville_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Libreville_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [0.37, 9.4, 0.46, 9.5]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Libreville_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Libreville_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [48.8156, 2.2242, 48.9021, 2.4699]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Paris_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Paris_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [48.8156, 2.2242, 48.9021, 2.4699]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Paris_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Paris_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [48.8156, 2.2242, 48.9021, 2.4699]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Paris_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Paris_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [40.7, -74.02, 40.78, -73.93]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\New York_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\New York_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [40.7, -74.02, 40.78, -73.93]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\New York_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\New York_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [40.7, -74.02, 40.78, -73.93]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\New York_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\New York_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [35.68, 139.75, 35.7, 139.77]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Tokyo_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Tokyo_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [35.68, 139.75, 35.7, 139.77]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Tokyo_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Tokyo_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [35.68, 139.75, 35.7, 139.77]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Tokyo_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Tokyo_amenity_metadata.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from geojson import FeatureCollection, Feature, Point, LineString\n",
    "from shapely.geometry import Point as ShapelyPoint\n",
    "\n",
    "def overpass_to_geojson(overpass_data):\n",
    "    \"\"\"\n",
    "    Convertit les données Overpass API en GeoJSON valide avec une colonne 'geom'.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Créer un dictionnaire pour accéder rapidement aux nodes par ID\n",
    "    nodes = {element[\"id\"]: element for element in overpass_data.get(\"elements\", []) if element[\"type\"] == \"node\"}\n",
    "\n",
    "    for element in overpass_data.get(\"elements\", []):\n",
    "        if element[\"type\"] == \"node\":\n",
    "            # Ajouter des points avec une colonne 'geom' contenant la géométrie\n",
    "            geom = ShapelyPoint(element[\"lon\"], element[\"lat\"])\n",
    "            feature = Feature(\n",
    "                geometry=Point((element[\"lon\"], element[\"lat\"])),\n",
    "                properties={**element.get(\"tags\", {}), \"geom\": geom.wkt}  # Ajout du WKT dans les propriétés\n",
    "            )\n",
    "            features.append(feature)\n",
    "        elif element[\"type\"] == \"way\":\n",
    "            # Construire des lignes en utilisant les nœuds associés\n",
    "            coordinates = [(nodes[node_id][\"lon\"], nodes[node_id][\"lat\"]) for node_id in element.get(\"nodes\", []) if node_id in nodes]\n",
    "            feature = Feature(\n",
    "                geometry=LineString(coordinates),\n",
    "                properties=element.get(\"tags\", {})\n",
    "            )\n",
    "            features.append(feature)\n",
    "\n",
    "    return FeatureCollection(features)\n",
    "\n",
    "\n",
    "def download_osm_data(bbox, output_path, query_type, metadata_output, city_name):\n",
    "    \"\"\"\n",
    "    Télécharge les données OpenStreetMap pour une zone géographique donnée (bbox) via Overpass API\n",
    "    et génère un fichier GeoJSON valide avec métadonnées et une colonne 'geom'.\n",
    "    \"\"\"\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "\n",
    "    overpass_query = f\"\"\"\n",
    "    [out:json];\n",
    "    (\n",
    "      node[\"{query_type}\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      way[\"{query_type}\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      relation[\"{query_type}\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        print(f\"Envoi de la requête à Overpass API pour le type '{query_type}' et la zone {bbox}\")\n",
    "        response = requests.get(overpass_url, params={\"data\": overpass_query})\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            # Conversion des données Overpass en GeoJSON\n",
    "            geojson_data = overpass_to_geojson(data)\n",
    "\n",
    "            # Sauvegarde des données en GeoJSON\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(geojson_data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Données téléchargées et sauvegardées dans : {output_path}\")\n",
    "\n",
    "            # Création des métadonnées\n",
    "            metadata = {\n",
    "                \"city\": city_name,\n",
    "                \"type\": query_type,\n",
    "                \"bbox\": {\n",
    "                    \"min_lat\": bbox[0],\n",
    "                    \"min_lon\": bbox[1],\n",
    "                    \"max_lat\": bbox[2],\n",
    "                    \"max_lon\": bbox[3]\n",
    "                },\n",
    "                \"file_path\": output_path\n",
    "            }\n",
    "\n",
    "            # Sauvegarde des métadonnées\n",
    "            with open(metadata_output, \"w\", encoding=\"utf-8\") as meta_file:\n",
    "                json.dump(metadata, meta_file, ensure_ascii=False, indent=2)\n",
    "            print(f\"Métadonnées sauvegardées dans : {metadata_output}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Erreur : statut {response.status_code}. Requête échouée pour la zone {bbox}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue lors de la requête Overpass : {e}\")\n",
    "\n",
    "\n",
    "# Liste des villes avec leurs coordonnées bbox\n",
    "cities = [\n",
    "    {\"name\": \"Abidjan\", \"bbox\": [5.3000, -4.0500, 5.4000, -4.0000]},\n",
    "    {\"name\": \"Cotonou\", \"bbox\": [6.3400, 2.3600, 6.3800, 2.4000]},\n",
    "    {\"name\": \"Bamako\", \"bbox\": [12.6000, -8.0500, 12.6500, -7.9500]},\n",
    "    {\"name\": \"Libreville\", \"bbox\": [0.3700, 9.4000, 0.4600, 9.5000]},\n",
    "    {\"name\": \"Paris\", \"bbox\": [48.8156, 2.2242, 48.9021, 2.4699]},\n",
    "    {\"name\": \"New York\", \"bbox\": [40.7000, -74.0200, 40.7800, -73.9300]},\n",
    "    {\"name\": \"Tokyo\", \"bbox\": [35.6800, 139.7500, 35.7000, 139.7700]},\n",
    "]\n",
    "\n",
    "# Types de données OSM à télécharger\n",
    "query_types = [\"highway\", \"building\", \"amenity\"]\n",
    "\n",
    "# Dossier de sortie\n",
    "output_folder = \"osm_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Télécharger les données pour chaque ville et chaque type\n",
    "for city in cities:\n",
    "    for query_type in query_types:\n",
    "        output_file = os.path.join(output_folder, f\"{city['name']}_{query_type}.geojson\")\n",
    "        metadata_file = os.path.join(output_folder, f\"{city['name']}_{query_type}_metadata.json\")\n",
    "        download_osm_data(city[\"bbox\"], output_file, query_type=query_type, metadata_output=metadata_file, city_name=city[\"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [5.3, -4.05, 5.4, -4.0]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Abidjan_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Abidjan_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [5.3, -4.05, 5.4, -4.0]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Abidjan_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Abidjan_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [5.3, -4.05, 5.4, -4.0]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Abidjan_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Abidjan_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [6.34, 2.36, 6.38, 2.4]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Cotonou_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Cotonou_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [6.34, 2.36, 6.38, 2.4]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Cotonou_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Cotonou_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [6.34, 2.36, 6.38, 2.4]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Cotonou_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Cotonou_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [12.6, -8.05, 12.65, -7.95]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Bamako_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Bamako_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [12.6, -8.05, 12.65, -7.95]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Bamako_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Bamako_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [12.6, -8.05, 12.65, -7.95]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Bamako_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Bamako_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [0.37, 9.4, 0.46, 9.5]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Libreville_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Libreville_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [0.37, 9.4, 0.46, 9.5]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Libreville_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Libreville_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [0.37, 9.4, 0.46, 9.5]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Libreville_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Libreville_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [48.8156, 2.2242, 48.9021, 2.4699]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Paris_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Paris_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [48.8156, 2.2242, 48.9021, 2.4699]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Paris_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Paris_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [48.8156, 2.2242, 48.9021, 2.4699]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Paris_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Paris_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [40.7, -74.02, 40.78, -73.93]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\New York_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\New York_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [40.7, -74.02, 40.78, -73.93]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\New York_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\New York_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [40.7, -74.02, 40.78, -73.93]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\New York_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\New York_amenity_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'highway' et la zone [35.68, 139.75, 35.7, 139.77]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Tokyo_highway.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Tokyo_highway_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'building' et la zone [35.68, 139.75, 35.7, 139.77]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Tokyo_building.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Tokyo_building_metadata.json\n",
      "Envoi de la requête à Overpass API pour le type 'amenity' et la zone [35.68, 139.75, 35.7, 139.77]\n",
      "Données téléchargées et sauvegardées dans : osm_data\\Tokyo_amenity.geojson\n",
      "Métadonnées sauvegardées dans : osm_data\\Tokyo_amenity_metadata.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from geojson import FeatureCollection, Feature, Point, LineString, Polygon\n",
    "\n",
    "def overpass_to_geojson(overpass_data):\n",
    "    \"\"\"\n",
    "    Convertit les données Overpass API en GeoJSON valide.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Créer un dictionnaire pour accéder rapidement aux nodes par ID\n",
    "    nodes = {element[\"id\"]: element for element in overpass_data.get(\"elements\", []) if element[\"type\"] == \"node\"}\n",
    "\n",
    "    for element in overpass_data.get(\"elements\", []):\n",
    "        if element[\"type\"] == \"node\":\n",
    "            # Ajouter des points\n",
    "            features.append(\n",
    "                Feature(\n",
    "                    geometry=Point((element[\"lon\"], element[\"lat\"])),\n",
    "                    properties=element.get(\"tags\", {})\n",
    "                )\n",
    "            )\n",
    "        elif element[\"type\"] == \"way\":\n",
    "            # Construire des lignes en utilisant les nœuds associés\n",
    "            coordinates = [(nodes[node_id][\"lon\"], nodes[node_id][\"lat\"]) for node_id in element.get(\"nodes\", []) if node_id in nodes]\n",
    "            features.append(\n",
    "                Feature(\n",
    "                    geometry=LineString(coordinates),\n",
    "                    properties=element.get(\"tags\", {})\n",
    "                )\n",
    "            )\n",
    "        elif element[\"type\"] == \"relation\":\n",
    "            # Les relations nécessitent une gestion spécifique pour construire des polygones ou autres\n",
    "            # Ici, on peut les ignorer ou gérer les cas les plus fréquents\n",
    "            pass\n",
    "\n",
    "    return FeatureCollection(features)\n",
    "\n",
    "\n",
    "def download_osm_data(bbox, output_path, query_type, metadata_output, city_name):\n",
    "    \"\"\"\n",
    "    Télécharge les données OpenStreetMap pour une zone géographique donnée (bbox) via Overpass API\n",
    "    et génère un fichier GeoJSON valide avec métadonnées.\n",
    "    \"\"\"\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "\n",
    "    overpass_query = f\"\"\"\n",
    "    [out:json];\n",
    "    (\n",
    "      node[\"{query_type}\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      way[\"{query_type}\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "      relation[\"{query_type}\"]({bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]});\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        print(f\"Envoi de la requête à Overpass API pour le type '{query_type}' et la zone {bbox}\")\n",
    "        response = requests.get(overpass_url, params={\"data\": overpass_query})\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            # Conversion des données Overpass en GeoJSON\n",
    "            geojson_data = overpass_to_geojson(data)\n",
    "\n",
    "            # Sauvegarde des données en GeoJSON\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(geojson_data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Données téléchargées et sauvegardées dans : {output_path}\")\n",
    "\n",
    "            # Création des métadonnées\n",
    "            metadata = {\n",
    "                \"city\": city_name,\n",
    "                \"type\": query_type,\n",
    "                \"bbox\": {\n",
    "                    \"min_lat\": bbox[0],\n",
    "                    \"min_lon\": bbox[1],\n",
    "                    \"max_lat\": bbox[2],\n",
    "                    \"max_lon\": bbox[3]\n",
    "                },\n",
    "                \"file_path\": output_path\n",
    "            }\n",
    "\n",
    "            # Sauvegarde des métadonnées\n",
    "            with open(metadata_output, \"w\", encoding=\"utf-8\") as meta_file:\n",
    "                json.dump(metadata, meta_file, ensure_ascii=False, indent=2)\n",
    "            print(f\"Métadonnées sauvegardées dans : {metadata_output}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Erreur : statut {response.status_code}. Requête échouée pour la zone {bbox}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue lors de la requête Overpass : {e}\")\n",
    "\n",
    "\n",
    "# Liste des villes avec leurs coordonnées bbox\n",
    "cities = [\n",
    "    {\"name\": \"Abidjan\", \"bbox\": [5.3000, -4.0500, 5.4000, -4.0000]},\n",
    "    {\"name\": \"Cotonou\", \"bbox\": [6.3400, 2.3600, 6.3800, 2.4000]},\n",
    "    {\"name\": \"Bamako\", \"bbox\": [12.6000, -8.0500, 12.6500, -7.9500]},\n",
    "    {\"name\": \"Libreville\", \"bbox\": [0.3700, 9.4000, 0.4600, 9.5000]},\n",
    "    {\"name\": \"Paris\", \"bbox\": [48.8156, 2.2242, 48.9021, 2.4699]},\n",
    "    {\"name\": \"New York\", \"bbox\": [40.7000, -74.0200, 40.7800, -73.9300]},\n",
    "    {\"name\": \"Tokyo\", \"bbox\": [35.6800, 139.7500, 35.7000, 139.7700]},\n",
    "]\n",
    "\n",
    "# Types de données OSM à télécharger\n",
    "query_types = [\"highway\", \"building\", \"amenity\"]\n",
    "\n",
    "# Dossier de sortie\n",
    "output_folder = \"osm_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Télécharger les données pour chaque ville et chaque type\n",
    "for city in cities:\n",
    "    for query_type in query_types:\n",
    "        output_file = os.path.join(output_folder, f\"{city['name']}_{query_type}.geojson\")\n",
    "        metadata_file = os.path.join(output_folder, f\"{city['name']}_{query_type}_metadata.json\")\n",
    "        download_osm_data(city[\"bbox\"], output_file, query_type=query_type, metadata_output=metadata_file, city_name=city[\"name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection de données educatives et documentaires vidéos avec un  API Open Access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche de vidéos pour : Abidjan géomatique extraction de données\n",
      "Résultats sauvegardés dans : youtube_videos\\Abidjan_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\Abidjan_videos.json\n",
      "Recherche de vidéos pour : Abidjan études de sol géologie\n",
      "Résultats sauvegardés dans : youtube_videos\\Abidjan_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\Abidjan_videos.json\n",
      "Recherche de vidéos pour : Cotonou rapport cartographique\n",
      "Résultats sauvegardés dans : youtube_videos\\Cotonou_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\Cotonou_videos.json\n",
      "Recherche de vidéos pour : Bamako études géologiques\n",
      "Résultats sauvegardés dans : youtube_videos\\Bamako_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\Bamako_videos.json\n",
      "Recherche de vidéos pour : Libreville géomatique rapports cartographiques\n",
      "Résultats sauvegardés dans : youtube_videos\\Libreville_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\Libreville_videos.json\n",
      "Recherche de vidéos pour : Paris géomatique études de sol\n",
      "Résultats sauvegardés dans : youtube_videos\\Paris_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\Paris_videos.json\n",
      "Recherche de vidéos pour : New York géospatial data analysis\n",
      "Résultats sauvegardés dans : youtube_videos\\New_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\New_videos.json\n",
      "Recherche de vidéos pour : Tokyo geomatics data extraction\n",
      "Résultats sauvegardés dans : youtube_videos\\Tokyo_videos.csv\n",
      "Résultats sauvegardés dans : youtube_videos\\Tokyo_videos.json\n"
     ]
    }
   ],
   "source": [
    "from youtubesearchpython import VideosSearch\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Répertoire pour sauvegarder les résultats\n",
    "RESULTS_DIR = \"youtube_videos\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Liste des recherches pour chaque ville\n",
    "search_queries = [\n",
    "    \"Abidjan géomatique extraction de données\",\n",
    "    \"Abidjan études de sol géologie\",\n",
    "    \"Cotonou rapport cartographique\",\n",
    "    \"Bamako études géologiques\",\n",
    "    \"Libreville géomatique rapports cartographiques\",\n",
    "    \"Paris géomatique études de sol\",\n",
    "    \"New York géospatial data analysis\",\n",
    "    \"Tokyo geomatics data extraction\"\n",
    "]\n",
    "\n",
    "# Fonction pour rechercher des vidéos\n",
    "def search_videos(query, max_results=5):\n",
    "    \"\"\"\n",
    "    Recherche des vidéos sur YouTube via l'API Python.\n",
    "    \"\"\"\n",
    "    videos_search = VideosSearch(query, limit=max_results)\n",
    "    results = videos_search.result()[\"result\"]\n",
    "    videos = []\n",
    "    for video in results:\n",
    "        videos.append({\n",
    "            \"title\": video[\"title\"],\n",
    "            \"url\": video[\"link\"],\n",
    "            \"description\": video[\"descriptionSnippet\"][0][\"text\"] if video.get(\"descriptionSnippet\") else \"No description\",\n",
    "            \"city\": query.split()[0],  # Ajouter la ville depuis la requête\n",
    "            \"extraction_date\": datetime.now().strftime('%Y-%m-%d')  # Date d'extraction\n",
    "        })\n",
    "    return videos\n",
    "\n",
    "# Enregistrer les résultats dans un fichier CSV\n",
    "def save_videos_to_csv(videos, city_name):\n",
    "    \"\"\"\n",
    "    Sauvegarde des résultats dans un fichier CSV.\n",
    "    \"\"\"\n",
    "    csv_file = os.path.join(RESULTS_DIR, f\"{city_name}_videos.csv\")\n",
    "    with open(csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"title\", \"url\", \"description\", \"city\", \"extraction_date\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(videos)\n",
    "    print(f\"Résultats sauvegardés dans : {csv_file}\")\n",
    "\n",
    "# Enregistrer les résultats dans un fichier JSON\n",
    "def save_videos_to_json(videos, city_name):\n",
    "    \"\"\"\n",
    "    Sauvegarde des résultats dans un fichier JSON.\n",
    "    \"\"\"\n",
    "    json_file = os.path.join(RESULTS_DIR, f\"{city_name}_videos.json\")\n",
    "    with open(json_file, mode=\"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(videos, file, ensure_ascii=False, indent=2)\n",
    "    print(f\"Résultats sauvegardés dans : {json_file}\")\n",
    "\n",
    "# Recherche et sauvegarde des vidéos\n",
    "def search_and_save_videos(search_queries, max_results=5):\n",
    "    \"\"\"\n",
    "    Recherche et sauvegarde des vidéos pour chaque requête.\n",
    "    \"\"\"\n",
    "    for query in search_queries:\n",
    "        city_name = query.split()[0]  # Utiliser le premier mot comme nom de la ville\n",
    "        print(f\"Recherche de vidéos pour : {query}\")\n",
    "        videos = search_videos(query, max_results=max_results)\n",
    "        if videos:\n",
    "            save_videos_to_csv(videos, city_name)\n",
    "            save_videos_to_json(videos, city_name)\n",
    "        else:\n",
    "            print(f\"Aucune vidéo trouvée pour : {query}\")\n",
    "\n",
    "# Lancer la recherche\n",
    "search_and_save_videos(search_queries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger et Fusionner les CSV et JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Les études géotechniques nécessaires avant tou...   \n",
      "1  Ecole Internationale des Ponts et Chaussées d'...   \n",
      "2  LA TERRE C'EST POUR LES PAUVRES, ARCHITECTURE ...   \n",
      "3  Institut Universitaire d'Abidjan,L'université ...   \n",
      "4  ABIDJAN : Les travaux de construction de 3 nou...   \n",
      "\n",
      "                                         description  \\\n",
      "0  ... de sol on veut ériger cette fondation d'où...   \n",
      "1   L'École Internationale des Ponts et Chaussées d'   \n",
      "2  j'espère que vous avez aimé la vidéo !!! Nous ...   \n",
      "3                                     No description   \n",
      "4  Si vous êtes intéressé par un partenariat avec...   \n",
      "\n",
      "                                           url     city   type metadata  \n",
      "0  https://www.youtube.com/watch?v=KJxl3UOv-h4  Abidjan  video       {}  \n",
      "1  https://www.youtube.com/watch?v=bQnHde84SqY  Abidjan  video       {}  \n",
      "2  https://www.youtube.com/watch?v=6VHgMPIb2-c  Abidjan  video       {}  \n",
      "3  https://www.youtube.com/watch?v=pnZ3g9hjaiE  Abidjan  video       {}  \n",
      "4  https://www.youtube.com/watch?v=NsvwwYX9F-s  Abidjan  video       {}  \n",
      "Fichier fusionné sauvegardé dans : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\youtube_videos\\merged_videos_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Répertoire contenant les fichiers CSV et JSON\n",
    "DATA_DIR = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\youtube_videos\"\n",
    "\n",
    "# Liste des fichiers CSV et JSON\n",
    "csv_files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".csv\")]\n",
    "json_files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".json\")]\n",
    "\n",
    "# Fusionner les CSV avec les métadonnées JSON\n",
    "all_data = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    # Charger le CSV\n",
    "    csv_path = os.path.join(DATA_DIR, csv_file)\n",
    "    videos_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Associer les JSON correspondants\n",
    "    for _, row in videos_df.iterrows():\n",
    "        city = row.get(\"city\", \"\")\n",
    "        json_path = os.path.join(DATA_DIR, f\"{city}.json\")\n",
    "        metadata = {}\n",
    "\n",
    "        if os.path.exists(json_path):\n",
    "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "        \n",
    "        # Fusionner les données\n",
    "        all_data.append({\n",
    "            \"title\": row.get(\"title\", \"\"),\n",
    "            \"description\": row.get(\"description\", \"\"),\n",
    "            \"url\": row.get(\"url\", \"\"),\n",
    "            \"city\": row.get(\"city\", \"\"),\n",
    "            \"type\": \"video\",\n",
    "            \"metadata\": metadata\n",
    "        })\n",
    "\n",
    "# Convertir en DataFrame pour un traitement facile\n",
    "merged_df = pd.DataFrame(all_data)\n",
    "\n",
    "# Aperçu des données fusionnées\n",
    "print(merged_df.head())\n",
    "\n",
    "# Sauvegarder le DataFrame fusionné dans un fichier CSV local\n",
    "output_file = os.path.join(DATA_DIR, \"merged_videos_data.csv\")\n",
    "merged_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"Fichier fusionné sauvegardé dans : {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stockage ( storage) des données acquises Données  (textes normalisés , IMG , Données géospatiales OSM ,Vidéos éducatives et documentaires )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base centrale pour les métadonnées :\n",
    "  * une base commune  Elasticsearch pour stocker les métadonnées de tous les types de données.\n",
    "\tCette base  va liée les différentes sources de données (académiques, géospatiales, vidéos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (8.16.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from elasticsearch) (8.15.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2.2.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\ec\\onedrive\\bureau\\generative_ia\\projet_generative_ia\\llm\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 02:41:21,597 - INFO - HEAD https://localhost:9200/ [status:200 duration:0.384s]\n",
      "2024-12-03 02:41:21,599 - INFO - Connexion à Elasticsearch réussie.\n",
      "2024-12-03 02:41:21,636 - INFO - HEAD https://localhost:9200/geosearch_metadata [status:200 duration:0.036s]\n",
      "2024-12-03 02:41:21,636 - INFO - L'index 'geosearch_metadata' existe déjà.\n",
      "2024-12-03 02:41:21,641 - INFO - Indexation des fichiers dans le répertoire : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\meta_doc_PDF\n",
      "2024-12-03 02:41:21,643 - WARNING - Le répertoire C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\meta_doc_PDF n'existe pas. Ignoré.\n",
      "2024-12-03 02:41:21,643 - INFO - Indexation des fichiers dans le répertoire : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_OSM\n",
      "2024-12-03 02:41:21,644 - WARNING - Le répertoire C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_OSM n'existe pas. Ignoré.\n",
      "2024-12-03 02:41:21,645 - INFO - Indexation des fichiers dans le répertoire : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_IMG\n",
      "2024-12-03 02:41:21,646 - WARNING - Le répertoire C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_IMG n'existe pas. Ignoré.\n",
      "2024-12-03 02:41:21,647 - INFO - Indexation des fichiers dans le répertoire : C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_videos\n",
      "2024-12-03 02:41:21,648 - WARNING - Le répertoire C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_videos n'existe pas. Ignoré.\n",
      "2024-12-03 02:41:21,649 - INFO - Indexation des métadonnées terminée.\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from requests.exceptions import SSLError\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "# Configurer les logs\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Paramètres Elasticsearch\n",
    "ES_HOST = \"https://localhost:9200\"\n",
    "ES_USERNAME = \"elastic\"\n",
    "ES_PASSWORD = \"4fHbHMK5UaiGY3tzqeTJ\"  # Remplacez par vos identifiants\n",
    "INDEX_NAME = \"geosearch_metadata\"\n",
    "\n",
    "# Répertoires des métadonnées\n",
    "DIRECTORIES = {\n",
    "    \"meta_doc_PDF\": r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\meta_doc_PDF\",\n",
    "    \"Json_OSM\": r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_OSM\",\n",
    "    \"Json_IMG\": r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_IMG\",\n",
    "    \"Json_videos\": r\"C:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\Collecte _Donnée\\Json_videos\",\n",
    "}\n",
    "\n",
    "# Répertoire de sauvegarde local\n",
    "BACKUP_DIR = r\"C:\\Users\\ec\\OneDrive\\Bureau\\Geosearch_Backup\"\n",
    "\n",
    "# Vérifiez et créez le répertoire de sauvegarde s'il n'existe pas\n",
    "if not os.path.exists(BACKUP_DIR):\n",
    "    os.makedirs(BACKUP_DIR)\n",
    "\n",
    "# Connexion sécurisée à Elasticsearch\n",
    "try:\n",
    "    es = Elasticsearch(\n",
    "        [ES_HOST],\n",
    "        basic_auth=(ES_USERNAME, ES_PASSWORD),\n",
    "        verify_certs=False,  # À éviter en production, ajoutez un certificat si possible\n",
    "    )\n",
    "    if es.ping():\n",
    "        logging.info(\"Connexion à Elasticsearch réussie.\")\n",
    "    else:\n",
    "        logging.error(\"Échec de la connexion à Elasticsearch. Vérifiez les paramètres.\")\n",
    "        exit()\n",
    "except SSLError as ssl_error:\n",
    "    logging.error(f\"Erreur SSL : {ssl_error}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erreur de connexion à Elasticsearch : {e}\")\n",
    "    exit()\n",
    "\n",
    "# Création de l'index avec mapping\n",
    "try:\n",
    "    if not es.indices.exists(index=INDEX_NAME):\n",
    "        es.indices.create(index=INDEX_NAME, body={\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"category\": {\"type\": \"keyword\"},\n",
    "                    \"file_path\": {\"type\": \"text\"},\n",
    "                    \"metadata\": {\"type\": \"object\"},\n",
    "                    \"location\": {\"type\": \"geo_point\"}  # Optionnel pour les données géospatiales\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "        logging.info(f\"Index '{INDEX_NAME}' créé avec succès.\")\n",
    "    else:\n",
    "        logging.info(f\"L'index '{INDEX_NAME}' existe déjà.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erreur lors de la création de l'index '{INDEX_NAME}': {e}\")\n",
    "    exit()\n",
    "\n",
    "# Fonction pour charger, indexer les fichiers JSON, et sauvegarder localement\n",
    "def load_metadata(directory, category):\n",
    "    if not os.path.exists(directory):\n",
    "        logging.warning(f\"Le répertoire {directory} n'existe pas. Ignoré.\")\n",
    "        return\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.json'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    # Préparation des données pour Elasticsearch\n",
    "                    doc = {\n",
    "                        \"category\": category,\n",
    "                        \"file_path\": file_path,\n",
    "                        \"metadata\": data\n",
    "                    }\n",
    "                    # Ajout de champs géospatiaux si disponibles\n",
    "                    if \"latitude\" in data and \"longitude\" in data:\n",
    "                        doc[\"location\"] = {\"lat\": data[\"latitude\"], \"lon\": data[\"longitude\"]}\n",
    "\n",
    "                    # Indexation dans Elasticsearch\n",
    "                    es.index(index=INDEX_NAME, body=doc)\n",
    "                    logging.info(f\"Fichier indexé : {file}\")\n",
    "\n",
    "                    # Sauvegarder une copie locale dans le répertoire BACKUP_DIR\n",
    "                    category_backup_dir = os.path.join(BACKUP_DIR, category)\n",
    "                    if not os.path.exists(category_backup_dir):\n",
    "                        os.makedirs(category_backup_dir)\n",
    "                    shutil.copy(file_path, category_backup_dir)\n",
    "                    logging.info(f\"Fichier sauvegardé localement : {file_path}\")\n",
    "            except json.JSONDecodeError:\n",
    "                logging.error(f\"Erreur JSON dans le fichier : {file}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Erreur lors du traitement du fichier {file}: {e}\")\n",
    "\n",
    "# Indexer les fichiers dans chaque répertoire\n",
    "for category, directory in DIRECTORIES.items():\n",
    "    logging.info(f\"Indexation des fichiers dans le répertoire : {directory}\")\n",
    "    load_metadata(directory, category)\n",
    "\n",
    "logging.info(\"Indexation des métadonnées terminée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "2024-11-29 00:39:54,280 - INFO - HEAD https://localhost:9200/geosearch_metadata [status:200 duration:0.004s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'index 'geosearch_metadata' existe.\n"
     ]
    }
   ],
   "source": [
    "# Vérifiez si l'index existe\n",
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    print(f\"L'index '{INDEX_NAME}' existe.\")\n",
    "else:\n",
    "    print(f\"L'index '{INDEX_NAME}' n'existe pas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ec\\OneDrive\\Bureau\\Generative_IA\\Projet_Generative_IA\\LLM\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "2024-11-29 00:40:09,538 - INFO - POST https://localhost:9200/geosearch_metadata/_search [status:200 duration:9.904s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents indexés :\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Abidjan_doc_1_metadata.json', 'metadata': {'title': '', 'author': 'HP', 'creation_date': \"D:20220905171359+00'00'\", 'modification_date': \"D:20220905171359+00'00'\", 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Abidjan_doc_2_metadata.json', 'metadata': {'title': 'World Bank Document', 'author': 'World Bank Group', 'creation_date': \"D:20190318032621-04'00'\", 'modification_date': 'D:20190318072648', 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Abidjan_doc_3_metadata.json', 'metadata': {'title': '', 'author': '清水研', 'creation_date': \"D:20190312105944+09'00'\", 'modification_date': \"D:20190516171006+09'00'\", 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Bamako_doc_1_metadata.json', 'metadata': {'title': 'PRIMATURE', 'author': 'dougoucolo konare', 'creation_date': 'D:20080818170321Z', 'modification_date': \"D:20220228192811+01'00'\", 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Bamako_doc_2_metadata.json', 'metadata': {'title': 'Microsoft Word - PNAEP TEXTE V.FINALE corrigée.DOC', 'author': 'Administrateur', 'creation_date': \"D:20041126105856+01'00'\", 'modification_date': \"D:20041126105856+01'00'\", 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Bamako_doc_3_metadata.json', 'metadata': {'title': 'PEMU', 'author': 'Veronique Verdeil', 'creation_date': \"D:20181130081348-05'00'\", 'modification_date': \"D:20181130081348-05'00'\", 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Cotonou_doc_1_metadata.json', 'metadata': {'title': '', 'author': 'AKPO Eric Ayedesso (PAC)', 'creation_date': \"D:20230830155225+01'00'\", 'modification_date': \"D:20230830155342+01'00'\", 'subject': '', 'keywords': ', docId:B80E961F3383D1631ACD76CDEDCE6B6A'}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Cotonou_doc_2_metadata.json', 'metadata': {'title': \"Projet de renforcement et de réhabilitation du réseau de distribution d'électricité au niveau régional et à Cotonou\", 'author': 'Millennium Challenge Corporation', 'creation_date': \"D:20211026180014-04'00'\", 'modification_date': \"D:20220113143552-05'00'\", 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Cotonou_doc_3_metadata.json', 'metadata': {'title': '', 'author': 'AKPO Eric Ayedesso (PAC)', 'creation_date': \"D:20230825164223+01'00'\", 'modification_date': \"D:20230830155319+01'00'\", 'subject': '', 'keywords': ''}}\n",
      "{'category': 'meta_doc_PDF', 'file_path': 'C:\\\\Users\\\\ec\\\\OneDrive\\\\Bureau\\\\Generative_IA\\\\Projet_Generative_IA\\\\Collecte _Donnée\\\\meta_doc_PDF\\\\Libreville_doc_1_metadata.json', 'metadata': {'title': '', 'author': '', 'creation_date': \"D:20230424152155+01'00'\", 'modification_date': \"D:20240207155538-01'00'\", 'subject': '', 'keywords': ''}}\n"
     ]
    }
   ],
   "source": [
    "response = es.search(index=INDEX_NAME, body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Documents indexés :\")\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit[\"_source\"])  # Affiche les données indexées\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Construction des vecteurs à partir des données textuelles (PDF fractionnés )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
